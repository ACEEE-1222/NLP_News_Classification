{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9ee79d24eee34e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'D:\\Desktop\\MachineLearning\\NLP\\NLP新闻分类赛')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-18T04:29:17.651380500Z",
     "start_time": "2025-01-18T04:29:17.631471Z"
    }
   },
   "id": "dd490484bead6556",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-18T04:40:21.957134100Z",
     "start_time": "2025-01-18T04:29:26.382999600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# 读取数据集\n",
    "train_df = pd.read_csv('./data/train_set.csv', sep='\\t')\n",
    "test_df = pd.read_csv('./data/test_a.csv', sep='\\t')\n",
    "# 初始化TF-IDF向量化器\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',  # 特征提取基于单词\n",
    "    ngram_range=(1, 3),  # 提取一元组和二元组作为特征\n",
    "    min_df=3,  # 忽略在少于 3 个文档中出现的词\n",
    "    max_df=0.9,  # 它将忽略那些在超过 90% 的文档中出现的词\n",
    "    use_idf=True,  # 使用逆文档频率对词频进行加权\n",
    "    max_features=3000,  # 不限制特征的数量\n",
    "    smooth_idf=True,  # 在 IDF 计算中添加 1 进行平滑处理\n",
    "    sublinear_tf=True  # 对词频进行子线性缩放，使用 1 + log(TF) 代替 TF\n",
    ")\n",
    "\n",
    "# 将文本转化为TF-IDF特征\n",
    "train_tfidf = tfidf.fit_transform(train_df['text'])\n",
    "test_tfidf = tfidf.transform(test_df['text'])\n",
    "\n",
    "# 打印矩阵类型\n",
    "print(type(train_tfidf))\n",
    "# 将稀疏矩阵保存为 .npz 格式\n",
    "sp.save_npz('./preprocess/train_tfidf.npz', train_tfidf)\n",
    "sp.save_npz('./preprocess/test_tfidf.npz', test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# 3. 分割数据集为训练集和测试集（80%训练集，20%测试集）\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. 初始化随机森林分类器\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 5. 设置超参数搜索空间\n",
    "param_grid = {\n",
    "    'max_depth': [None],\n",
    "    'n_estimators': range(100, 301, 100),  # 决策树的数量\n",
    "    'min_samples_split': [10, 20, 30],  # 分割内部节点所需的最小样本数\n",
    "    'min_samples_leaf': [10, 20, 30],  # 叶子节点的最小样本数\n",
    "    'max_features': ['sqrt', 'log2', None]  # 在每个决策树分裂时选择的最大特征数\n",
    "}\n",
    "\n",
    "# 6. 使用 HalvingGridSearchCV 进行超参数调优\n",
    "halving_grid_search = HalvingGridSearchCV(estimator=rf, param_grid=param_grid,\n",
    "                                          factor=3,  # 每次折半搜索空间\n",
    "                                          max_resources=50000,  # 自动选择最大资源\n",
    "                                          cv=2,  # k折交叉验证\n",
    "                                          verbose=2,\n",
    "                                          n_jobs=-1,\n",
    "                                          scoring=make_scorer(f1_score, average='macro')  # 设置 F1 分数作为评分标准\n",
    "                                          )\n",
    "\n",
    "# 7. 训练模型\n",
    "halving_grid_search.fit(X, y)\n",
    "\n",
    "# 8. 输出最优超参数\n",
    "print(\"Best parameters found: \", halving_grid_search.best_params_)\n",
    "print(\"Best score: \", -halving_grid_search.best_score_)\n",
    "\n",
    "# 9. 使用最优模型进行预测\n",
    "best_rf = halving_grid_search.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed921ca7e29d5240",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 获取模型的所有参数\n",
    "all_params = best_rf.get_params()\n",
    "print(all_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42eace401448b4c4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_set.csv', sep='\\t')\n",
    "test_df = pd.read_csv('data/test_a.csv', sep='\\t')\n",
    "X_train = sp.load_npz('preprocess/train_tfidf.npz')\n",
    "y_train = train_df['label'].values\n",
    "X_test = sp.load_npz('preprocess/test_tfidf.npz')\n",
    "print(type(y_train))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "593788fd34ff7f7c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# 初始化随机森林分类器\n",
    "rf = RandomForestClassifier(random_state=42,\n",
    "                            n_estimators=1000,\n",
    "                            max_depth=None,\n",
    "                            max_features='sqrt',\n",
    "                            min_samples_split=20,\n",
    "                            min_samples_leaf=20,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=2)\n",
    "\n",
    "# 初始化 StratifiedKFold 进行分层五折交叉验证\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 存储每折的验证集和测试集的预测结果\n",
    "val_predictions = []\n",
    "test_predictions = []\n",
    "# 存储每折的验证集和测试集的真实标签\n",
    "val_true_labels = []\n",
    "test_true_labels = []\n",
    "\n",
    "# 进行五折交叉验证\n",
    "for train_index, val_index in skf.split(X_train, y_train):\n",
    "    # 分割训练集和验证集\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # 训练模型\n",
    "    rf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # 在验证集上进行预测\n",
    "    y_val_pred = rf.predict(X_val_fold)\n",
    "    val_predictions.append(y_val_pred)\n",
    "    val_true_labels.append(y_val_fold)\n",
    "\n",
    "    # 在测试集上进行预测\n",
    "    y_test_pred = rf.predict(X_test)\n",
    "    test_predictions.append(y_test_pred)\n",
    "\n",
    "# 你可以将预测结果和真实标签转换为 numpy 数组，方便后续处理\n",
    "val_predictions = np.array(val_predictions)\n",
    "test_predictions = np.array(test_predictions)\n",
    "val_true_labels = np.array(val_true_labels)\n",
    "\n",
    "# 计算验证集和测试集的评估指标（例如 F1 分数）\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 计算每折验证集的 F1 分数\n",
    "val_f1_scores = []\n",
    "for i in range(5):\n",
    "    f1 = f1_score(val_true_labels[i], val_predictions[i], average='macro')\n",
    "    val_f1_scores.append(f1)\n",
    "print(\"F1 scores on validation sets:\", val_f1_scores)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e74385cdbdc5d4b1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 130 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=16)]: Done 333 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=16)]: Done 616 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    6.3s finished\n"
     ]
    }
   ],
   "source": [
    "# 整合测试集预测结果\n",
    "# 假设我们取多数投票结果作为最终的测试集预测结果\n",
    "\n",
    "y_test_pred = rf.predict(test_tfidf)\n",
    "# final_test_pred = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=test_predictions)\n",
    "test_df['label'] = y_test_pred\n",
    "test_df['label'].to_csv(\"./result/submission_randomforest.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-18T04:41:27.309049500Z",
     "start_time": "2025-01-18T04:41:20.817865700Z"
    }
   },
   "id": "3d614d4eaad0c2dd",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# 保存模型\n",
    "# joblib.dump(final_rf, './model/rf_clf.joblib')\n",
    "# 加载模型\n",
    "# best_rf_loaded = joblib.load('rf_clf.joblib')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c92081c2bebbc65",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## xgboost"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb1bf0f592bf3924"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "X = train_test\n",
    "y = train_df['label']  # 假设标签列名为 'label'\n",
    "\n",
    "# 4. 初始化 XGBoost 分类器\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    random_state=42,  # 设置随机种子，确保结果可重复\n",
    "    booster='gbtree',  # 使用梯度提升树作为提升类型\n",
    "    learning_rate=0.1,  # 学习率，控制每棵树的贡献\n",
    "    objective='multi:softmax',  # 目标函数设置为多分类任务，使用 softmax 作为多分类的目标函数\n",
    "    subsample=0.8,  # 训练数据的子样本比例（随机选择样本，防止过拟合）\n",
    "    colsample_bytree=0.5,  # 每棵树随机选择特征的比例\n",
    "    reg_lambda=10,  # L2 正则化项的权重\n",
    "    nthread=-1,  # 使用的 CPU 核心数，-1 表示使用所有可用核心\n",
    "    num_class=19,  # 类别数量（多分类任务的类别数）\n",
    "    # XGBoost 中没有 colsample_bylevel 参数，可以使用 colsample_bynode 代替，不过功能不完全相同\n",
    "    # min_child_weight=1.5,       # XGBoost 中对应的是 min_child_weight，但需要注意它们的计算方式可能有细微差异\n",
    "    eval_metric='mlogloss'  # 评估指标使用多分类对数损失\n",
    ")\n",
    "\n",
    "# 5. 设置超参数搜索空间（适用于XGBoost）\n",
    "param_grid = {\n",
    "    'n_estimators': range(100, 301, 100),  # 决策树的数量\n",
    "    'max_depth': range(10, 21, 5),  # 树的最大深度\n",
    "    'colsample_bytree': [0.8, 1.0],  # 每棵树的列采样比例\n",
    "    'min_child_weight': [1, 3, 5]  # 每个叶子节点的最小样本权重\n",
    "}\n",
    "\n",
    "# 6. 使用 HalvingGridSearchCV 进行超参数调优\n",
    "halving_grid_search = HalvingGridSearchCV(estimator=xgb_clf, param_grid=param_grid,\n",
    "                                          factor=3,  # 每次折半搜索空间\n",
    "                                          max_resources=50000,  # 自动选择最大资源\n",
    "                                          cv=3,  # 2折交叉验证\n",
    "                                          verbose=1,\n",
    "                                          n_jobs=-1,\n",
    "                                          scoring=make_scorer(f1_score, average='macro')  # 设置 F1 分数作为评分标准\n",
    "                                          )\n",
    "\n",
    "# 7. 训练模型\n",
    "halving_grid_search.fit(X, y)\n",
    "\n",
    "# 8. 输出最优超参数\n",
    "print(\"Best parameters found: \", halving_grid_search.best_params_)\n",
    "print(\"Best score: \", -halving_grid_search.best_score_)\n",
    "\n",
    "# 9. 使用最优模型进行预测\n",
    "best_xgb = halving_grid_search.best_estimator_\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d921ef3036421ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# 假设 train_test 和 train_df 数据已准备好\n",
    "X = train_test\n",
    "y = train_df['label']  # 假设标签列名为 'label'\n",
    "\n",
    "# 4. 初始化 LightGBM 分类器\n",
    "lgbm = lgb.LGBMClassifier(random_state=42,  # 设置随机种子，确保结果可重复\n",
    "                          boosting_type='gbdt',  # 使用梯度提升树（Gradient Boosting Decision Tree）作为提升类型\n",
    "                          max_depth=-1,  # 树的最大深度，-1表示没有限制\n",
    "                          learning_rate=0.1,  # 学习率，控制每棵树的贡献\n",
    "                          objective='multiclass',  # 目标函数设置为多分类任务\n",
    "                          subsample=0.7,  # 训练数据的子样本比例（随机选择样本，防止过拟合）\n",
    "                          colsample_bytree=0.5,  # 每棵树随机选择特征的比例\n",
    "                          reg_lambda=10,  # L2 正则化项的权重\n",
    "                          n_jobs=-1,  # 使用的CPU核心数，-1表示使用所有可用核心\n",
    "                          num_class=19,  # 类别数量（多分类任务的类别数）\n",
    "                          colsample_bylevel=0.5,  # 每一层的特征采样比例\n",
    "                          min_child_weight=1.5,  # 叶子节点最小的样本权重和，用于防止过拟合\n",
    "                          metric='multi_logloss')  # 评估指标使用多分类对数损失\n",
    "\n",
    "# 5. 设置超参数搜索空间\n",
    "param_grid = {\n",
    "    'n_estimators': range(10, 511, 100),  # 基学习器的数量\n",
    "    'num_leaves': [31, 50, 80],  # 最大叶子数\n",
    "}\n",
    "\n",
    "# 6. 使用 HalvingGridSearchCV 进行超参数调优\n",
    "halving_grid_search = HalvingGridSearchCV(estimator=lgbm, param_grid=param_grid,\n",
    "                                          factor=3,  # 每次折半搜索空间\n",
    "                                          max_resources=100000,  # 自动选择最大资源\n",
    "                                          cv=3,  # 2折交叉验证\n",
    "                                          verbose=2,\n",
    "                                          n_jobs=-1,\n",
    "                                          scoring=make_scorer(f1_score, average='macro')  # 设置 F1 分数作为评分标准\n",
    "                                          )\n",
    "\n",
    "# 7. 训练模型\n",
    "halving_grid_search.fit(X, y)\n",
    "\n",
    "# 8. 输出最优超参数\n",
    "print(\"Best parameters found: \", halving_grid_search.best_params_)\n",
    "print(\"Best score: \", -halving_grid_search.best_score_)\n",
    "\n",
    "# 9. 使用最优模型进行预测\n",
    "best_lgbm = halving_grid_search.best_estimator_\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6e2f5bf4646234e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nlp",
   "language": "python",
   "display_name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
