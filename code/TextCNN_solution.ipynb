{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:\\Desktop\\MachineLearning\\NLP\\NLP新闻分类赛')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:55:40.898075100Z",
     "start_time": "2025-01-19T15:55:40.878807800Z"
    }
   },
   "id": "6732d501dbdc4a4d",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:55:48.710075400Z",
     "start_time": "2025-01-19T15:55:48.690255500Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "# set seed\n",
    "seed = 1222\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set cuda\n",
    "gpu = 0\n",
    "use_cuda = gpu >= 0 and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device(\"cuda\", gpu)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "logging.info(\"Use cuda: %s, gpu id: %d.\", use_cuda, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# split data to 10 fold\n",
    "fold_num = 10\n",
    "data_file = './data/train_set.csv'\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def all_data2fold(fold_num):\n",
    "    fold_data = []\n",
    "    f = pd.read_csv(data_file, sep='\\t')\n",
    "    texts = f['text'].tolist()\n",
    "    labels = f['label'].tolist()\n",
    "\n",
    "    total = len(labels)\n",
    "\n",
    "    index = list(range(total))\n",
    "    # 打乱数据\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    # all_texts 和 all_labels 都是 shuffle 之后的数据\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    for i in index:\n",
    "        all_texts.append(texts[i])\n",
    "        all_labels.append(labels[i])\n",
    "\n",
    "    # 构造一个 dict，key 为 label，value 是一个 list，存储的是该类对应的 index\n",
    "    label2id = {}\n",
    "    for i in range(total):\n",
    "        label = str(all_labels[i])\n",
    "        if label not in label2id:\n",
    "            label2id[label] = [i]\n",
    "        else:\n",
    "            label2id[label].append(i)\n",
    "\n",
    "    # all_index 是一个 list，里面包括 10 个 list，称为 10 个 fold，存储 10 个 fold 对应的 index\n",
    "    all_index = [[] for _ in range(fold_num)]\n",
    "    for label, data in label2id.items():\n",
    "        # print(label, len(data))\n",
    "        batch_size = int(len(data) / fold_num)\n",
    "        # other 表示多出来的数据，other 的数据量是小于 fold_num 的\n",
    "        other = len(data) - batch_size * fold_num\n",
    "        # 把每一类对应的 index，添加到每个 fold 里面去\n",
    "        for i in range(fold_num):\n",
    "            # 如果 i < other，那么将一个数据添加到这一轮 batch 的数据中\n",
    "            cur_batch_size = batch_size + 1 if i < other else batch_size\n",
    "            # print(cur_batch_size)\n",
    "            # batch_data 是该轮 batch 对应的索引\n",
    "            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "            all_index[i].extend(batch_data)\n",
    "\n",
    "    batch_size = int(total / fold_num)\n",
    "    other_texts = []\n",
    "    other_labels = []\n",
    "    other_num = 0\n",
    "    start = 0\n",
    "\n",
    "    # 由于上面在分 batch 的过程中，每个 batch 的数据量不一样，这里是把数据平均到每个 batch\n",
    "    for fold in range(fold_num):\n",
    "        num = len(all_index[fold])\n",
    "        texts = [all_texts[i] for i in all_index[fold]]\n",
    "        labels = [all_labels[i] for i in all_index[fold]]\n",
    "\n",
    "        if num > batch_size: # 如果大于 batch_size 那么就取 batch_size 大小的数据\n",
    "            fold_texts = texts[:batch_size]\n",
    "            other_texts.extend(texts[batch_size:])\n",
    "            fold_labels = labels[:batch_size]\n",
    "            other_labels.extend(labels[batch_size:])\n",
    "            other_num += num - batch_size\n",
    "        elif num < batch_size: # 如果小于 batch_size，那么就补全到 batch_size 的大小\n",
    "            end = start + batch_size - num\n",
    "            fold_texts = texts + other_texts[start: end]\n",
    "            fold_labels = labels + other_labels[start: end]\n",
    "            start = end\n",
    "        else:\n",
    "            fold_texts = texts\n",
    "            fold_labels = labels\n",
    "\n",
    "        assert batch_size == len(fold_labels)\n",
    "\n",
    "        # shuffle\n",
    "        index = list(range(batch_size))\n",
    "        np.random.shuffle(index)\n",
    "        # 这里是为了打乱数据\n",
    "        shuffle_fold_texts = []\n",
    "        shuffle_fold_labels = []\n",
    "        for i in index:\n",
    "            shuffle_fold_texts.append(fold_texts[i])\n",
    "            shuffle_fold_labels.append(fold_labels[i])\n",
    "\n",
    "        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n",
    "        fold_data.append(data)\n",
    "\n",
    "    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n",
    "\n",
    "    return fold_data\n",
    "\n",
    "# fold_data 是一个 list，有 10 个元素，每个元素是 dict，包括 label 和 text\n",
    "fold_data = all_data2fold(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:03.359723300Z",
     "start_time": "2025-01-19T15:55:54.297860200Z"
    }
   },
   "id": "fb20dc74e26186c9",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build train, dev, test data\n",
    "fold_id = 9\n",
    "\n",
    "# dev\n",
    "dev_data = fold_data[fold_id]\n",
    "\n",
    "# train 取出前 9 个 fold 的数据\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for i in range(0, fold_id):\n",
    "    data = fold_data[i]\n",
    "    train_texts.extend(data['text'])\n",
    "    train_labels.extend(data['label'])\n",
    "\n",
    "train_data = {'label': train_labels, 'text': train_texts}\n",
    "\n",
    "# test 读取测试集数据\n",
    "test_data_file = './data/test_a.csv'\n",
    "f = pd.read_csv(test_data_file, sep='\\t', encoding='UTF-8')\n",
    "texts = f['text'].tolist()\n",
    "test_data = {'label': [0] * len(texts), 'text': texts}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:05.641242400Z",
     "start_time": "2025-01-19T15:56:03.363640100Z"
    }
   },
   "id": "44817640241e329a",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build vocab\n",
    "from collections import Counter\n",
    "from transformers import BasicTokenizer\n",
    "\n",
    "basic_tokenizer = BasicTokenizer()\n",
    "\n",
    "# Vocab 的作用是：\n",
    "# 1. 创建 词 和 index 对应的字典，这里包括 2 份字典，分别是：_id2word 和 _id2extword\n",
    "# 其中 _id2word 是从新闻得到的， 把词频小于 5 的词替换为了 UNK。对应到模型输入的 batch_inputs1。\n",
    "# _id2extword 是从 word2vec.txt 中得到的，有 5976 个词。对应到模型输入的 batch_inputs2。\n",
    "# 后面会有两个 embedding 层，其中 _id2word 对应的 embedding 是可学习的，_id2extword 对应的 embedding 是从文件中加载的，是固定的\n",
    "# 2.创建 label 和 index 对应的字典\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, train_data):\n",
    "        self.min_count = 5\n",
    "        self.pad = 0\n",
    "        self.unk = 1\n",
    "        self._id2word = ['[PAD]', '[UNK]']\n",
    "        self._id2extword = ['[PAD]', '[UNK]']\n",
    "\n",
    "        self._id2label = []\n",
    "        self.target_names = []\n",
    "\n",
    "        self.build_vocab(train_data)\n",
    "\n",
    "        reverse = lambda x: dict(zip(x, range(len(x))))\n",
    "        #创建词和 index 对应的字典\n",
    "        self._word2id = reverse(self._id2word)\n",
    "        #创建 label 和 index 对应的字典\n",
    "        self._label2id = reverse(self._id2label)\n",
    "\n",
    "        logging.info(\"Build vocab: words %d, labels %d.\" % (self.word_size, self.label_size))\n",
    "\n",
    "    #创建词典\n",
    "    def build_vocab(self, data):\n",
    "        self.word_counter = Counter()\n",
    "        #计算每个词出现的次数\n",
    "        for text in data['text']:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                self.word_counter[word] += 1\n",
    "        # 去掉频次小于 min_count = 5 的词，把词存到 _id2word\n",
    "        for word, count in self.word_counter.most_common():\n",
    "            if count >= self.min_count:\n",
    "                self._id2word.append(word)\n",
    "\n",
    "        label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育', 7: '财经',\n",
    "                      8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n",
    "\n",
    "        self.label_counter = Counter(data['label'])\n",
    "\n",
    "        for label in range(len(self.label_counter)):\n",
    "            count = self.label_counter[label] # 取出 label 对应的次数\n",
    "            self._id2label.append(label)\n",
    "            self.target_names.append(label2name[label]) # 根据label数字取出对应的名字\n",
    "\n",
    "    def load_pretrained_embs(self, embfile):\n",
    "        with open(embfile, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            items = lines[0].split()\n",
    "            # 第一行分别是单词数量、词向量维度\n",
    "            word_count, embedding_dim = int(items[0]), int(items[1])\n",
    "\n",
    "        index = len(self._id2extword)\n",
    "        embeddings = np.zeros((word_count + index, embedding_dim))\n",
    "        self._id2extword = ['[PAD]', '[UNK]']\n",
    "        # 下面的代码和 word2vec.txt 的结构有关\n",
    "        for line in lines[1:]:\n",
    "            values = line.split()\n",
    "            self._id2extword.append(values[0]) # 首先添加第一列的单词\n",
    "            vector = np.array(values[1:], dtype='float64') # 然后添加后面 100 列的词向量\n",
    "            embeddings[self.unk] += vector\n",
    "            embeddings[index] = vector\n",
    "            index += 1\n",
    "\n",
    "        # unk 的词向量是所有词的平均\n",
    "        embeddings[self.unk] = embeddings[self.unk] / word_count\n",
    "        # 除以标准差干嘛？\n",
    "        embeddings = embeddings / np.std(embeddings)\n",
    "\n",
    "        reverse = lambda x: dict(zip(x, range(len(x))))\n",
    "        self._extword2id = reverse(self._id2extword)\n",
    "        \n",
    "        print(len(set(self._id2extword)),len(self._id2extword))\n",
    "        assert len(set(self._id2extword)) == len(self._id2extword)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    # 根据单词得到 id\n",
    "    def word2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            # dict.get(key, default=None)\n",
    "            return [self._word2id.get(x, self.unk) for x in xs]\n",
    "        return self._word2id.get(xs, self.unk)\n",
    "    # 根据单词得到 ext id\n",
    "    def extword2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._extword2id.get(x, self.unk) for x in xs]\n",
    "        return self._extword2id.get(xs, self.unk)\n",
    "    # 根据 label 得到 id\n",
    "    def label2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._label2id.get(x, self.unk) for x in xs]\n",
    "        return self._label2id.get(xs, self.unk)\n",
    "\n",
    "# 为什么使用 @property？\n",
    "# 避免直接修改： 使用 @property 装饰器将这些方法定义为属性，而不是普通的函数。这样，代码可以通过 vocab.word_size、vocab.extword_size 和 vocab.label_size 访问这些方法，就像访问属性一样，而不是调用函数。这种方式使得这些方法看起来像属性（而非函数），从而提高了代码的简洁性和可读性。\n",
    "# \n",
    "# 保证一致性： 通过这些属性方法，确保返回的大小总是当前词汇表或标签表的最新大小。直接访问字典的长度可能不如通过方法来管理的统一，特别是在构建词汇表或标签表的过程中。\n",
    "    @property\n",
    "    def word_size(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    @property\n",
    "    def extword_size(self):\n",
    "        return len(self._id2extword)\n",
    "\n",
    "    @property\n",
    "    def label_size(self):\n",
    "        return len(self._id2label)\n",
    "\n",
    "\n",
    "vocab = Vocab(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:37.420245100Z",
     "start_time": "2025-01-19T15:56:05.669030600Z"
    }
   },
   "id": "e0e9b48c65079f80",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Attention 模块在这个任务中关注的是 每个文章中不同句子位置的重要性。\n",
    "# 给每个句子计算出一个Attention score并进行加权求和（sum(每个文章hidden state第i个位置*第i个Attention score)）\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
    "#       torch.from_numpy(b) 将 numpy 数组 b 转换为一个 PyTorch 张量。\n",
    "#       self.bias.data.copy_() 方法将转换后的 PyTorch 张量的数据复制到 self.bias 的数据存储中，这样就将 self.bias 中的数据初始化为全零。\n",
    "        b = np.zeros(hidden_size, dtype=np.float32)\n",
    "        self.bias.data.copy_(torch.from_numpy(b))\n",
    "\n",
    "        self.query = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.query.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "    def forward(self, batch_hidden, batch_masks):\n",
    "        # batch_hidden: b * doc_len * hidden_size (2 * hidden_size of lstm)\n",
    "        # batch_masks:  b x doc_len\n",
    "\n",
    "        # linear\n",
    "        # key： b * doc_len * hidden\n",
    "        key = torch.matmul(batch_hidden, self.weight) + self.bias\n",
    "\n",
    "        # compute attention\n",
    "        # matmul 会进行广播\n",
    "        #outputs: b * doc_len\n",
    "        outputs = torch.matmul(key, self.query)\n",
    "        # 1 - batch_masks 就是取反，把没有单词的句子置为 0\n",
    "        # masked_fill 的作用是 在 为 1 的地方替换为 value: float(-1e32)\n",
    "        # 将掩码的值替换为 -1e32（非常小的负数）而不是直接设为 0 是为了确保在 softmax 操作中，使掩码的部分对注意力得分的计算几乎没有影响。\n",
    "        masked_outputs = outputs.masked_fill((1 - batch_masks).bool(), float(-1e32))\n",
    "        #attn_scores：b * doc_len\n",
    "        attn_scores = F.softmax(masked_outputs, dim=1)\n",
    "\n",
    "        # 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0\n",
    "        masked_attn_scores = attn_scores.masked_fill((1 - batch_masks).bool(), 0.0)\n",
    "\n",
    "        # sum weighted sources\n",
    "        # masked_attn_scores.unsqueeze(1)：# b * 1 * doc_len\n",
    "        # key：b * doc_len * hidden\n",
    "        # batch_outputs：b * hidden\n",
    "        # torch.bmm：\n",
    "        # torch.bmm 是 PyTorch 中的一个函数，代表批量矩阵乘法（Batch Matrix-Matrix Product）。\n",
    "        # 它接受两个三维张量作为输入，第一个张量的形状是 (b, n, m)，第二个张量的形状是 (b, m, p)，其中 b 是批次大小，n、m 和 p 是矩阵的维度。\n",
    "        # 该函数会对每一对矩阵（在批次中）进行矩阵乘法操作，最终输出一个形状为 (b, n, p) 的张量。\n",
    "        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(1), key).squeeze(1)\n",
    "\n",
    "        return batch_outputs, attn_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:37.445537500Z",
     "start_time": "2025-01-19T15:56:37.428269700Z"
    }
   },
   "id": "727f1630ebda567f",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 读取训练好的词向量文件\n",
    "word2vec_path = './emb/word2vec.txt'\n",
    "dropout = 0.25\n",
    "\n",
    "# 输入是：\n",
    "# 输出是：\n",
    "class WordCNNEncoder(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super(WordCNNEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.word_dims = 100 # 词向量的长度是 100 维\n",
    "        # padding_idx 表示当取第 0 个词时，向量全为 0\n",
    "        # 这个 Embedding 层是可学习的\n",
    "        self.word_embed = nn.Embedding(vocab.word_size, self.word_dims, padding_idx=0)\n",
    "\n",
    "        extword_embed = vocab.load_pretrained_embs(word2vec_path)\n",
    "        extword_size, word_dims = extword_embed.shape\n",
    "        logging.info(\"Load extword embed: words %d, dims %d.\" % (extword_size, word_dims))\n",
    "\n",
    "        # # 这个 Embedding 层是不可学习的\n",
    "        self.extword_embed = nn.Embedding(extword_size, word_dims, padding_idx=0)\n",
    "        self.extword_embed.weight.data.copy_(torch.from_numpy(extword_embed))\n",
    "        self.extword_embed.weight.requires_grad = False\n",
    "\n",
    "        input_size = self.word_dims\n",
    "\n",
    "        self.filter_sizes = [2, 3, 4]  # n-gram window\n",
    "        self.out_channel = 100 #这个应该不用和word_dims相等\n",
    "        # 3 个卷积层，卷积核大小分别为 [2,100], [3,100], [4,100]\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, self.out_channel, (filter_size, input_size), bias=True)\n",
    "                                    for filter_size in self.filter_sizes])\n",
    "\n",
    "    def forward(self, word_ids, extword_ids):\n",
    "        # word_ids: sentence_num * sentence_len\n",
    "        # extword_ids: sentence_num * sentence_len\n",
    "        # batch_masks: sentence_num * sentence_len\n",
    "        sen_num, sent_len = word_ids.shape\n",
    "\n",
    "        # word_embed: sentence_num * sentence_len * 100\n",
    "        # 根据 index 取出词向量\n",
    "        word_embed = self.word_embed(word_ids)\n",
    "        extword_embed = self.extword_embed(extword_ids)\n",
    "        batch_embed = word_embed + extword_embed\n",
    "\n",
    "        if self.training:\n",
    "            batch_embed = self.dropout(batch_embed)\n",
    "        # batch_embed: sentence_num x 1 x sentence_len x 100\n",
    "        # squeeze 是为了添加一个 channel 的维度，成为 B * C * H * W\n",
    "        # 方便下面做 卷积\n",
    "        batch_embed.unsqueeze_(1)\n",
    "\n",
    "        pooled_outputs = []\n",
    "        # 通过 3 个卷积核做 3 次卷积核池化\n",
    "        for i in range(len(self.filter_sizes)):\n",
    "            # 通过池化公式计算池化后的高度: o = (i-k)/s+1\n",
    "            # 其中 o 表示输出的长度\n",
    "            # k 表示卷积核大小\n",
    "            # s 表示步长，这里为 1\n",
    "            filter_height = sent_len - self.filter_sizes[i] + 1\n",
    "            # conv：sentence_num * out_channel * filter_height * 1\n",
    "            conv = self.convs[i](batch_embed)\n",
    "            hidden = F.relu(conv)\n",
    "            # 定义池化层\n",
    "            mp = nn.MaxPool2d((filter_height, 1))  # (filter_height, filter_width)\n",
    "            # pooled：sentence_num * out_channel * 1 * 1 -> sen_num * out_channel\n",
    "            # 也可以通过 squeeze 来删除无用的维度\n",
    "            pooled = mp(hidden).reshape(sen_num,\n",
    "                                        self.out_channel)\n",
    "\n",
    "            pooled_outputs.append(pooled)\n",
    "        # 拼接 3 个池化后的向量\n",
    "        # reps: sen_num * (3*out_channel)\n",
    "        reps = torch.cat(pooled_outputs, dim=1)\n",
    "\n",
    "        if self.training:\n",
    "            reps = self.dropout(reps)\n",
    "\n",
    "        return reps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:37.463608400Z",
     "start_time": "2025-01-19T15:56:37.436490900Z"
    }
   },
   "id": "f891f5fffaa782fd",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build sent encoder\n",
    "sent_hidden_size = 256\n",
    "sent_num_layers = 2\n",
    "\n",
    "\n",
    "class SentEncoder(nn.Module):\n",
    "    def __init__(self, sent_rep_size):\n",
    "        super(SentEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.sent_lstm = nn.LSTM(\n",
    "            input_size=sent_rep_size, # 每个句子经过 CNN 后得到 300 维向量\n",
    "            hidden_size=sent_hidden_size,# 输出的维度\n",
    "            num_layers=sent_num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, sent_reps, sent_masks):\n",
    "        # sent_reps:  b * doc_len * sent_rep_size\n",
    "        # sent_masks: b * doc_len\n",
    "        # sent_hiddens:  b * doc_len * hidden*2\n",
    "        sent_hiddens, _ = self.sent_lstm(sent_reps)\n",
    "        # 对应相乘，用到广播，是为了只保留有句子的位置的数值\n",
    "        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(2)\n",
    "\n",
    "        if self.training:\n",
    "            sent_hiddens = self.dropout(sent_hiddens)\n",
    "\n",
    "        return sent_hiddens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:37.475349800Z",
     "start_time": "2025-01-19T15:56:37.456624200Z"
    }
   },
   "id": "5af005c66add6e1b",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5978 5978\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "# build model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super(Model, self).__init__()\n",
    "        self.sent_rep_size = 3*100 # 经过 CNN 后得到的 300 维向量\n",
    "        self.doc_rep_size = sent_hidden_size * 2 # lstm 最后输出的向量长度\n",
    "        self.all_parameters = {}\n",
    "        parameters = []\n",
    "        self.word_encoder = WordCNNEncoder(vocab)\n",
    "\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.word_encoder.parameters())))\n",
    "\n",
    "        self.sent_encoder = SentEncoder(self.sent_rep_size)\n",
    "        self.sent_attention = Attention(self.doc_rep_size)\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_encoder.parameters())))\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_attention.parameters())))\n",
    "        # doc_rep_size\n",
    "        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=True)\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.out.parameters())))\n",
    "\n",
    "        if use_cuda:\n",
    "            self.to(device)\n",
    "\n",
    "        if len(parameters) > 0:\n",
    "            self.all_parameters[\"basic_parameters\"] = parameters\n",
    "\n",
    "        logging.info('Build model with cnn word encoder, lstm sent encoder.')\n",
    "\n",
    "        para_num = sum([np.prod(list(p.size())) for p in self.parameters()])\n",
    "        logging.info('Model param num: %.2f M.' % (para_num / 1e6))\n",
    "    def forward(self, batch_inputs):\n",
    "        # batch_inputs(batch_inputs1, batch_inputs2): b * doc_len * sentence_len\n",
    "        # batch_masks : b * doc_len * sentence_len\n",
    "        batch_inputs1, batch_inputs2, batch_masks = batch_inputs\n",
    "        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[0], batch_inputs1.shape[1], batch_inputs1.shape[2]\n",
    "        # batch_inputs1: sentence_num * sentence_len\n",
    "        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)\n",
    "        # batch_inputs2: sentence_num * sentence_len\n",
    "        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)\n",
    "        # batch_masks: sentence_num * sentence_len\n",
    "        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)\n",
    "        # sent_reps: sentence_num * sentence_rep_size\n",
    "        # sen_num * (3*out_channel) =  sen_num * 300\n",
    "        sent_reps = self.word_encoder(batch_inputs1, batch_inputs2)\n",
    "\n",
    "\n",
    "        # sent_reps：b * doc_len * sent_rep_size (doc_len即sentence_num，为每个文章句子的数量)\n",
    "        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)\n",
    "        # batch_masks：b * doc_len * max_sent_len\n",
    "        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)\n",
    "        # sent_masks：b * doc_len any(2) 表示在 第二个维度上判断\n",
    "        # 表示如果如果一个句子中有词 true，那么这个句子就是 true，用于给 lstm 过滤\n",
    "        sent_masks = batch_masks.bool().any(2).float()  # b x doc_len\n",
    "        # sent_hiddens: b * doc_len * num_directions * hidden_size\n",
    "        # sent_hiddens:  batch, doc_len, 2 * hidden_size\n",
    "        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)\n",
    "\n",
    "\n",
    "        # doc_reps: b * (2 * hidden_size)\n",
    "        # atten_scores: b * doc_len\n",
    "        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)\n",
    "\n",
    "        # b * num_labels\n",
    "        batch_outputs = self.out(doc_reps)\n",
    "\n",
    "        return batch_outputs\n",
    "\n",
    "\n",
    "model = Model(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:37.735271400Z",
     "start_time": "2025-01-19T15:56:37.467326Z"
    }
   },
   "id": "771c7a819ac59a90",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build optimizer\n",
    "learning_rate = 2e-4\n",
    "decay = .75\n",
    "decay_step = 10\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, model_parameters):\n",
    "        self.all_params = []\n",
    "        self.optims = []\n",
    "        self.schedulers = []\n",
    "\n",
    "        for name, parameters in model_parameters.items():\n",
    "            if name.startswith(\"basic\"):\n",
    "                optim = torch.optim.Adam(parameters, lr=learning_rate)\n",
    "                self.optims.append(optim)\n",
    "\n",
    "                l = lambda step: decay ** (step // decay_step) #lambda step: 表示定义了一个匿名函数，它接受一个参数 step，即当前训练步数。\n",
    "                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)\n",
    "                self.schedulers.append(scheduler)\n",
    "                self.all_params.extend(parameters)\n",
    "\n",
    "            else:\n",
    "                Exception(\"no nameed parameters.\")\n",
    "\n",
    "        self.num = len(self.optims)\n",
    "\n",
    "    def step(self):\n",
    "        for optim, scheduler in zip(self.optims, self.schedulers):\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for optim in self.optims:\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def get_lr(self):\n",
    "        lrs = tuple(map(lambda x: x.get_lr()[-1], self.schedulers))\n",
    "        lr = ' %.5f' * self.num\n",
    "        res = lr % lrs\n",
    "        return res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:37.768420600Z",
     "start_time": "2025-01-19T15:56:37.738621100Z"
    }
   },
   "id": "307a15880c49e57a",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#\n",
    "# 作用是：根据一篇文章，把这篇文章分割成多个句子\n",
    "# text 是一个新闻的文章\n",
    "# vocab 是词典\n",
    "# max_sent_len 表示每句话的长度\n",
    "# max_segment 表示最多有几句话\n",
    "# 最后返回的 segments 是一个list，其中每个元素是 tuple：(句子长度，句子本身)\n",
    "def sentence_split(text, vocab, max_sent_len=256, max_segment=16):\n",
    "\n",
    "    words = text.strip().split()\n",
    "    document_len = len(words)\n",
    "    # 划分句子的索引，句子长度为 max_sent_len\n",
    "    index = list(range(0, document_len, max_sent_len))\n",
    "    index.append(document_len)\n",
    "\n",
    "    segments = []\n",
    "    for i in range(len(index) - 1):\n",
    "        # 根据索引划分句子\n",
    "        segment = words[index[i]: index[i + 1]]\n",
    "        assert len(segment) > 0\n",
    "        # 把出现太少的词替换为 UNK\n",
    "        segment = [word if word in vocab._id2word else '<UNK>' for word in segment]\n",
    "        # 添加 tuple:(句子长度，句子本身)\n",
    "        segments.append([len(segment), segment])\n",
    "\n",
    "    assert len(segments) > 0\n",
    "    # 如果大于 max_segment 句话，则局数减少一半，返回一半的句子\n",
    "    if len(segments) > max_segment:\n",
    "        segment_ = int(max_segment / 2)\n",
    "        return segments[:segment_] + segments[-segment_:] #则只返回前后各一半的句子。\n",
    "    else:\n",
    "        # 否则返回全部句子\n",
    "        return segments"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:37.769424900Z",
     "start_time": "2025-01-19T15:56:37.752648600Z"
    }
   },
   "id": "12010a5d5f5ffc21",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 最后返回的数据是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)\n",
    "# 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)\n",
    "def get_examples(data, vocab, max_sent_len=256, max_segment=8):\n",
    "    label2id = vocab.label2id\n",
    "    examples = []\n",
    "\n",
    "    for text, label in zip(data['text'], data['label']):\n",
    "        # label\n",
    "        id = label2id(label)\n",
    "\n",
    "        # sents_words: 是一个list，其中每个元素是 tuple：(句子长度，句子本身)\n",
    "        sents_words = sentence_split(text, vocab, max_sent_len, max_segment)\n",
    "        doc = []\n",
    "        for sent_len, sent_words in sents_words:\n",
    "            # 把 word 转为 id\n",
    "            word_ids = vocab.word2id(sent_words)\n",
    "            # 把 word 转为 ext id\n",
    "            extword_ids = vocab.extword2id(sent_words)\n",
    "            doc.append([sent_len, word_ids, extword_ids])\n",
    "        examples.append([id, len(doc), doc])\n",
    "\n",
    "    logging.info('Total %d docs.' % len(examples))\n",
    "    return examples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:37.790780900Z",
     "start_time": "2025-01-19T15:56:37.766352400Z"
    }
   },
   "id": "397fe18cecfdcc39",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build loader\n",
    "# data 参数就是 get_examples() 得到的\n",
    "# data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)\n",
    "# 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)\n",
    "def batch_slice(data, batch_size):\n",
    "    batch_num = int(np.ceil(len(data) / float(batch_size)))\n",
    "    for i in range(batch_num):\n",
    "        # 如果 i < batch_num - 1，那么大小为 batch_size，否则就是最后一批数据\n",
    "        cur_batch_size = batch_size if i < batch_num - 1 else len(data) - batch_size * i\n",
    "        docs = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "\n",
    "        yield docs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:37.813377200Z",
     "start_time": "2025-01-19T15:56:37.779748500Z"
    }
   },
   "id": "334a5332c67760ea",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# data 参数就是 get_examples() 得到的\n",
    "# data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)\n",
    "# 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)\n",
    "def data_iter(data, batch_size, shuffle=True, noise=1.0):\n",
    "    \"\"\"\n",
    "    randomly permute data, then sort by source length, and partition into batches\n",
    "    ensure that the length of  sentences in each batch\n",
    "    \"\"\"\n",
    "\n",
    "    batched_data = []\n",
    "    if shuffle:\n",
    "        # 这里是打乱所有数据\n",
    "        np.random.shuffle(data)\n",
    "        # lengths 表示的是 每篇文章的句子数量\n",
    "        lengths = [example[1] for example in data]\n",
    "        noisy_lengths = [- (l + np.random.uniform(- noise, noise)) for l in lengths]\n",
    "        sorted_indices = np.argsort(noisy_lengths).tolist()\n",
    "        sorted_data = [data[i] for i in sorted_indices]\n",
    "    else:\n",
    "        sorted_data = data\n",
    "    # 把 batch 的数据放进一个 list\n",
    "    batched_data.extend(list(batch_slice(sorted_data, batch_size)))\n",
    "\n",
    "    if shuffle:\n",
    "        # 打乱 多个 batch\n",
    "        np.random.shuffle(batched_data)\n",
    "\n",
    "    for batch in batched_data:\n",
    "        yield batch\n",
    "        # yield 是 Python 中用于生成器（generator）的一种关键字。它的作用是暂停函数的执行并返回一个值给调用方，但与常规的 return 不同，yield 会保存函数的执行状态，以便下一次从暂停的地方继续执行。换句话说，yield 可以使函数返回一个值并“冻结”函数状态，直到下次需要更多的值时再恢复执行。"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:38.144816600Z",
     "start_time": "2025-01-19T15:56:38.123438500Z"
    }
   },
   "id": "2bb1e03494ea6c9c",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# some function\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def get_score(y_ture, y_pred):\n",
    "    y_ture = np.array(y_ture)\n",
    "    y_pred = np.array(y_pred)\n",
    "    f1 = f1_score(y_ture, y_pred, average='macro') * 100\n",
    "    p = precision_score(y_ture, y_pred, average='macro') * 100\n",
    "    r = recall_score(y_ture, y_pred, average='macro') * 100\n",
    "\n",
    "    return str((reformat(p, 2), reformat(r, 2), reformat(f1, 2))), reformat(f1, 2)\n",
    "\n",
    "# 保留 n 位小数点\n",
    "def reformat(num, n):\n",
    "    return float(format(num, '0.' + str(n) + 'f'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:56:41.505173800Z",
     "start_time": "2025-01-19T15:56:41.488515900Z"
    }
   },
   "id": "6a8b6e976d1ad234",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:---------Build Trainer--------\n",
      "INFO:root:Total 180000 docs.\n",
      "INFO:root:Total 20000 docs.\n",
      "INFO:root:Total 50000 docs.\n",
      "INFO:root:---------Start Training--------\n",
      "INFO:root:Start training...\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch   1 | step   3 | batch   3/352 | lr 0.00020 | loss 0.4392 | s/batch 9.61\n",
      "INFO:root:| epoch   1 | step   6 | batch   6/352 | lr 0.00020 | loss 0.4472 | s/batch 5.04\n",
      "INFO:root:| epoch   1 | step   9 | batch   9/352 | lr 0.00020 | loss 0.4786 | s/batch 3.69\n",
      "INFO:root:| epoch   1 | step  12 | batch  12/352 | lr 0.00020 | loss 0.4836 | s/batch 15.98\n",
      "INFO:root:| epoch   1 | step  15 | batch  15/352 | lr 0.00020 | loss 0.4228 | s/batch 4.69\n",
      "INFO:root:| epoch   1 | step  18 | batch  18/352 | lr 0.00020 | loss 0.4455 | s/batch 3.88\n",
      "INFO:root:| epoch   1 | step  21 | batch  21/352 | lr 0.00020 | loss 0.4465 | s/batch 3.88\n",
      "INFO:root:| epoch   1 | step  24 | batch  24/352 | lr 0.00020 | loss 0.4461 | s/batch 4.77\n",
      "INFO:root:| epoch   1 | step  27 | batch  27/352 | lr 0.00020 | loss 0.3959 | s/batch 7.47\n",
      "INFO:root:| epoch   1 | step  30 | batch  30/352 | lr 0.00020 | loss 0.4540 | s/batch 8.72\n",
      "INFO:root:| epoch   1 | step  33 | batch  33/352 | lr 0.00020 | loss 0.3892 | s/batch 12.23\n",
      "INFO:root:| epoch   1 | step  36 | batch  36/352 | lr 0.00020 | loss 0.4201 | s/batch 10.12\n",
      "INFO:root:| epoch   1 | step  39 | batch  39/352 | lr 0.00020 | loss 0.3938 | s/batch 12.24\n",
      "INFO:root:| epoch   1 | step  42 | batch  42/352 | lr 0.00020 | loss 0.4399 | s/batch 10.97\n",
      "INFO:root:| epoch   1 | step  45 | batch  45/352 | lr 0.00020 | loss 0.4264 | s/batch 4.31\n",
      "INFO:root:| epoch   1 | step  48 | batch  48/352 | lr 0.00020 | loss 0.3877 | s/batch 2.74\n",
      "INFO:root:| epoch   1 | step  51 | batch  51/352 | lr 0.00020 | loss 0.4393 | s/batch 13.48\n",
      "INFO:root:| epoch   1 | step  54 | batch  54/352 | lr 0.00020 | loss 0.4855 | s/batch 11.82\n",
      "INFO:root:| epoch   1 | step  57 | batch  57/352 | lr 0.00020 | loss 0.4662 | s/batch 9.13\n",
      "INFO:root:| epoch   1 | step  60 | batch  60/352 | lr 0.00020 | loss 0.4158 | s/batch 9.62\n",
      "INFO:root:| epoch   1 | step  63 | batch  63/352 | lr 0.00020 | loss 0.4175 | s/batch 3.86\n",
      "INFO:root:| epoch   1 | step  66 | batch  66/352 | lr 0.00020 | loss 0.4066 | s/batch 6.57\n",
      "INFO:root:| epoch   1 | step  69 | batch  69/352 | lr 0.00020 | loss 0.3753 | s/batch 11.09\n",
      "INFO:root:| epoch   1 | step  72 | batch  72/352 | lr 0.00020 | loss 0.4023 | s/batch 8.58\n",
      "INFO:root:| epoch   1 | step  75 | batch  75/352 | lr 0.00020 | loss 0.4444 | s/batch 3.72\n",
      "INFO:root:| epoch   1 | step  78 | batch  78/352 | lr 0.00020 | loss 0.4403 | s/batch 11.17\n",
      "INFO:root:| epoch   1 | step  81 | batch  81/352 | lr 0.00020 | loss 0.4025 | s/batch 4.30\n",
      "INFO:root:| epoch   1 | step  84 | batch  84/352 | lr 0.00020 | loss 0.4428 | s/batch 7.97\n",
      "INFO:root:| epoch   1 | step  87 | batch  87/352 | lr 0.00020 | loss 0.4805 | s/batch 9.25\n",
      "INFO:root:| epoch   1 | step  90 | batch  90/352 | lr 0.00020 | loss 0.4192 | s/batch 10.20\n",
      "INFO:root:| epoch   1 | step  93 | batch  93/352 | lr 0.00020 | loss 0.4035 | s/batch 4.72\n",
      "INFO:root:| epoch   1 | step  96 | batch  96/352 | lr 0.00020 | loss 0.3408 | s/batch 5.68\n",
      "INFO:root:| epoch   1 | step  99 | batch  99/352 | lr 0.00020 | loss 0.3787 | s/batch 5.02\n",
      "INFO:root:| epoch   1 | step 102 | batch 102/352 | lr 0.00020 | loss 0.3494 | s/batch 5.24\n",
      "INFO:root:| epoch   1 | step 105 | batch 105/352 | lr 0.00020 | loss 0.3903 | s/batch 12.12\n",
      "INFO:root:| epoch   1 | step 108 | batch 108/352 | lr 0.00020 | loss 0.3852 | s/batch 11.09\n",
      "INFO:root:| epoch   1 | step 111 | batch 111/352 | lr 0.00020 | loss 0.3582 | s/batch 5.84\n",
      "INFO:root:| epoch   1 | step 114 | batch 114/352 | lr 0.00020 | loss 0.3849 | s/batch 16.02\n",
      "INFO:root:| epoch   1 | step 117 | batch 117/352 | lr 0.00020 | loss 0.4503 | s/batch 3.71\n",
      "INFO:root:| epoch   1 | step 120 | batch 120/352 | lr 0.00020 | loss 0.4110 | s/batch 5.12\n",
      "INFO:root:| epoch   1 | step 123 | batch 123/352 | lr 0.00020 | loss 0.4286 | s/batch 3.98\n",
      "INFO:root:| epoch   1 | step 126 | batch 126/352 | lr 0.00020 | loss 0.4292 | s/batch 9.70\n",
      "INFO:root:| epoch   1 | step 129 | batch 129/352 | lr 0.00020 | loss 0.2850 | s/batch 5.66\n",
      "INFO:root:| epoch   1 | step 132 | batch 132/352 | lr 0.00020 | loss 0.3963 | s/batch 12.41\n",
      "INFO:root:| epoch   1 | step 135 | batch 135/352 | lr 0.00020 | loss 0.3596 | s/batch 8.67\n",
      "INFO:root:| epoch   1 | step 138 | batch 138/352 | lr 0.00020 | loss 0.4231 | s/batch 5.02\n",
      "INFO:root:| epoch   1 | step 141 | batch 141/352 | lr 0.00020 | loss 0.3849 | s/batch 5.06\n",
      "INFO:root:| epoch   1 | step 144 | batch 144/352 | lr 0.00020 | loss 0.3532 | s/batch 5.83\n",
      "INFO:root:| epoch   1 | step 147 | batch 147/352 | lr 0.00020 | loss 0.3680 | s/batch 5.07\n",
      "INFO:root:| epoch   1 | step 150 | batch 150/352 | lr 0.00020 | loss 0.3704 | s/batch 5.02\n",
      "INFO:root:| epoch   1 | step 153 | batch 153/352 | lr 0.00020 | loss 0.4054 | s/batch 10.53\n",
      "INFO:root:| epoch   1 | step 156 | batch 156/352 | lr 0.00020 | loss 0.4036 | s/batch 4.46\n",
      "INFO:root:| epoch   1 | step 159 | batch 159/352 | lr 0.00020 | loss 0.4073 | s/batch 18.38\n",
      "INFO:root:| epoch   1 | step 162 | batch 162/352 | lr 0.00020 | loss 0.3918 | s/batch 6.43\n",
      "INFO:root:| epoch   1 | step 165 | batch 165/352 | lr 0.00020 | loss 0.4211 | s/batch 17.01\n",
      "INFO:root:| epoch   1 | step 168 | batch 168/352 | lr 0.00020 | loss 0.3957 | s/batch 7.16\n",
      "INFO:root:| epoch   1 | step 171 | batch 171/352 | lr 0.00020 | loss 0.3829 | s/batch 10.34\n",
      "INFO:root:| epoch   1 | step 174 | batch 174/352 | lr 0.00020 | loss 0.4018 | s/batch 12.35\n",
      "INFO:root:| epoch   1 | step 177 | batch 177/352 | lr 0.00020 | loss 0.3552 | s/batch 5.05\n",
      "INFO:root:| epoch   1 | step 180 | batch 180/352 | lr 0.00020 | loss 0.4170 | s/batch 8.61\n",
      "INFO:root:| epoch   1 | step 183 | batch 183/352 | lr 0.00020 | loss 0.3488 | s/batch 5.19\n",
      "INFO:root:| epoch   1 | step 186 | batch 186/352 | lr 0.00020 | loss 0.3840 | s/batch 9.68\n",
      "INFO:root:| epoch   1 | step 189 | batch 189/352 | lr 0.00020 | loss 0.3697 | s/batch 9.48\n",
      "INFO:root:| epoch   1 | step 192 | batch 192/352 | lr 0.00020 | loss 0.3677 | s/batch 4.54\n",
      "INFO:root:| epoch   1 | step 195 | batch 195/352 | lr 0.00020 | loss 0.4538 | s/batch 8.97\n",
      "INFO:root:| epoch   1 | step 198 | batch 198/352 | lr 0.00020 | loss 0.4648 | s/batch 9.24\n",
      "INFO:root:| epoch   1 | step 201 | batch 201/352 | lr 0.00020 | loss 0.3267 | s/batch 7.42\n",
      "INFO:root:| epoch   1 | step 204 | batch 204/352 | lr 0.00020 | loss 0.3932 | s/batch 8.64\n",
      "INFO:root:| epoch   1 | step 207 | batch 207/352 | lr 0.00020 | loss 0.3383 | s/batch 8.53\n",
      "INFO:root:| epoch   1 | step 210 | batch 210/352 | lr 0.00020 | loss 0.4064 | s/batch 11.07\n",
      "INFO:root:| epoch   1 | step 213 | batch 213/352 | lr 0.00020 | loss 0.4314 | s/batch 9.29\n",
      "INFO:root:| epoch   1 | step 216 | batch 216/352 | lr 0.00020 | loss 0.4012 | s/batch 11.26\n",
      "INFO:root:| epoch   1 | step 219 | batch 219/352 | lr 0.00020 | loss 0.3350 | s/batch 6.35\n",
      "INFO:root:| epoch   1 | step 222 | batch 222/352 | lr 0.00020 | loss 0.3439 | s/batch 9.99\n",
      "INFO:root:| epoch   1 | step 225 | batch 225/352 | lr 0.00020 | loss 0.3667 | s/batch 10.42\n",
      "INFO:root:| epoch   1 | step 228 | batch 228/352 | lr 0.00020 | loss 0.3364 | s/batch 11.46\n",
      "INFO:root:| epoch   1 | step 231 | batch 231/352 | lr 0.00020 | loss 0.3907 | s/batch 2.78\n",
      "INFO:root:| epoch   1 | step 234 | batch 234/352 | lr 0.00020 | loss 0.4131 | s/batch 5.39\n",
      "INFO:root:| epoch   1 | step 237 | batch 237/352 | lr 0.00020 | loss 0.3690 | s/batch 5.74\n",
      "INFO:root:| epoch   1 | step 240 | batch 240/352 | lr 0.00020 | loss 0.3769 | s/batch 3.86\n",
      "INFO:root:| epoch   1 | step 243 | batch 243/352 | lr 0.00020 | loss 0.3561 | s/batch 4.38\n",
      "INFO:root:| epoch   1 | step 246 | batch 246/352 | lr 0.00020 | loss 0.3942 | s/batch 16.40\n",
      "INFO:root:| epoch   1 | step 249 | batch 249/352 | lr 0.00020 | loss 0.3428 | s/batch 9.14\n",
      "INFO:root:| epoch   1 | step 252 | batch 252/352 | lr 0.00020 | loss 0.3791 | s/batch 5.69\n",
      "INFO:root:| epoch   1 | step 255 | batch 255/352 | lr 0.00020 | loss 0.4146 | s/batch 9.82\n",
      "INFO:root:| epoch   1 | step 258 | batch 258/352 | lr 0.00020 | loss 0.3387 | s/batch 7.09\n",
      "INFO:root:| epoch   1 | step 261 | batch 261/352 | lr 0.00020 | loss 0.3259 | s/batch 5.04\n",
      "INFO:root:| epoch   1 | step 264 | batch 264/352 | lr 0.00020 | loss 0.3525 | s/batch 5.05\n",
      "INFO:root:| epoch   1 | step 267 | batch 267/352 | lr 0.00020 | loss 0.3675 | s/batch 3.62\n",
      "INFO:root:| epoch   1 | step 270 | batch 270/352 | lr 0.00020 | loss 0.3932 | s/batch 2.19\n",
      "INFO:root:| epoch   1 | step 273 | batch 273/352 | lr 0.00020 | loss 0.3923 | s/batch 12.19\n",
      "INFO:root:| epoch   1 | step 276 | batch 276/352 | lr 0.00020 | loss 0.3893 | s/batch 9.38\n",
      "INFO:root:| epoch   1 | step 279 | batch 279/352 | lr 0.00020 | loss 0.3490 | s/batch 8.26\n",
      "INFO:root:| epoch   1 | step 282 | batch 282/352 | lr 0.00020 | loss 0.3394 | s/batch 15.38\n",
      "INFO:root:| epoch   1 | step 285 | batch 285/352 | lr 0.00020 | loss 0.3991 | s/batch 4.03\n",
      "INFO:root:| epoch   1 | step 288 | batch 288/352 | lr 0.00020 | loss 0.4106 | s/batch 9.23\n",
      "INFO:root:| epoch   1 | step 291 | batch 291/352 | lr 0.00020 | loss 0.3579 | s/batch 6.29\n",
      "INFO:root:| epoch   1 | step 294 | batch 294/352 | lr 0.00020 | loss 0.3984 | s/batch 8.21\n",
      "INFO:root:| epoch   1 | step 297 | batch 297/352 | lr 0.00020 | loss 0.3857 | s/batch 9.98\n",
      "INFO:root:| epoch   1 | step 300 | batch 300/352 | lr 0.00020 | loss 0.3632 | s/batch 5.13\n",
      "INFO:root:| epoch   1 | step 303 | batch 303/352 | lr 0.00020 | loss 0.3302 | s/batch 10.53\n",
      "INFO:root:| epoch   1 | step 306 | batch 306/352 | lr 0.00020 | loss 0.3561 | s/batch 12.18\n",
      "INFO:root:| epoch   1 | step 309 | batch 309/352 | lr 0.00020 | loss 0.3728 | s/batch 3.67\n",
      "INFO:root:| epoch   1 | step 312 | batch 312/352 | lr 0.00020 | loss 0.2924 | s/batch 9.88\n",
      "INFO:root:| epoch   1 | step 315 | batch 315/352 | lr 0.00020 | loss 0.4106 | s/batch 8.80\n",
      "INFO:root:| epoch   1 | step 318 | batch 318/352 | lr 0.00020 | loss 0.3522 | s/batch 3.84\n",
      "INFO:root:| epoch   1 | step 321 | batch 321/352 | lr 0.00020 | loss 0.3261 | s/batch 6.91\n",
      "INFO:root:| epoch   1 | step 324 | batch 324/352 | lr 0.00020 | loss 0.3508 | s/batch 6.35\n",
      "INFO:root:| epoch   1 | step 327 | batch 327/352 | lr 0.00020 | loss 0.3926 | s/batch 8.70\n",
      "INFO:root:| epoch   1 | step 330 | batch 330/352 | lr 0.00020 | loss 0.3368 | s/batch 9.42\n",
      "INFO:root:| epoch   1 | step 333 | batch 333/352 | lr 0.00020 | loss 0.3919 | s/batch 12.61\n",
      "INFO:root:| epoch   1 | step 336 | batch 336/352 | lr 0.00020 | loss 0.3833 | s/batch 9.17\n",
      "INFO:root:| epoch   1 | step 339 | batch 339/352 | lr 0.00020 | loss 0.4094 | s/batch 10.32\n",
      "INFO:root:| epoch   1 | step 342 | batch 342/352 | lr 0.00020 | loss 0.3620 | s/batch 6.06\n",
      "INFO:root:| epoch   1 | step 345 | batch 345/352 | lr 0.00020 | loss 0.3667 | s/batch 3.28\n",
      "INFO:root:| epoch   1 | step 348 | batch 348/352 | lr 0.00020 | loss 0.3328 | s/batch 9.95\n",
      "INFO:root:| epoch   1 | step 351 | batch 351/352 | lr 0.00020 | loss 0.3373 | s/batch 10.93\n",
      "INFO:root:| epoch   1 | score (84.46, 79.43, 81.42) | f1 81.42 | loss 0.3908 | time 2831.20\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.8870    0.8919    0.8894     35027\n",
      "          股票     0.8913    0.9189    0.9049     33251\n",
      "          体育     0.9607    0.9707    0.9657     28283\n",
      "          娱乐     0.8915    0.9065    0.8989     19920\n",
      "          时政     0.8059    0.8408    0.8230     13515\n",
      "          社会     0.7915    0.7978    0.7947     11009\n",
      "          教育     0.9001    0.8838    0.8919      8987\n",
      "          财经     0.8049    0.6944    0.7456      7957\n",
      "          家居     0.7859    0.8026    0.7942      7063\n",
      "          游戏     0.8604    0.8163    0.8378      5285\n",
      "          房产     0.8969    0.8607    0.8784      4428\n",
      "          时尚     0.6999    0.6498    0.6739      2818\n",
      "          彩票     0.8750    0.6663    0.7565      1639\n",
      "          星座     0.7725    0.4193    0.5436       818\n",
      "\n",
      "    accuracy                         0.8777    180000\n",
      "   macro avg     0.8446    0.7943    0.8142    180000\n",
      "weighted avg     0.8769    0.8777    0.8766    180000\n",
      "\n",
      "INFO:root:| epoch   1 | dev | score (90.54, 87.65, 88.88) | f1 88.88 | time 260.38\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9280    0.9203    0.9241      3891\n",
      "          股票     0.9380    0.9250    0.9314      3694\n",
      "          体育     0.9792    0.9755    0.9774      3142\n",
      "          娱乐     0.9242    0.9367    0.9304      2213\n",
      "          时政     0.8206    0.8927    0.8551      1501\n",
      "          社会     0.7894    0.8675    0.8266      1223\n",
      "          教育     0.9411    0.8958    0.9179       998\n",
      "          财经     0.8558    0.8258    0.8405       884\n",
      "          家居     0.9137    0.8776    0.8953       784\n",
      "          游戏     0.9299    0.8718    0.8999       593\n",
      "          房产     0.9269    0.9533    0.9399       492\n",
      "          时尚     0.8344    0.8371    0.8357       313\n",
      "          彩票     0.9524    0.7692    0.8511       182\n",
      "          星座     0.9420    0.7222    0.8176        90\n",
      "\n",
      "    accuracy                         0.9151     20000\n",
      "   macro avg     0.9054    0.8765    0.8888     20000\n",
      "weighted avg     0.9167    0.9151    0.9154     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 0.00, current dev = 88.88\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch   2 | step 355 | batch   3/352 | lr 0.00020 | loss 0.3439 | s/batch 3.83\n",
      "INFO:root:| epoch   2 | step 358 | batch   6/352 | lr 0.00020 | loss 0.3537 | s/batch 11.23\n",
      "INFO:root:| epoch   2 | step 361 | batch   9/352 | lr 0.00020 | loss 0.4088 | s/batch 8.34\n",
      "INFO:root:| epoch   2 | step 364 | batch  12/352 | lr 0.00020 | loss 0.3442 | s/batch 5.38\n",
      "INFO:root:| epoch   2 | step 367 | batch  15/352 | lr 0.00020 | loss 0.3368 | s/batch 11.90\n",
      "INFO:root:| epoch   2 | step 370 | batch  18/352 | lr 0.00020 | loss 0.3733 | s/batch 3.48\n",
      "INFO:root:| epoch   2 | step 373 | batch  21/352 | lr 0.00020 | loss 0.3910 | s/batch 9.02\n",
      "INFO:root:| epoch   2 | step 376 | batch  24/352 | lr 0.00020 | loss 0.2935 | s/batch 7.15\n",
      "INFO:root:| epoch   2 | step 379 | batch  27/352 | lr 0.00020 | loss 0.3515 | s/batch 9.82\n",
      "INFO:root:| epoch   2 | step 382 | batch  30/352 | lr 0.00020 | loss 0.3539 | s/batch 6.49\n",
      "INFO:root:| epoch   2 | step 385 | batch  33/352 | lr 0.00020 | loss 0.3628 | s/batch 11.82\n",
      "INFO:root:| epoch   2 | step 388 | batch  36/352 | lr 0.00020 | loss 0.3200 | s/batch 7.23\n",
      "INFO:root:| epoch   2 | step 391 | batch  39/352 | lr 0.00020 | loss 0.3673 | s/batch 3.73\n",
      "INFO:root:| epoch   2 | step 394 | batch  42/352 | lr 0.00020 | loss 0.3601 | s/batch 4.00\n",
      "INFO:root:| epoch   2 | step 397 | batch  45/352 | lr 0.00020 | loss 0.4244 | s/batch 1.80\n",
      "INFO:root:| epoch   2 | step 400 | batch  48/352 | lr 0.00020 | loss 0.3038 | s/batch 4.98\n",
      "INFO:root:| epoch   2 | step 403 | batch  51/352 | lr 0.00020 | loss 0.3204 | s/batch 5.19\n",
      "INFO:root:| epoch   2 | step 406 | batch  54/352 | lr 0.00020 | loss 0.3429 | s/batch 4.48\n",
      "INFO:root:| epoch   2 | step 409 | batch  57/352 | lr 0.00020 | loss 0.3606 | s/batch 4.58\n",
      "INFO:root:| epoch   2 | step 412 | batch  60/352 | lr 0.00020 | loss 0.3905 | s/batch 9.30\n",
      "INFO:root:| epoch   2 | step 415 | batch  63/352 | lr 0.00020 | loss 0.3687 | s/batch 9.21\n",
      "INFO:root:| epoch   2 | step 418 | batch  66/352 | lr 0.00020 | loss 0.3249 | s/batch 3.48\n",
      "INFO:root:| epoch   2 | step 421 | batch  69/352 | lr 0.00020 | loss 0.3546 | s/batch 2.74\n",
      "INFO:root:| epoch   2 | step 424 | batch  72/352 | lr 0.00020 | loss 0.3662 | s/batch 4.65\n",
      "INFO:root:| epoch   2 | step 427 | batch  75/352 | lr 0.00020 | loss 0.3393 | s/batch 12.50\n",
      "INFO:root:| epoch   2 | step 430 | batch  78/352 | lr 0.00020 | loss 0.3243 | s/batch 7.96\n",
      "INFO:root:| epoch   2 | step 433 | batch  81/352 | lr 0.00020 | loss 0.3612 | s/batch 12.37\n",
      "INFO:root:| epoch   2 | step 436 | batch  84/352 | lr 0.00020 | loss 0.3872 | s/batch 11.46\n",
      "INFO:root:| epoch   2 | step 439 | batch  87/352 | lr 0.00020 | loss 0.3994 | s/batch 2.56\n",
      "INFO:root:| epoch   2 | step 442 | batch  90/352 | lr 0.00020 | loss 0.3171 | s/batch 7.14\n",
      "INFO:root:| epoch   2 | step 445 | batch  93/352 | lr 0.00020 | loss 0.3463 | s/batch 3.88\n",
      "INFO:root:| epoch   2 | step 448 | batch  96/352 | lr 0.00020 | loss 0.3413 | s/batch 2.75\n",
      "INFO:root:| epoch   2 | step 451 | batch  99/352 | lr 0.00020 | loss 0.3500 | s/batch 8.19\n",
      "INFO:root:| epoch   2 | step 454 | batch 102/352 | lr 0.00020 | loss 0.3925 | s/batch 15.66\n",
      "INFO:root:| epoch   2 | step 457 | batch 105/352 | lr 0.00020 | loss 0.4156 | s/batch 9.00\n",
      "INFO:root:| epoch   2 | step 460 | batch 108/352 | lr 0.00020 | loss 0.3518 | s/batch 9.90\n",
      "INFO:root:| epoch   2 | step 463 | batch 111/352 | lr 0.00020 | loss 0.3219 | s/batch 5.08\n",
      "INFO:root:| epoch   2 | step 466 | batch 114/352 | lr 0.00020 | loss 0.3978 | s/batch 2.18\n",
      "INFO:root:| epoch   2 | step 469 | batch 117/352 | lr 0.00020 | loss 0.3698 | s/batch 7.23\n",
      "INFO:root:| epoch   2 | step 472 | batch 120/352 | lr 0.00020 | loss 0.3339 | s/batch 5.73\n",
      "INFO:root:| epoch   2 | step 475 | batch 123/352 | lr 0.00020 | loss 0.3696 | s/batch 11.08\n",
      "INFO:root:| epoch   2 | step 478 | batch 126/352 | lr 0.00020 | loss 0.3028 | s/batch 13.87\n",
      "INFO:root:| epoch   2 | step 481 | batch 129/352 | lr 0.00020 | loss 0.3218 | s/batch 5.09\n",
      "INFO:root:| epoch   2 | step 484 | batch 132/352 | lr 0.00020 | loss 0.3433 | s/batch 3.92\n",
      "INFO:root:| epoch   2 | step 487 | batch 135/352 | lr 0.00020 | loss 0.4178 | s/batch 7.59\n",
      "INFO:root:| epoch   2 | step 490 | batch 138/352 | lr 0.00020 | loss 0.3360 | s/batch 5.00\n",
      "INFO:root:| epoch   2 | step 493 | batch 141/352 | lr 0.00020 | loss 0.3912 | s/batch 16.36\n",
      "INFO:root:| epoch   2 | step 496 | batch 144/352 | lr 0.00020 | loss 0.3657 | s/batch 8.49\n",
      "INFO:root:| epoch   2 | step 499 | batch 147/352 | lr 0.00020 | loss 0.3191 | s/batch 8.47\n",
      "INFO:root:| epoch   2 | step 502 | batch 150/352 | lr 0.00020 | loss 0.3649 | s/batch 8.78\n",
      "INFO:root:| epoch   2 | step 505 | batch 153/352 | lr 0.00020 | loss 0.3433 | s/batch 3.77\n",
      "INFO:root:| epoch   2 | step 508 | batch 156/352 | lr 0.00020 | loss 0.3701 | s/batch 2.99\n",
      "INFO:root:| epoch   2 | step 511 | batch 159/352 | lr 0.00020 | loss 0.2978 | s/batch 5.64\n",
      "INFO:root:| epoch   2 | step 514 | batch 162/352 | lr 0.00020 | loss 0.3445 | s/batch 9.83\n",
      "INFO:root:| epoch   2 | step 517 | batch 165/352 | lr 0.00020 | loss 0.3443 | s/batch 11.75\n",
      "INFO:root:| epoch   2 | step 520 | batch 168/352 | lr 0.00020 | loss 0.2977 | s/batch 6.93\n",
      "INFO:root:| epoch   2 | step 523 | batch 171/352 | lr 0.00020 | loss 0.3672 | s/batch 9.56\n",
      "INFO:root:| epoch   2 | step 526 | batch 174/352 | lr 0.00020 | loss 0.3475 | s/batch 9.09\n",
      "INFO:root:| epoch   2 | step 529 | batch 177/352 | lr 0.00020 | loss 0.2789 | s/batch 8.65\n",
      "INFO:root:| epoch   2 | step 532 | batch 180/352 | lr 0.00020 | loss 0.3258 | s/batch 7.62\n",
      "INFO:root:| epoch   2 | step 535 | batch 183/352 | lr 0.00020 | loss 0.3252 | s/batch 12.20\n",
      "INFO:root:| epoch   2 | step 538 | batch 186/352 | lr 0.00020 | loss 0.3358 | s/batch 6.37\n",
      "INFO:root:| epoch   2 | step 541 | batch 189/352 | lr 0.00020 | loss 0.3833 | s/batch 12.88\n",
      "INFO:root:| epoch   2 | step 544 | batch 192/352 | lr 0.00020 | loss 0.4033 | s/batch 11.26\n",
      "INFO:root:| epoch   2 | step 547 | batch 195/352 | lr 0.00020 | loss 0.3147 | s/batch 5.59\n",
      "INFO:root:| epoch   2 | step 550 | batch 198/352 | lr 0.00020 | loss 0.3175 | s/batch 3.69\n",
      "INFO:root:| epoch   2 | step 553 | batch 201/352 | lr 0.00020 | loss 0.3645 | s/batch 9.10\n",
      "INFO:root:| epoch   2 | step 556 | batch 204/352 | lr 0.00020 | loss 0.3492 | s/batch 9.60\n",
      "INFO:root:| epoch   2 | step 559 | batch 207/352 | lr 0.00020 | loss 0.3210 | s/batch 4.21\n",
      "INFO:root:| epoch   2 | step 562 | batch 210/352 | lr 0.00020 | loss 0.3940 | s/batch 9.97\n",
      "INFO:root:| epoch   2 | step 565 | batch 213/352 | lr 0.00020 | loss 0.3156 | s/batch 5.09\n",
      "INFO:root:| epoch   2 | step 568 | batch 216/352 | lr 0.00020 | loss 0.3517 | s/batch 11.24\n",
      "INFO:root:| epoch   2 | step 571 | batch 219/352 | lr 0.00020 | loss 0.3140 | s/batch 4.95\n",
      "INFO:root:| epoch   2 | step 574 | batch 222/352 | lr 0.00020 | loss 0.3469 | s/batch 10.87\n",
      "INFO:root:| epoch   2 | step 577 | batch 225/352 | lr 0.00020 | loss 0.3011 | s/batch 6.29\n",
      "INFO:root:| epoch   2 | step 580 | batch 228/352 | lr 0.00020 | loss 0.3065 | s/batch 5.83\n",
      "INFO:root:| epoch   2 | step 583 | batch 231/352 | lr 0.00020 | loss 0.3534 | s/batch 3.01\n",
      "INFO:root:| epoch   2 | step 586 | batch 234/352 | lr 0.00020 | loss 0.3848 | s/batch 9.56\n",
      "INFO:root:| epoch   2 | step 589 | batch 237/352 | lr 0.00020 | loss 0.3042 | s/batch 4.78\n",
      "INFO:root:| epoch   2 | step 592 | batch 240/352 | lr 0.00020 | loss 0.2704 | s/batch 5.66\n",
      "INFO:root:| epoch   2 | step 595 | batch 243/352 | lr 0.00020 | loss 0.2997 | s/batch 10.81\n",
      "INFO:root:| epoch   2 | step 598 | batch 246/352 | lr 0.00020 | loss 0.3080 | s/batch 7.00\n",
      "INFO:root:| epoch   2 | step 601 | batch 249/352 | lr 0.00020 | loss 0.3307 | s/batch 13.29\n",
      "INFO:root:| epoch   2 | step 604 | batch 252/352 | lr 0.00020 | loss 0.2900 | s/batch 8.91\n",
      "INFO:root:| epoch   2 | step 607 | batch 255/352 | lr 0.00020 | loss 0.3274 | s/batch 7.66\n",
      "INFO:root:| epoch   2 | step 610 | batch 258/352 | lr 0.00020 | loss 0.3521 | s/batch 9.95\n",
      "INFO:root:| epoch   2 | step 613 | batch 261/352 | lr 0.00020 | loss 0.3224 | s/batch 10.35\n",
      "INFO:root:| epoch   2 | step 616 | batch 264/352 | lr 0.00020 | loss 0.2982 | s/batch 4.40\n",
      "INFO:root:| epoch   2 | step 619 | batch 267/352 | lr 0.00020 | loss 0.3748 | s/batch 14.31\n",
      "INFO:root:| epoch   2 | step 622 | batch 270/352 | lr 0.00020 | loss 0.3045 | s/batch 9.20\n",
      "INFO:root:| epoch   2 | step 625 | batch 273/352 | lr 0.00020 | loss 0.3368 | s/batch 10.44\n",
      "INFO:root:| epoch   2 | step 628 | batch 276/352 | lr 0.00020 | loss 0.3394 | s/batch 10.05\n",
      "INFO:root:| epoch   2 | step 631 | batch 279/352 | lr 0.00020 | loss 0.3040 | s/batch 5.58\n",
      "INFO:root:| epoch   2 | step 634 | batch 282/352 | lr 0.00020 | loss 0.3179 | s/batch 3.67\n",
      "INFO:root:| epoch   2 | step 637 | batch 285/352 | lr 0.00020 | loss 0.3423 | s/batch 7.70\n",
      "INFO:root:| epoch   2 | step 640 | batch 288/352 | lr 0.00020 | loss 0.3772 | s/batch 13.40\n",
      "INFO:root:| epoch   2 | step 643 | batch 291/352 | lr 0.00020 | loss 0.3688 | s/batch 3.86\n",
      "INFO:root:| epoch   2 | step 646 | batch 294/352 | lr 0.00020 | loss 0.3191 | s/batch 7.71\n",
      "INFO:root:| epoch   2 | step 649 | batch 297/352 | lr 0.00020 | loss 0.2979 | s/batch 4.20\n",
      "INFO:root:| epoch   2 | step 652 | batch 300/352 | lr 0.00020 | loss 0.3757 | s/batch 16.93\n",
      "INFO:root:| epoch   2 | step 655 | batch 303/352 | lr 0.00020 | loss 0.3239 | s/batch 8.91\n",
      "INFO:root:| epoch   2 | step 658 | batch 306/352 | lr 0.00020 | loss 0.3414 | s/batch 9.61\n",
      "INFO:root:| epoch   2 | step 661 | batch 309/352 | lr 0.00020 | loss 0.3096 | s/batch 10.27\n",
      "INFO:root:| epoch   2 | step 664 | batch 312/352 | lr 0.00020 | loss 0.4172 | s/batch 9.93\n",
      "INFO:root:| epoch   2 | step 667 | batch 315/352 | lr 0.00020 | loss 0.3026 | s/batch 4.38\n",
      "INFO:root:| epoch   2 | step 670 | batch 318/352 | lr 0.00020 | loss 0.2963 | s/batch 8.31\n",
      "INFO:root:| epoch   2 | step 673 | batch 321/352 | lr 0.00020 | loss 0.3579 | s/batch 13.71\n",
      "INFO:root:| epoch   2 | step 676 | batch 324/352 | lr 0.00020 | loss 0.3609 | s/batch 12.24\n",
      "INFO:root:| epoch   2 | step 679 | batch 327/352 | lr 0.00020 | loss 0.3424 | s/batch 4.69\n",
      "INFO:root:| epoch   2 | step 682 | batch 330/352 | lr 0.00020 | loss 0.3057 | s/batch 4.52\n",
      "INFO:root:| epoch   2 | step 685 | batch 333/352 | lr 0.00020 | loss 0.3389 | s/batch 3.12\n",
      "INFO:root:| epoch   2 | step 688 | batch 336/352 | lr 0.00020 | loss 0.3115 | s/batch 11.38\n",
      "INFO:root:| epoch   2 | step 691 | batch 339/352 | lr 0.00020 | loss 0.2984 | s/batch 9.82\n",
      "INFO:root:| epoch   2 | step 694 | batch 342/352 | lr 0.00020 | loss 0.3410 | s/batch 10.30\n",
      "INFO:root:| epoch   2 | step 697 | batch 345/352 | lr 0.00020 | loss 0.2747 | s/batch 8.95\n",
      "INFO:root:| epoch   2 | step 700 | batch 348/352 | lr 0.00020 | loss 0.3378 | s/batch 11.27\n",
      "INFO:root:| epoch   2 | step 703 | batch 351/352 | lr 0.00020 | loss 0.3178 | s/batch 10.48\n",
      "INFO:root:| epoch   2 | score (86.38, 83.47, 84.79) | f1 84.79 | loss 0.3419 | time 2787.16\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.8992    0.9022    0.9007     35027\n",
      "          股票     0.9046    0.9256    0.9150     33251\n",
      "          体育     0.9679    0.9734    0.9706     28283\n",
      "          娱乐     0.9057    0.9165    0.9111     19920\n",
      "          时政     0.8207    0.8577    0.8388     13515\n",
      "          社会     0.8064    0.8039    0.8051     11009\n",
      "          教育     0.9130    0.8976    0.9052      8987\n",
      "          财经     0.8276    0.7358    0.7790      7957\n",
      "          家居     0.8285    0.8290    0.8287      7063\n",
      "          游戏     0.8767    0.8369    0.8563      5285\n",
      "          房产     0.9180    0.8902    0.9039      4428\n",
      "          时尚     0.7563    0.7402    0.7482      2818\n",
      "          彩票     0.8907    0.7706    0.8263      1639\n",
      "          星座     0.7774    0.6064    0.6813       818\n",
      "\n",
      "    accuracy                         0.8922    180000\n",
      "   macro avg     0.8638    0.8347    0.8479    180000\n",
      "weighted avg     0.8918    0.8922    0.8917    180000\n",
      "\n",
      "INFO:root:| epoch   2 | dev | score (91.24, 89.15, 90.04) | f1 90.04 | time 256.40\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9342    0.9193    0.9267      3891\n",
      "          股票     0.9372    0.9331    0.9352      3694\n",
      "          体育     0.9787    0.9803    0.9795      3142\n",
      "          娱乐     0.9474    0.9282    0.9377      2213\n",
      "          时政     0.8342    0.8947    0.8634      1501\n",
      "          社会     0.7978    0.8741    0.8342      1223\n",
      "          教育     0.9319    0.9188    0.9253       998\n",
      "          财经     0.8391    0.8552    0.8471       884\n",
      "          家居     0.9209    0.8916    0.9060       784\n",
      "          游戏     0.9497    0.8600    0.9027       593\n",
      "          房产     0.9710    0.9512    0.9610       492\n",
      "          时尚     0.8234    0.8786    0.8501       313\n",
      "          彩票     0.9497    0.8297    0.8856       182\n",
      "          星座     0.9583    0.7667    0.8519        90\n",
      "\n",
      "    accuracy                         0.9207     20000\n",
      "   macro avg     0.9124    0.8915    0.9004     20000\n",
      "weighted avg     0.9224    0.9207    0.9212     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 88.88, current dev = 90.04\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch   3 | step 707 | batch   3/352 | lr 0.00020 | loss 0.3891 | s/batch 14.04\n",
      "INFO:root:| epoch   3 | step 710 | batch   6/352 | lr 0.00020 | loss 0.3142 | s/batch 10.20\n",
      "INFO:root:| epoch   3 | step 713 | batch   9/352 | lr 0.00020 | loss 0.3175 | s/batch 6.31\n",
      "INFO:root:| epoch   3 | step 716 | batch  12/352 | lr 0.00020 | loss 0.3429 | s/batch 9.45\n",
      "INFO:root:| epoch   3 | step 719 | batch  15/352 | lr 0.00020 | loss 0.3272 | s/batch 11.66\n",
      "INFO:root:| epoch   3 | step 722 | batch  18/352 | lr 0.00020 | loss 0.3006 | s/batch 8.35\n",
      "INFO:root:| epoch   3 | step 725 | batch  21/352 | lr 0.00020 | loss 0.3578 | s/batch 1.84\n",
      "INFO:root:| epoch   3 | step 728 | batch  24/352 | lr 0.00020 | loss 0.3075 | s/batch 6.56\n",
      "INFO:root:| epoch   3 | step 731 | batch  27/352 | lr 0.00020 | loss 0.3505 | s/batch 13.47\n",
      "INFO:root:| epoch   3 | step 734 | batch  30/352 | lr 0.00020 | loss 0.3264 | s/batch 3.85\n",
      "INFO:root:| epoch   3 | step 737 | batch  33/352 | lr 0.00020 | loss 0.3253 | s/batch 5.13\n",
      "INFO:root:| epoch   3 | step 740 | batch  36/352 | lr 0.00020 | loss 0.3553 | s/batch 5.97\n",
      "INFO:root:| epoch   3 | step 743 | batch  39/352 | lr 0.00020 | loss 0.3182 | s/batch 7.73\n",
      "INFO:root:| epoch   3 | step 746 | batch  42/352 | lr 0.00020 | loss 0.3153 | s/batch 9.39\n",
      "INFO:root:| epoch   3 | step 749 | batch  45/352 | lr 0.00020 | loss 0.2943 | s/batch 6.58\n",
      "INFO:root:| epoch   3 | step 752 | batch  48/352 | lr 0.00020 | loss 0.3036 | s/batch 3.46\n",
      "INFO:root:| epoch   3 | step 755 | batch  51/352 | lr 0.00020 | loss 0.3117 | s/batch 12.13\n",
      "INFO:root:| epoch   3 | step 758 | batch  54/352 | lr 0.00020 | loss 0.3358 | s/batch 3.01\n",
      "INFO:root:| epoch   3 | step 761 | batch  57/352 | lr 0.00020 | loss 0.3241 | s/batch 9.98\n",
      "INFO:root:| epoch   3 | step 764 | batch  60/352 | lr 0.00020 | loss 0.3082 | s/batch 5.90\n",
      "INFO:root:| epoch   3 | step 767 | batch  63/352 | lr 0.00020 | loss 0.3520 | s/batch 9.32\n",
      "INFO:root:| epoch   3 | step 770 | batch  66/352 | lr 0.00020 | loss 0.3370 | s/batch 11.15\n",
      "INFO:root:| epoch   3 | step 773 | batch  69/352 | lr 0.00020 | loss 0.2744 | s/batch 5.00\n",
      "INFO:root:| epoch   3 | step 776 | batch  72/352 | lr 0.00020 | loss 0.2754 | s/batch 6.22\n",
      "INFO:root:| epoch   3 | step 779 | batch  75/352 | lr 0.00020 | loss 0.3633 | s/batch 13.03\n",
      "INFO:root:| epoch   3 | step 782 | batch  78/352 | lr 0.00020 | loss 0.2864 | s/batch 8.79\n",
      "INFO:root:| epoch   3 | step 785 | batch  81/352 | lr 0.00020 | loss 0.3320 | s/batch 2.73\n",
      "INFO:root:| epoch   3 | step 788 | batch  84/352 | lr 0.00020 | loss 0.2962 | s/batch 4.90\n",
      "INFO:root:| epoch   3 | step 791 | batch  87/352 | lr 0.00020 | loss 0.3250 | s/batch 14.86\n",
      "INFO:root:| epoch   3 | step 794 | batch  90/352 | lr 0.00020 | loss 0.3581 | s/batch 13.44\n",
      "INFO:root:| epoch   3 | step 797 | batch  93/352 | lr 0.00020 | loss 0.3045 | s/batch 6.95\n",
      "INFO:root:| epoch   3 | step 800 | batch  96/352 | lr 0.00020 | loss 0.3025 | s/batch 3.97\n",
      "INFO:root:| epoch   3 | step 803 | batch  99/352 | lr 0.00020 | loss 0.3273 | s/batch 4.37\n",
      "INFO:root:| epoch   3 | step 806 | batch 102/352 | lr 0.00020 | loss 0.3216 | s/batch 3.94\n",
      "INFO:root:| epoch   3 | step 809 | batch 105/352 | lr 0.00020 | loss 0.3737 | s/batch 9.51\n",
      "INFO:root:| epoch   3 | step 812 | batch 108/352 | lr 0.00020 | loss 0.3452 | s/batch 8.69\n",
      "INFO:root:| epoch   3 | step 815 | batch 111/352 | lr 0.00020 | loss 0.3234 | s/batch 5.58\n",
      "INFO:root:| epoch   3 | step 818 | batch 114/352 | lr 0.00020 | loss 0.2996 | s/batch 7.48\n",
      "INFO:root:| epoch   3 | step 821 | batch 117/352 | lr 0.00020 | loss 0.3380 | s/batch 11.97\n",
      "INFO:root:| epoch   3 | step 824 | batch 120/352 | lr 0.00020 | loss 0.3018 | s/batch 3.89\n",
      "INFO:root:| epoch   3 | step 827 | batch 123/352 | lr 0.00020 | loss 0.3396 | s/batch 6.18\n",
      "INFO:root:| epoch   3 | step 830 | batch 126/352 | lr 0.00020 | loss 0.3064 | s/batch 10.01\n",
      "INFO:root:| epoch   3 | step 833 | batch 129/352 | lr 0.00020 | loss 0.2888 | s/batch 4.33\n",
      "INFO:root:| epoch   3 | step 836 | batch 132/352 | lr 0.00020 | loss 0.2765 | s/batch 5.06\n",
      "INFO:root:| epoch   3 | step 839 | batch 135/352 | lr 0.00020 | loss 0.2874 | s/batch 6.91\n",
      "INFO:root:| epoch   3 | step 842 | batch 138/352 | lr 0.00020 | loss 0.2867 | s/batch 8.27\n",
      "INFO:root:| epoch   3 | step 845 | batch 141/352 | lr 0.00020 | loss 0.3116 | s/batch 13.25\n",
      "INFO:root:| epoch   3 | step 848 | batch 144/352 | lr 0.00020 | loss 0.3208 | s/batch 9.78\n",
      "INFO:root:| epoch   3 | step 851 | batch 147/352 | lr 0.00020 | loss 0.3194 | s/batch 4.30\n",
      "INFO:root:| epoch   3 | step 854 | batch 150/352 | lr 0.00020 | loss 0.3392 | s/batch 14.25\n",
      "INFO:root:| epoch   3 | step 857 | batch 153/352 | lr 0.00020 | loss 0.3269 | s/batch 8.10\n",
      "INFO:root:| epoch   3 | step 860 | batch 156/352 | lr 0.00020 | loss 0.3356 | s/batch 5.77\n",
      "INFO:root:| epoch   3 | step 863 | batch 159/352 | lr 0.00020 | loss 0.3272 | s/batch 3.70\n",
      "INFO:root:| epoch   3 | step 866 | batch 162/352 | lr 0.00020 | loss 0.2834 | s/batch 8.49\n",
      "INFO:root:| epoch   3 | step 869 | batch 165/352 | lr 0.00020 | loss 0.2895 | s/batch 6.58\n",
      "INFO:root:| epoch   3 | step 872 | batch 168/352 | lr 0.00020 | loss 0.2890 | s/batch 4.39\n",
      "INFO:root:| epoch   3 | step 875 | batch 171/352 | lr 0.00020 | loss 0.3288 | s/batch 11.44\n",
      "INFO:root:| epoch   3 | step 878 | batch 174/352 | lr 0.00020 | loss 0.3339 | s/batch 4.50\n",
      "INFO:root:| epoch   3 | step 881 | batch 177/352 | lr 0.00020 | loss 0.2981 | s/batch 4.43\n",
      "INFO:root:| epoch   3 | step 884 | batch 180/352 | lr 0.00020 | loss 0.2975 | s/batch 3.15\n",
      "INFO:root:| epoch   3 | step 887 | batch 183/352 | lr 0.00020 | loss 0.3510 | s/batch 9.10\n",
      "INFO:root:| epoch   3 | step 890 | batch 186/352 | lr 0.00020 | loss 0.2783 | s/batch 12.15\n",
      "INFO:root:| epoch   3 | step 893 | batch 189/352 | lr 0.00020 | loss 0.3074 | s/batch 11.58\n",
      "INFO:root:| epoch   3 | step 896 | batch 192/352 | lr 0.00020 | loss 0.3443 | s/batch 10.67\n",
      "INFO:root:| epoch   3 | step 899 | batch 195/352 | lr 0.00020 | loss 0.2840 | s/batch 6.50\n",
      "INFO:root:| epoch   3 | step 902 | batch 198/352 | lr 0.00020 | loss 0.3046 | s/batch 8.88\n",
      "INFO:root:| epoch   3 | step 905 | batch 201/352 | lr 0.00020 | loss 0.2649 | s/batch 7.50\n",
      "INFO:root:| epoch   3 | step 908 | batch 204/352 | lr 0.00020 | loss 0.3461 | s/batch 9.57\n",
      "INFO:root:| epoch   3 | step 911 | batch 207/352 | lr 0.00020 | loss 0.2623 | s/batch 8.26\n",
      "INFO:root:| epoch   3 | step 914 | batch 210/352 | lr 0.00020 | loss 0.3322 | s/batch 8.74\n",
      "INFO:root:| epoch   3 | step 917 | batch 213/352 | lr 0.00020 | loss 0.2801 | s/batch 4.53\n",
      "INFO:root:| epoch   3 | step 920 | batch 216/352 | lr 0.00020 | loss 0.3477 | s/batch 10.29\n",
      "INFO:root:| epoch   3 | step 923 | batch 219/352 | lr 0.00020 | loss 0.3545 | s/batch 15.36\n",
      "INFO:root:| epoch   3 | step 926 | batch 222/352 | lr 0.00020 | loss 0.2822 | s/batch 8.98\n",
      "INFO:root:| epoch   3 | step 929 | batch 225/352 | lr 0.00020 | loss 0.2963 | s/batch 9.34\n",
      "INFO:root:| epoch   3 | step 932 | batch 228/352 | lr 0.00020 | loss 0.3659 | s/batch 16.87\n",
      "INFO:root:| epoch   3 | step 935 | batch 231/352 | lr 0.00020 | loss 0.3211 | s/batch 9.06\n",
      "INFO:root:| epoch   3 | step 938 | batch 234/352 | lr 0.00020 | loss 0.3050 | s/batch 11.59\n",
      "INFO:root:| epoch   3 | step 941 | batch 237/352 | lr 0.00020 | loss 0.3031 | s/batch 9.23\n",
      "INFO:root:| epoch   3 | step 944 | batch 240/352 | lr 0.00020 | loss 0.2795 | s/batch 8.63\n",
      "INFO:root:| epoch   3 | step 947 | batch 243/352 | lr 0.00020 | loss 0.3207 | s/batch 4.90\n",
      "INFO:root:| epoch   3 | step 950 | batch 246/352 | lr 0.00020 | loss 0.3174 | s/batch 14.21\n",
      "INFO:root:| epoch   3 | step 953 | batch 249/352 | lr 0.00020 | loss 0.2842 | s/batch 4.27\n",
      "INFO:root:| epoch   3 | step 956 | batch 252/352 | lr 0.00020 | loss 0.2820 | s/batch 5.12\n",
      "INFO:root:| epoch   3 | step 959 | batch 255/352 | lr 0.00020 | loss 0.3011 | s/batch 6.91\n",
      "INFO:root:| epoch   3 | step 962 | batch 258/352 | lr 0.00020 | loss 0.3047 | s/batch 9.03\n",
      "INFO:root:| epoch   3 | step 965 | batch 261/352 | lr 0.00020 | loss 0.3206 | s/batch 5.19\n",
      "INFO:root:| epoch   3 | step 968 | batch 264/352 | lr 0.00020 | loss 0.3458 | s/batch 3.72\n",
      "INFO:root:| epoch   3 | step 971 | batch 267/352 | lr 0.00020 | loss 0.3027 | s/batch 8.92\n",
      "INFO:root:| epoch   3 | step 974 | batch 270/352 | lr 0.00020 | loss 0.2990 | s/batch 3.69\n",
      "INFO:root:| epoch   3 | step 977 | batch 273/352 | lr 0.00020 | loss 0.2902 | s/batch 6.18\n",
      "INFO:root:| epoch   3 | step 980 | batch 276/352 | lr 0.00020 | loss 0.2910 | s/batch 11.65\n",
      "INFO:root:| epoch   3 | step 983 | batch 279/352 | lr 0.00020 | loss 0.2763 | s/batch 6.94\n",
      "INFO:root:| epoch   3 | step 986 | batch 282/352 | lr 0.00020 | loss 0.3078 | s/batch 3.15\n",
      "INFO:root:| epoch   3 | step 989 | batch 285/352 | lr 0.00020 | loss 0.3476 | s/batch 8.14\n",
      "INFO:root:| epoch   3 | step 992 | batch 288/352 | lr 0.00020 | loss 0.2897 | s/batch 4.49\n",
      "INFO:root:| epoch   3 | step 995 | batch 291/352 | lr 0.00020 | loss 0.2579 | s/batch 5.67\n",
      "INFO:root:| epoch   3 | step 998 | batch 294/352 | lr 0.00020 | loss 0.2536 | s/batch 5.09\n",
      "INFO:root:| epoch   3 | step 1001 | batch 297/352 | lr 0.00015 | loss 0.2852 | s/batch 3.98\n",
      "INFO:root:| epoch   3 | step 1004 | batch 300/352 | lr 0.00015 | loss 0.2883 | s/batch 11.18\n",
      "INFO:root:| epoch   3 | step 1007 | batch 303/352 | lr 0.00015 | loss 0.3043 | s/batch 10.87\n",
      "INFO:root:| epoch   3 | step 1010 | batch 306/352 | lr 0.00015 | loss 0.3016 | s/batch 2.70\n",
      "INFO:root:| epoch   3 | step 1013 | batch 309/352 | lr 0.00015 | loss 0.3465 | s/batch 9.16\n",
      "INFO:root:| epoch   3 | step 1016 | batch 312/352 | lr 0.00015 | loss 0.3418 | s/batch 7.87\n",
      "INFO:root:| epoch   3 | step 1019 | batch 315/352 | lr 0.00015 | loss 0.3268 | s/batch 11.76\n",
      "INFO:root:| epoch   3 | step 1022 | batch 318/352 | lr 0.00015 | loss 0.2731 | s/batch 4.98\n",
      "INFO:root:| epoch   3 | step 1025 | batch 321/352 | lr 0.00015 | loss 0.2897 | s/batch 5.71\n",
      "INFO:root:| epoch   3 | step 1028 | batch 324/352 | lr 0.00015 | loss 0.3431 | s/batch 14.01\n",
      "INFO:root:| epoch   3 | step 1031 | batch 327/352 | lr 0.00015 | loss 0.3437 | s/batch 7.40\n",
      "INFO:root:| epoch   3 | step 1034 | batch 330/352 | lr 0.00015 | loss 0.3125 | s/batch 7.56\n",
      "INFO:root:| epoch   3 | step 1037 | batch 333/352 | lr 0.00015 | loss 0.2899 | s/batch 7.10\n",
      "INFO:root:| epoch   3 | step 1040 | batch 336/352 | lr 0.00015 | loss 0.3172 | s/batch 8.83\n",
      "INFO:root:| epoch   3 | step 1043 | batch 339/352 | lr 0.00015 | loss 0.2771 | s/batch 6.90\n",
      "INFO:root:| epoch   3 | step 1046 | batch 342/352 | lr 0.00015 | loss 0.3320 | s/batch 9.70\n",
      "INFO:root:| epoch   3 | step 1049 | batch 345/352 | lr 0.00015 | loss 0.2944 | s/batch 3.66\n",
      "INFO:root:| epoch   3 | step 1052 | batch 348/352 | lr 0.00015 | loss 0.2692 | s/batch 13.05\n",
      "INFO:root:| epoch   3 | step 1055 | batch 351/352 | lr 0.00015 | loss 0.2744 | s/batch 5.97\n",
      "INFO:root:| epoch   3 | score (87.94, 85.7, 86.75) | f1 86.75 | loss 0.3122 | time 2768.75\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9048    0.9086    0.9067     35027\n",
      "          股票     0.9104    0.9277    0.9189     33251\n",
      "          体育     0.9738    0.9759    0.9749     28283\n",
      "          娱乐     0.9145    0.9255    0.9199     19920\n",
      "          时政     0.8302    0.8659    0.8477     13515\n",
      "          社会     0.8222    0.8177    0.8199     11009\n",
      "          教育     0.9152    0.9016    0.9084      8987\n",
      "          财经     0.8405    0.7556    0.7958      7957\n",
      "          家居     0.8486    0.8485    0.8486      7063\n",
      "          游戏     0.8804    0.8471    0.8635      5285\n",
      "          房产     0.9360    0.9115    0.9236      4428\n",
      "          时尚     0.8028    0.7818    0.7922      2818\n",
      "          彩票     0.9140    0.8237    0.8665      1639\n",
      "          星座     0.8175    0.7066    0.7580       818\n",
      "\n",
      "    accuracy                         0.9009    180000\n",
      "   macro avg     0.8794    0.8570    0.8675    180000\n",
      "weighted avg     0.9007    0.9009    0.9006    180000\n",
      "\n",
      "INFO:root:| epoch   3 | dev | score (92.08, 91.32, 91.66) | f1 91.66 | time 256.95\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9442    0.9226    0.9333      3891\n",
      "          股票     0.9263    0.9494    0.9377      3694\n",
      "          体育     0.9815    0.9815    0.9815      3142\n",
      "          娱乐     0.9463    0.9467    0.9465      2213\n",
      "          时政     0.8434    0.9041    0.8727      1501\n",
      "          社会     0.8643    0.8594    0.8618      1223\n",
      "          教育     0.9204    0.9389    0.9296       998\n",
      "          财经     0.9034    0.7930    0.8446       884\n",
      "          家居     0.9216    0.9145    0.9181       784\n",
      "          游戏     0.9307    0.9056    0.9179       593\n",
      "          房产     0.9621    0.9797    0.9708       492\n",
      "          时尚     0.8688    0.8882    0.8784       313\n",
      "          彩票     0.9486    0.9121    0.9300       182\n",
      "          星座     0.9302    0.8889    0.9091        90\n",
      "\n",
      "    accuracy                         0.9291     20000\n",
      "   macro avg     0.9208    0.9132    0.9166     20000\n",
      "weighted avg     0.9295    0.9291    0.9290     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 90.04, current dev = 91.66\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch   4 | step 1059 | batch   3/352 | lr 0.00015 | loss 0.3390 | s/batch 13.91\n",
      "INFO:root:| epoch   4 | step 1062 | batch   6/352 | lr 0.00015 | loss 0.2678 | s/batch 5.77\n",
      "INFO:root:| epoch   4 | step 1065 | batch   9/352 | lr 0.00015 | loss 0.2832 | s/batch 11.11\n",
      "INFO:root:| epoch   4 | step 1068 | batch  12/352 | lr 0.00015 | loss 0.2900 | s/batch 3.84\n",
      "INFO:root:| epoch   4 | step 1071 | batch  15/352 | lr 0.00015 | loss 0.3043 | s/batch 2.94\n",
      "INFO:root:| epoch   4 | step 1074 | batch  18/352 | lr 0.00015 | loss 0.2817 | s/batch 3.74\n",
      "INFO:root:| epoch   4 | step 1077 | batch  21/352 | lr 0.00015 | loss 0.2569 | s/batch 7.63\n",
      "INFO:root:| epoch   4 | step 1080 | batch  24/352 | lr 0.00015 | loss 0.2891 | s/batch 11.20\n",
      "INFO:root:| epoch   4 | step 1083 | batch  27/352 | lr 0.00015 | loss 0.3039 | s/batch 14.61\n",
      "INFO:root:| epoch   4 | step 1086 | batch  30/352 | lr 0.00015 | loss 0.2937 | s/batch 7.96\n",
      "INFO:root:| epoch   4 | step 1089 | batch  33/352 | lr 0.00015 | loss 0.2494 | s/batch 4.96\n",
      "INFO:root:| epoch   4 | step 1092 | batch  36/352 | lr 0.00015 | loss 0.3098 | s/batch 8.35\n",
      "INFO:root:| epoch   4 | step 1095 | batch  39/352 | lr 0.00015 | loss 0.2853 | s/batch 9.91\n",
      "INFO:root:| epoch   4 | step 1098 | batch  42/352 | lr 0.00015 | loss 0.2782 | s/batch 5.74\n",
      "INFO:root:| epoch   4 | step 1101 | batch  45/352 | lr 0.00015 | loss 0.2681 | s/batch 5.74\n",
      "INFO:root:| epoch   4 | step 1104 | batch  48/352 | lr 0.00015 | loss 0.2780 | s/batch 3.70\n",
      "INFO:root:| epoch   4 | step 1107 | batch  51/352 | lr 0.00015 | loss 0.3226 | s/batch 3.76\n",
      "INFO:root:| epoch   4 | step 1110 | batch  54/352 | lr 0.00015 | loss 0.3255 | s/batch 6.30\n",
      "INFO:root:| epoch   4 | step 1113 | batch  57/352 | lr 0.00015 | loss 0.2324 | s/batch 9.49\n",
      "INFO:root:| epoch   4 | step 1116 | batch  60/352 | lr 0.00015 | loss 0.2713 | s/batch 12.56\n",
      "INFO:root:| epoch   4 | step 1119 | batch  63/352 | lr 0.00015 | loss 0.2904 | s/batch 3.13\n",
      "INFO:root:| epoch   4 | step 1122 | batch  66/352 | lr 0.00015 | loss 0.3053 | s/batch 6.76\n",
      "INFO:root:| epoch   4 | step 1125 | batch  69/352 | lr 0.00015 | loss 0.2844 | s/batch 7.25\n",
      "INFO:root:| epoch   4 | step 1128 | batch  72/352 | lr 0.00015 | loss 0.2548 | s/batch 7.90\n",
      "INFO:root:| epoch   4 | step 1131 | batch  75/352 | lr 0.00015 | loss 0.3116 | s/batch 9.10\n",
      "INFO:root:| epoch   4 | step 1134 | batch  78/352 | lr 0.00015 | loss 0.2859 | s/batch 3.14\n",
      "INFO:root:| epoch   4 | step 1137 | batch  81/352 | lr 0.00015 | loss 0.2528 | s/batch 5.61\n",
      "INFO:root:| epoch   4 | step 1140 | batch  84/352 | lr 0.00015 | loss 0.2940 | s/batch 5.01\n",
      "INFO:root:| epoch   4 | step 1143 | batch  87/352 | lr 0.00015 | loss 0.2837 | s/batch 2.12\n",
      "INFO:root:| epoch   4 | step 1146 | batch  90/352 | lr 0.00015 | loss 0.3077 | s/batch 7.66\n",
      "INFO:root:| epoch   4 | step 1149 | batch  93/352 | lr 0.00015 | loss 0.2811 | s/batch 7.05\n",
      "INFO:root:| epoch   4 | step 1152 | batch  96/352 | lr 0.00015 | loss 0.2869 | s/batch 3.63\n",
      "INFO:root:| epoch   4 | step 1155 | batch  99/352 | lr 0.00015 | loss 0.2893 | s/batch 13.74\n",
      "INFO:root:| epoch   4 | step 1158 | batch 102/352 | lr 0.00015 | loss 0.2823 | s/batch 4.55\n",
      "INFO:root:| epoch   4 | step 1161 | batch 105/352 | lr 0.00015 | loss 0.3076 | s/batch 8.69\n",
      "INFO:root:| epoch   4 | step 1164 | batch 108/352 | lr 0.00015 | loss 0.2516 | s/batch 8.09\n",
      "INFO:root:| epoch   4 | step 1167 | batch 111/352 | lr 0.00015 | loss 0.3218 | s/batch 15.23\n",
      "INFO:root:| epoch   4 | step 1170 | batch 114/352 | lr 0.00015 | loss 0.2489 | s/batch 9.75\n",
      "INFO:root:| epoch   4 | step 1173 | batch 117/352 | lr 0.00015 | loss 0.3002 | s/batch 5.07\n",
      "INFO:root:| epoch   4 | step 1176 | batch 120/352 | lr 0.00015 | loss 0.2670 | s/batch 5.75\n",
      "INFO:root:| epoch   4 | step 1179 | batch 123/352 | lr 0.00015 | loss 0.3310 | s/batch 18.37\n",
      "INFO:root:| epoch   4 | step 1182 | batch 126/352 | lr 0.00015 | loss 0.2758 | s/batch 11.22\n",
      "INFO:root:| epoch   4 | step 1185 | batch 129/352 | lr 0.00015 | loss 0.2774 | s/batch 6.52\n",
      "INFO:root:| epoch   4 | step 1188 | batch 132/352 | lr 0.00015 | loss 0.2944 | s/batch 6.44\n",
      "INFO:root:| epoch   4 | step 1191 | batch 135/352 | lr 0.00015 | loss 0.2723 | s/batch 3.14\n",
      "INFO:root:| epoch   4 | step 1194 | batch 138/352 | lr 0.00015 | loss 0.2978 | s/batch 3.46\n",
      "INFO:root:| epoch   4 | step 1197 | batch 141/352 | lr 0.00015 | loss 0.2681 | s/batch 12.32\n",
      "INFO:root:| epoch   4 | step 1200 | batch 144/352 | lr 0.00015 | loss 0.2907 | s/batch 5.66\n",
      "INFO:root:| epoch   4 | step 1203 | batch 147/352 | lr 0.00015 | loss 0.2749 | s/batch 8.15\n",
      "INFO:root:| epoch   4 | step 1206 | batch 150/352 | lr 0.00015 | loss 0.2940 | s/batch 2.27\n",
      "INFO:root:| epoch   4 | step 1209 | batch 153/352 | lr 0.00015 | loss 0.2631 | s/batch 7.98\n",
      "INFO:root:| epoch   4 | step 1212 | batch 156/352 | lr 0.00015 | loss 0.2455 | s/batch 8.85\n",
      "INFO:root:| epoch   4 | step 1215 | batch 159/352 | lr 0.00015 | loss 0.2749 | s/batch 6.36\n",
      "INFO:root:| epoch   4 | step 1218 | batch 162/352 | lr 0.00015 | loss 0.2779 | s/batch 9.86\n",
      "INFO:root:| epoch   4 | step 1221 | batch 165/352 | lr 0.00015 | loss 0.2997 | s/batch 2.81\n",
      "INFO:root:| epoch   4 | step 1224 | batch 168/352 | lr 0.00015 | loss 0.3160 | s/batch 4.05\n",
      "INFO:root:| epoch   4 | step 1227 | batch 171/352 | lr 0.00015 | loss 0.3136 | s/batch 9.20\n",
      "INFO:root:| epoch   4 | step 1230 | batch 174/352 | lr 0.00015 | loss 0.3023 | s/batch 8.67\n",
      "INFO:root:| epoch   4 | step 1233 | batch 177/352 | lr 0.00015 | loss 0.3045 | s/batch 9.10\n",
      "INFO:root:| epoch   4 | step 1236 | batch 180/352 | lr 0.00015 | loss 0.3369 | s/batch 10.07\n",
      "INFO:root:| epoch   4 | step 1239 | batch 183/352 | lr 0.00015 | loss 0.2792 | s/batch 14.06\n",
      "INFO:root:| epoch   4 | step 1242 | batch 186/352 | lr 0.00015 | loss 0.2713 | s/batch 5.05\n",
      "INFO:root:| epoch   4 | step 1245 | batch 189/352 | lr 0.00015 | loss 0.2798 | s/batch 6.30\n",
      "INFO:root:| epoch   4 | step 1248 | batch 192/352 | lr 0.00015 | loss 0.2934 | s/batch 10.15\n",
      "INFO:root:| epoch   4 | step 1251 | batch 195/352 | lr 0.00015 | loss 0.2983 | s/batch 10.42\n",
      "INFO:root:| epoch   4 | step 1254 | batch 198/352 | lr 0.00015 | loss 0.2896 | s/batch 5.57\n",
      "INFO:root:| epoch   4 | step 1257 | batch 201/352 | lr 0.00015 | loss 0.3000 | s/batch 9.21\n",
      "INFO:root:| epoch   4 | step 1260 | batch 204/352 | lr 0.00015 | loss 0.2992 | s/batch 2.80\n",
      "INFO:root:| epoch   4 | step 1263 | batch 207/352 | lr 0.00015 | loss 0.3137 | s/batch 14.74\n",
      "INFO:root:| epoch   4 | step 1266 | batch 210/352 | lr 0.00015 | loss 0.2479 | s/batch 7.58\n",
      "INFO:root:| epoch   4 | step 1269 | batch 213/352 | lr 0.00015 | loss 0.2717 | s/batch 6.20\n",
      "INFO:root:| epoch   4 | step 1272 | batch 216/352 | lr 0.00015 | loss 0.2605 | s/batch 4.58\n",
      "INFO:root:| epoch   4 | step 1275 | batch 219/352 | lr 0.00015 | loss 0.2711 | s/batch 3.99\n",
      "INFO:root:| epoch   4 | step 1278 | batch 222/352 | lr 0.00015 | loss 0.2962 | s/batch 11.97\n",
      "INFO:root:| epoch   4 | step 1281 | batch 225/352 | lr 0.00015 | loss 0.2927 | s/batch 7.21\n",
      "INFO:root:| epoch   4 | step 1284 | batch 228/352 | lr 0.00015 | loss 0.3009 | s/batch 10.13\n",
      "INFO:root:| epoch   4 | step 1287 | batch 231/352 | lr 0.00015 | loss 0.2631 | s/batch 6.46\n",
      "INFO:root:| epoch   4 | step 1290 | batch 234/352 | lr 0.00015 | loss 0.3132 | s/batch 13.56\n",
      "INFO:root:| epoch   4 | step 1293 | batch 237/352 | lr 0.00015 | loss 0.2743 | s/batch 3.84\n",
      "INFO:root:| epoch   4 | step 1296 | batch 240/352 | lr 0.00015 | loss 0.3062 | s/batch 7.60\n",
      "INFO:root:| epoch   4 | step 1299 | batch 243/352 | lr 0.00015 | loss 0.2658 | s/batch 7.61\n",
      "INFO:root:| epoch   4 | step 1302 | batch 246/352 | lr 0.00015 | loss 0.3035 | s/batch 4.00\n",
      "INFO:root:| epoch   4 | step 1305 | batch 249/352 | lr 0.00015 | loss 0.2785 | s/batch 5.70\n",
      "INFO:root:| epoch   4 | step 1308 | batch 252/352 | lr 0.00015 | loss 0.3284 | s/batch 10.20\n",
      "INFO:root:| epoch   4 | step 1311 | batch 255/352 | lr 0.00015 | loss 0.2643 | s/batch 4.98\n",
      "INFO:root:| epoch   4 | step 1314 | batch 258/352 | lr 0.00015 | loss 0.3020 | s/batch 12.19\n",
      "INFO:root:| epoch   4 | step 1317 | batch 261/352 | lr 0.00015 | loss 0.2373 | s/batch 9.46\n",
      "INFO:root:| epoch   4 | step 1320 | batch 264/352 | lr 0.00015 | loss 0.3241 | s/batch 8.85\n",
      "INFO:root:| epoch   4 | step 1323 | batch 267/352 | lr 0.00015 | loss 0.2778 | s/batch 6.19\n",
      "INFO:root:| epoch   4 | step 1326 | batch 270/352 | lr 0.00015 | loss 0.2843 | s/batch 4.08\n",
      "INFO:root:| epoch   4 | step 1329 | batch 273/352 | lr 0.00015 | loss 0.2966 | s/batch 3.95\n",
      "INFO:root:| epoch   4 | step 1332 | batch 276/352 | lr 0.00015 | loss 0.2534 | s/batch 11.49\n",
      "INFO:root:| epoch   4 | step 1335 | batch 279/352 | lr 0.00015 | loss 0.2936 | s/batch 8.99\n",
      "INFO:root:| epoch   4 | step 1338 | batch 282/352 | lr 0.00015 | loss 0.2699 | s/batch 5.68\n",
      "INFO:root:| epoch   4 | step 1341 | batch 285/352 | lr 0.00015 | loss 0.2888 | s/batch 8.35\n",
      "INFO:root:| epoch   4 | step 1344 | batch 288/352 | lr 0.00015 | loss 0.3508 | s/batch 16.35\n",
      "INFO:root:| epoch   4 | step 1347 | batch 291/352 | lr 0.00015 | loss 0.2443 | s/batch 7.75\n",
      "INFO:root:| epoch   4 | step 1350 | batch 294/352 | lr 0.00015 | loss 0.3141 | s/batch 10.19\n",
      "INFO:root:| epoch   4 | step 1353 | batch 297/352 | lr 0.00015 | loss 0.3204 | s/batch 13.99\n",
      "INFO:root:| epoch   4 | step 1356 | batch 300/352 | lr 0.00015 | loss 0.3089 | s/batch 3.80\n",
      "INFO:root:| epoch   4 | step 1359 | batch 303/352 | lr 0.00015 | loss 0.3164 | s/batch 3.34\n",
      "INFO:root:| epoch   4 | step 1362 | batch 306/352 | lr 0.00015 | loss 0.2988 | s/batch 14.45\n",
      "INFO:root:| epoch   4 | step 1365 | batch 309/352 | lr 0.00015 | loss 0.3082 | s/batch 9.62\n",
      "INFO:root:| epoch   4 | step 1368 | batch 312/352 | lr 0.00015 | loss 0.3030 | s/batch 10.51\n",
      "INFO:root:| epoch   4 | step 1371 | batch 315/352 | lr 0.00015 | loss 0.2736 | s/batch 4.65\n",
      "INFO:root:| epoch   4 | step 1374 | batch 318/352 | lr 0.00015 | loss 0.2946 | s/batch 3.15\n",
      "INFO:root:| epoch   4 | step 1377 | batch 321/352 | lr 0.00015 | loss 0.3133 | s/batch 8.42\n",
      "INFO:root:| epoch   4 | step 1380 | batch 324/352 | lr 0.00015 | loss 0.2548 | s/batch 4.62\n",
      "INFO:root:| epoch   4 | step 1383 | batch 327/352 | lr 0.00015 | loss 0.3125 | s/batch 10.80\n",
      "INFO:root:| epoch   4 | step 1386 | batch 330/352 | lr 0.00015 | loss 0.2855 | s/batch 3.15\n",
      "INFO:root:| epoch   4 | step 1389 | batch 333/352 | lr 0.00015 | loss 0.2955 | s/batch 9.79\n",
      "INFO:root:| epoch   4 | step 1392 | batch 336/352 | lr 0.00015 | loss 0.2319 | s/batch 7.77\n",
      "INFO:root:| epoch   4 | step 1395 | batch 339/352 | lr 0.00015 | loss 0.3028 | s/batch 14.92\n",
      "INFO:root:| epoch   4 | step 1398 | batch 342/352 | lr 0.00015 | loss 0.2799 | s/batch 10.99\n",
      "INFO:root:| epoch   4 | step 1401 | batch 345/352 | lr 0.00015 | loss 0.2258 | s/batch 11.62\n",
      "INFO:root:| epoch   4 | step 1404 | batch 348/352 | lr 0.00015 | loss 0.2652 | s/batch 5.06\n",
      "INFO:root:| epoch   4 | step 1407 | batch 351/352 | lr 0.00015 | loss 0.3062 | s/batch 8.66\n",
      "INFO:root:| epoch   4 | score (88.96, 87.17, 88.02) | f1 88.02 | loss 0.2876 | time 2745.57\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9130    0.9139    0.9135     35027\n",
      "          股票     0.9182    0.9327    0.9254     33251\n",
      "          体育     0.9752    0.9779    0.9765     28283\n",
      "          娱乐     0.9178    0.9308    0.9243     19920\n",
      "          时政     0.8452    0.8774    0.8610     13515\n",
      "          社会     0.8320    0.8280    0.8300     11009\n",
      "          教育     0.9183    0.9072    0.9127      8987\n",
      "          财经     0.8502    0.7738    0.8102      7957\n",
      "          家居     0.8654    0.8601    0.8627      7063\n",
      "          游戏     0.8918    0.8621    0.8767      5285\n",
      "          房产     0.9498    0.9277    0.9386      4428\n",
      "          时尚     0.8149    0.8062    0.8106      2818\n",
      "          彩票     0.9183    0.8499    0.8828      1639\n",
      "          星座     0.8445    0.7567    0.7982       818\n",
      "\n",
      "    accuracy                         0.9085    180000\n",
      "   macro avg     0.8896    0.8717    0.8802    180000\n",
      "weighted avg     0.9083    0.9085    0.9083    180000\n",
      "\n",
      "INFO:root:| epoch   4 | dev | score (92.23, 91.95, 92.05) | f1 92.05 | time 256.72\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9539    0.9149    0.9340      3891\n",
      "          股票     0.9475    0.9383    0.9429      3694\n",
      "          体育     0.9749    0.9876    0.9812      3142\n",
      "          娱乐     0.9350    0.9553    0.9450      2213\n",
      "          时政     0.8521    0.9061    0.8783      1501\n",
      "          社会     0.8569    0.8716    0.8642      1223\n",
      "          教育     0.9303    0.9359    0.9331       998\n",
      "          财经     0.8567    0.8722    0.8643       884\n",
      "          家居     0.9392    0.9056    0.9221       784\n",
      "          游戏     0.9175    0.9191    0.9183       593\n",
      "          房产     0.9897    0.9797    0.9847       492\n",
      "          时尚     0.8529    0.9073    0.8793       313\n",
      "          彩票     0.9639    0.8791    0.9195       182\n",
      "          星座     0.9419    0.9000    0.9205        90\n",
      "\n",
      "    accuracy                         0.9318     20000\n",
      "   macro avg     0.9223    0.9195    0.9205     20000\n",
      "weighted avg     0.9326    0.9318    0.9320     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 91.66, current dev = 92.05\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch   5 | step 1411 | batch   3/352 | lr 0.00015 | loss 0.2920 | s/batch 2.89\n",
      "INFO:root:| epoch   5 | step 1414 | batch   6/352 | lr 0.00015 | loss 0.2490 | s/batch 12.19\n",
      "INFO:root:| epoch   5 | step 1417 | batch   9/352 | lr 0.00015 | loss 0.3417 | s/batch 8.98\n",
      "INFO:root:| epoch   5 | step 1420 | batch  12/352 | lr 0.00015 | loss 0.2152 | s/batch 7.01\n",
      "INFO:root:| epoch   5 | step 1423 | batch  15/352 | lr 0.00015 | loss 0.2613 | s/batch 4.28\n",
      "INFO:root:| epoch   5 | step 1426 | batch  18/352 | lr 0.00015 | loss 0.2671 | s/batch 7.27\n",
      "INFO:root:| epoch   5 | step 1429 | batch  21/352 | lr 0.00015 | loss 0.2507 | s/batch 4.34\n",
      "INFO:root:| epoch   5 | step 1432 | batch  24/352 | lr 0.00015 | loss 0.2486 | s/batch 4.07\n",
      "INFO:root:| epoch   5 | step 1435 | batch  27/352 | lr 0.00015 | loss 0.2923 | s/batch 10.23\n",
      "INFO:root:| epoch   5 | step 1438 | batch  30/352 | lr 0.00015 | loss 0.2882 | s/batch 10.05\n",
      "INFO:root:| epoch   5 | step 1441 | batch  33/352 | lr 0.00015 | loss 0.2782 | s/batch 9.35\n",
      "INFO:root:| epoch   5 | step 1444 | batch  36/352 | lr 0.00015 | loss 0.2229 | s/batch 6.21\n",
      "INFO:root:| epoch   5 | step 1447 | batch  39/352 | lr 0.00015 | loss 0.2248 | s/batch 4.38\n",
      "INFO:root:| epoch   5 | step 1450 | batch  42/352 | lr 0.00015 | loss 0.2399 | s/batch 6.25\n",
      "INFO:root:| epoch   5 | step 1453 | batch  45/352 | lr 0.00015 | loss 0.3066 | s/batch 13.33\n",
      "INFO:root:| epoch   5 | step 1456 | batch  48/352 | lr 0.00015 | loss 0.2933 | s/batch 8.49\n",
      "INFO:root:| epoch   5 | step 1459 | batch  51/352 | lr 0.00015 | loss 0.2449 | s/batch 10.83\n",
      "INFO:root:| epoch   5 | step 1462 | batch  54/352 | lr 0.00015 | loss 0.2974 | s/batch 3.69\n",
      "INFO:root:| epoch   5 | step 1465 | batch  57/352 | lr 0.00015 | loss 0.2565 | s/batch 5.60\n",
      "INFO:root:| epoch   5 | step 1468 | batch  60/352 | lr 0.00015 | loss 0.3144 | s/batch 3.21\n",
      "INFO:root:| epoch   5 | step 1471 | batch  63/352 | lr 0.00015 | loss 0.2751 | s/batch 4.50\n",
      "INFO:root:| epoch   5 | step 1474 | batch  66/352 | lr 0.00015 | loss 0.2975 | s/batch 8.72\n",
      "INFO:root:| epoch   5 | step 1477 | batch  69/352 | lr 0.00015 | loss 0.2338 | s/batch 10.24\n",
      "INFO:root:| epoch   5 | step 1480 | batch  72/352 | lr 0.00015 | loss 0.2519 | s/batch 7.89\n",
      "INFO:root:| epoch   5 | step 1483 | batch  75/352 | lr 0.00015 | loss 0.3104 | s/batch 5.66\n",
      "INFO:root:| epoch   5 | step 1486 | batch  78/352 | lr 0.00015 | loss 0.2593 | s/batch 9.03\n",
      "INFO:root:| epoch   5 | step 1489 | batch  81/352 | lr 0.00015 | loss 0.3041 | s/batch 4.05\n",
      "INFO:root:| epoch   5 | step 1492 | batch  84/352 | lr 0.00015 | loss 0.3051 | s/batch 2.42\n",
      "INFO:root:| epoch   5 | step 1495 | batch  87/352 | lr 0.00015 | loss 0.3075 | s/batch 13.98\n",
      "INFO:root:| epoch   5 | step 1498 | batch  90/352 | lr 0.00015 | loss 0.2709 | s/batch 3.76\n",
      "INFO:root:| epoch   5 | step 1501 | batch  93/352 | lr 0.00015 | loss 0.2781 | s/batch 5.65\n",
      "INFO:root:| epoch   5 | step 1504 | batch  96/352 | lr 0.00015 | loss 0.2347 | s/batch 3.75\n",
      "INFO:root:| epoch   5 | step 1507 | batch  99/352 | lr 0.00015 | loss 0.2889 | s/batch 10.29\n",
      "INFO:root:| epoch   5 | step 1510 | batch 102/352 | lr 0.00015 | loss 0.2403 | s/batch 4.17\n",
      "INFO:root:| epoch   5 | step 1513 | batch 105/352 | lr 0.00015 | loss 0.3025 | s/batch 10.98\n",
      "INFO:root:| epoch   5 | step 1516 | batch 108/352 | lr 0.00015 | loss 0.2799 | s/batch 8.61\n",
      "INFO:root:| epoch   5 | step 1519 | batch 111/352 | lr 0.00015 | loss 0.3033 | s/batch 8.27\n",
      "INFO:root:| epoch   5 | step 1522 | batch 114/352 | lr 0.00015 | loss 0.3240 | s/batch 3.88\n",
      "INFO:root:| epoch   5 | step 1525 | batch 117/352 | lr 0.00015 | loss 0.2495 | s/batch 4.94\n",
      "INFO:root:| epoch   5 | step 1528 | batch 120/352 | lr 0.00015 | loss 0.3098 | s/batch 7.62\n",
      "INFO:root:| epoch   5 | step 1531 | batch 123/352 | lr 0.00015 | loss 0.3264 | s/batch 10.11\n",
      "INFO:root:| epoch   5 | step 1534 | batch 126/352 | lr 0.00015 | loss 0.2386 | s/batch 11.93\n",
      "INFO:root:| epoch   5 | step 1537 | batch 129/352 | lr 0.00015 | loss 0.2817 | s/batch 5.73\n",
      "INFO:root:| epoch   5 | step 1540 | batch 132/352 | lr 0.00015 | loss 0.2889 | s/batch 6.36\n",
      "INFO:root:| epoch   5 | step 1543 | batch 135/352 | lr 0.00015 | loss 0.2744 | s/batch 10.84\n",
      "INFO:root:| epoch   5 | step 1546 | batch 138/352 | lr 0.00015 | loss 0.2623 | s/batch 5.90\n",
      "INFO:root:| epoch   5 | step 1549 | batch 141/352 | lr 0.00015 | loss 0.2611 | s/batch 3.26\n",
      "INFO:root:| epoch   5 | step 1552 | batch 144/352 | lr 0.00015 | loss 0.2874 | s/batch 12.06\n",
      "INFO:root:| epoch   5 | step 1555 | batch 147/352 | lr 0.00015 | loss 0.2597 | s/batch 9.40\n",
      "INFO:root:| epoch   5 | step 1558 | batch 150/352 | lr 0.00015 | loss 0.2761 | s/batch 4.62\n",
      "INFO:root:| epoch   5 | step 1561 | batch 153/352 | lr 0.00015 | loss 0.2848 | s/batch 13.23\n",
      "INFO:root:| epoch   5 | step 1564 | batch 156/352 | lr 0.00015 | loss 0.2412 | s/batch 6.95\n",
      "INFO:root:| epoch   5 | step 1567 | batch 159/352 | lr 0.00015 | loss 0.2567 | s/batch 9.95\n",
      "INFO:root:| epoch   5 | step 1570 | batch 162/352 | lr 0.00015 | loss 0.3024 | s/batch 10.35\n",
      "INFO:root:| epoch   5 | step 1573 | batch 165/352 | lr 0.00015 | loss 0.2673 | s/batch 9.36\n",
      "INFO:root:| epoch   5 | step 1576 | batch 168/352 | lr 0.00015 | loss 0.3057 | s/batch 9.26\n",
      "INFO:root:| epoch   5 | step 1579 | batch 171/352 | lr 0.00015 | loss 0.3440 | s/batch 14.78\n",
      "INFO:root:| epoch   5 | step 1582 | batch 174/352 | lr 0.00015 | loss 0.3272 | s/batch 13.26\n",
      "INFO:root:| epoch   5 | step 1585 | batch 177/352 | lr 0.00015 | loss 0.2834 | s/batch 10.18\n",
      "INFO:root:| epoch   5 | step 1588 | batch 180/352 | lr 0.00015 | loss 0.2960 | s/batch 3.79\n",
      "INFO:root:| epoch   5 | step 1591 | batch 183/352 | lr 0.00015 | loss 0.3010 | s/batch 3.98\n",
      "INFO:root:| epoch   5 | step 1594 | batch 186/352 | lr 0.00015 | loss 0.2595 | s/batch 7.44\n",
      "INFO:root:| epoch   5 | step 1597 | batch 189/352 | lr 0.00015 | loss 0.2220 | s/batch 7.57\n",
      "INFO:root:| epoch   5 | step 1600 | batch 192/352 | lr 0.00015 | loss 0.2789 | s/batch 4.41\n",
      "INFO:root:| epoch   5 | step 1603 | batch 195/352 | lr 0.00015 | loss 0.2542 | s/batch 8.44\n",
      "INFO:root:| epoch   5 | step 1606 | batch 198/352 | lr 0.00015 | loss 0.2766 | s/batch 12.25\n",
      "INFO:root:| epoch   5 | step 1609 | batch 201/352 | lr 0.00015 | loss 0.2573 | s/batch 11.52\n",
      "INFO:root:| epoch   5 | step 1612 | batch 204/352 | lr 0.00015 | loss 0.2707 | s/batch 5.05\n",
      "INFO:root:| epoch   5 | step 1615 | batch 207/352 | lr 0.00015 | loss 0.2561 | s/batch 8.53\n",
      "INFO:root:| epoch   5 | step 1618 | batch 210/352 | lr 0.00015 | loss 0.2605 | s/batch 7.04\n",
      "INFO:root:| epoch   5 | step 1621 | batch 213/352 | lr 0.00015 | loss 0.2784 | s/batch 3.80\n",
      "INFO:root:| epoch   5 | step 1624 | batch 216/352 | lr 0.00015 | loss 0.2713 | s/batch 14.53\n",
      "INFO:root:| epoch   5 | step 1627 | batch 219/352 | lr 0.00015 | loss 0.3129 | s/batch 10.20\n",
      "INFO:root:| epoch   5 | step 1630 | batch 222/352 | lr 0.00015 | loss 0.2883 | s/batch 9.77\n",
      "INFO:root:| epoch   5 | step 1633 | batch 225/352 | lr 0.00015 | loss 0.2608 | s/batch 5.06\n",
      "INFO:root:| epoch   5 | step 1636 | batch 228/352 | lr 0.00015 | loss 0.2615 | s/batch 5.60\n",
      "INFO:root:| epoch   5 | step 1639 | batch 231/352 | lr 0.00015 | loss 0.3060 | s/batch 8.83\n",
      "INFO:root:| epoch   5 | step 1642 | batch 234/352 | lr 0.00015 | loss 0.2512 | s/batch 9.92\n",
      "INFO:root:| epoch   5 | step 1645 | batch 237/352 | lr 0.00015 | loss 0.2470 | s/batch 6.25\n",
      "INFO:root:| epoch   5 | step 1648 | batch 240/352 | lr 0.00015 | loss 0.2830 | s/batch 8.46\n",
      "INFO:root:| epoch   5 | step 1651 | batch 243/352 | lr 0.00015 | loss 0.2595 | s/batch 8.92\n",
      "INFO:root:| epoch   5 | step 1654 | batch 246/352 | lr 0.00015 | loss 0.2990 | s/batch 5.10\n",
      "INFO:root:| epoch   5 | step 1657 | batch 249/352 | lr 0.00015 | loss 0.2997 | s/batch 12.97\n",
      "INFO:root:| epoch   5 | step 1660 | batch 252/352 | lr 0.00015 | loss 0.2790 | s/batch 6.32\n",
      "INFO:root:| epoch   5 | step 1663 | batch 255/352 | lr 0.00015 | loss 0.2856 | s/batch 9.47\n",
      "INFO:root:| epoch   5 | step 1666 | batch 258/352 | lr 0.00015 | loss 0.3320 | s/batch 16.64\n",
      "INFO:root:| epoch   5 | step 1669 | batch 261/352 | lr 0.00015 | loss 0.2421 | s/batch 4.01\n",
      "INFO:root:| epoch   5 | step 1672 | batch 264/352 | lr 0.00015 | loss 0.3430 | s/batch 13.96\n",
      "INFO:root:| epoch   5 | step 1675 | batch 267/352 | lr 0.00015 | loss 0.2462 | s/batch 7.26\n",
      "INFO:root:| epoch   5 | step 1678 | batch 270/352 | lr 0.00015 | loss 0.2818 | s/batch 5.32\n",
      "INFO:root:| epoch   5 | step 1681 | batch 273/352 | lr 0.00015 | loss 0.2989 | s/batch 10.12\n",
      "INFO:root:| epoch   5 | step 1684 | batch 276/352 | lr 0.00015 | loss 0.2488 | s/batch 4.89\n",
      "INFO:root:| epoch   5 | step 1687 | batch 279/352 | lr 0.00015 | loss 0.2993 | s/batch 9.40\n",
      "INFO:root:| epoch   5 | step 1690 | batch 282/352 | lr 0.00015 | loss 0.2890 | s/batch 12.41\n",
      "INFO:root:| epoch   5 | step 1693 | batch 285/352 | lr 0.00015 | loss 0.2987 | s/batch 12.30\n",
      "INFO:root:| epoch   5 | step 1696 | batch 288/352 | lr 0.00015 | loss 0.2772 | s/batch 4.05\n",
      "INFO:root:| epoch   5 | step 1699 | batch 291/352 | lr 0.00015 | loss 0.2444 | s/batch 6.92\n",
      "INFO:root:| epoch   5 | step 1702 | batch 294/352 | lr 0.00015 | loss 0.2675 | s/batch 9.75\n",
      "INFO:root:| epoch   5 | step 1705 | batch 297/352 | lr 0.00015 | loss 0.2503 | s/batch 5.54\n",
      "INFO:root:| epoch   5 | step 1708 | batch 300/352 | lr 0.00015 | loss 0.2498 | s/batch 8.05\n",
      "INFO:root:| epoch   5 | step 1711 | batch 303/352 | lr 0.00015 | loss 0.2864 | s/batch 3.93\n",
      "INFO:root:| epoch   5 | step 1714 | batch 306/352 | lr 0.00015 | loss 0.2577 | s/batch 8.63\n",
      "INFO:root:| epoch   5 | step 1717 | batch 309/352 | lr 0.00015 | loss 0.2621 | s/batch 3.40\n",
      "INFO:root:| epoch   5 | step 1720 | batch 312/352 | lr 0.00015 | loss 0.3216 | s/batch 3.65\n",
      "INFO:root:| epoch   5 | step 1723 | batch 315/352 | lr 0.00015 | loss 0.2728 | s/batch 10.36\n",
      "INFO:root:| epoch   5 | step 1726 | batch 318/352 | lr 0.00015 | loss 0.2422 | s/batch 5.77\n",
      "INFO:root:| epoch   5 | step 1729 | batch 321/352 | lr 0.00015 | loss 0.3152 | s/batch 10.05\n",
      "INFO:root:| epoch   5 | step 1732 | batch 324/352 | lr 0.00015 | loss 0.2322 | s/batch 7.59\n",
      "INFO:root:| epoch   5 | step 1735 | batch 327/352 | lr 0.00015 | loss 0.2643 | s/batch 8.78\n",
      "INFO:root:| epoch   5 | step 1738 | batch 330/352 | lr 0.00015 | loss 0.2407 | s/batch 9.57\n",
      "INFO:root:| epoch   5 | step 1741 | batch 333/352 | lr 0.00015 | loss 0.2374 | s/batch 8.26\n",
      "INFO:root:| epoch   5 | step 1744 | batch 336/352 | lr 0.00015 | loss 0.3008 | s/batch 2.81\n",
      "INFO:root:| epoch   5 | step 1747 | batch 339/352 | lr 0.00015 | loss 0.2918 | s/batch 4.18\n",
      "INFO:root:| epoch   5 | step 1750 | batch 342/352 | lr 0.00015 | loss 0.3053 | s/batch 11.32\n",
      "INFO:root:| epoch   5 | step 1753 | batch 345/352 | lr 0.00015 | loss 0.2386 | s/batch 5.05\n",
      "INFO:root:| epoch   5 | step 1756 | batch 348/352 | lr 0.00015 | loss 0.2569 | s/batch 3.14\n",
      "INFO:root:| epoch   5 | step 1759 | batch 351/352 | lr 0.00015 | loss 0.2937 | s/batch 3.78\n",
      "INFO:root:| epoch   5 | score (89.67, 88.01, 88.8) | f1 88.8 | loss 0.2758 | time 2728.12\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9170    0.9188    0.9179     35027\n",
      "          股票     0.9214    0.9339    0.9276     33251\n",
      "          体育     0.9764    0.9787    0.9776     28283\n",
      "          娱乐     0.9216    0.9341    0.9278     19920\n",
      "          时政     0.8484    0.8800    0.8639     13515\n",
      "          社会     0.8405    0.8322    0.8363     11009\n",
      "          教育     0.9231    0.9120    0.9175      8987\n",
      "          财经     0.8544    0.7857    0.8186      7957\n",
      "          家居     0.8710    0.8682    0.8696      7063\n",
      "          游戏     0.8958    0.8702    0.8828      5285\n",
      "          房产     0.9525    0.9332    0.9427      4428\n",
      "          时尚     0.8357    0.8215    0.8286      2818\n",
      "          彩票     0.9232    0.8578    0.8893      1639\n",
      "          星座     0.8725    0.7946    0.8317       818\n",
      "\n",
      "    accuracy                         0.9125    180000\n",
      "   macro avg     0.8967    0.8801    0.8880    180000\n",
      "weighted avg     0.9124    0.9125    0.9123    180000\n",
      "\n",
      "INFO:root:| epoch   5 | dev | score (92.28, 92.02, 92.12) | f1 92.12 | time 256.71\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9310    0.9437    0.9373      3891\n",
      "          股票     0.9609    0.9177    0.9388      3694\n",
      "          体育     0.9828    0.9831    0.9830      3142\n",
      "          娱乐     0.9295    0.9652    0.9470      2213\n",
      "          时政     0.8477    0.9087    0.8772      1501\n",
      "          社会     0.8793    0.8520    0.8654      1223\n",
      "          教育     0.9329    0.9339    0.9334       998\n",
      "          财经     0.8451    0.8824    0.8633       884\n",
      "          家居     0.9420    0.8903    0.9154       784\n",
      "          游戏     0.9469    0.9022    0.9240       593\n",
      "          房产     0.9837    0.9817    0.9827       492\n",
      "          时尚     0.9169    0.8818    0.8990       313\n",
      "          彩票     0.9278    0.9176    0.9227       182\n",
      "          星座     0.8925    0.9222    0.9071        90\n",
      "\n",
      "    accuracy                         0.9324     20000\n",
      "   macro avg     0.9228    0.9202    0.9212     20000\n",
      "weighted avg     0.9332    0.9324    0.9325     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 92.05, current dev = 92.12\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch   6 | step 1763 | batch   3/352 | lr 0.00015 | loss 0.2958 | s/batch 12.00\n",
      "INFO:root:| epoch   6 | step 1766 | batch   6/352 | lr 0.00015 | loss 0.2606 | s/batch 11.21\n",
      "INFO:root:| epoch   6 | step 1769 | batch   9/352 | lr 0.00015 | loss 0.2741 | s/batch 11.94\n",
      "INFO:root:| epoch   6 | step 1772 | batch  12/352 | lr 0.00015 | loss 0.2471 | s/batch 5.14\n",
      "INFO:root:| epoch   6 | step 1775 | batch  15/352 | lr 0.00015 | loss 0.3169 | s/batch 3.21\n",
      "INFO:root:| epoch   6 | step 1778 | batch  18/352 | lr 0.00015 | loss 0.2691 | s/batch 8.27\n",
      "INFO:root:| epoch   6 | step 1781 | batch  21/352 | lr 0.00015 | loss 0.2447 | s/batch 11.10\n",
      "INFO:root:| epoch   6 | step 1784 | batch  24/352 | lr 0.00015 | loss 0.2489 | s/batch 6.21\n",
      "INFO:root:| epoch   6 | step 1787 | batch  27/352 | lr 0.00015 | loss 0.2964 | s/batch 2.56\n",
      "INFO:root:| epoch   6 | step 1790 | batch  30/352 | lr 0.00015 | loss 0.2334 | s/batch 4.45\n",
      "INFO:root:| epoch   6 | step 1793 | batch  33/352 | lr 0.00015 | loss 0.2530 | s/batch 3.99\n",
      "INFO:root:| epoch   6 | step 1796 | batch  36/352 | lr 0.00015 | loss 0.2686 | s/batch 9.97\n",
      "INFO:root:| epoch   6 | step 1799 | batch  39/352 | lr 0.00015 | loss 0.2942 | s/batch 3.63\n",
      "INFO:root:| epoch   6 | step 1802 | batch  42/352 | lr 0.00015 | loss 0.2728 | s/batch 3.99\n",
      "INFO:root:| epoch   6 | step 1805 | batch  45/352 | lr 0.00015 | loss 0.2926 | s/batch 14.75\n",
      "INFO:root:| epoch   6 | step 1808 | batch  48/352 | lr 0.00015 | loss 0.3208 | s/batch 14.32\n",
      "INFO:root:| epoch   6 | step 1811 | batch  51/352 | lr 0.00015 | loss 0.2522 | s/batch 4.39\n",
      "INFO:root:| epoch   6 | step 1814 | batch  54/352 | lr 0.00015 | loss 0.3012 | s/batch 14.54\n",
      "INFO:root:| epoch   6 | step 1817 | batch  57/352 | lr 0.00015 | loss 0.2473 | s/batch 9.28\n",
      "INFO:root:| epoch   6 | step 1820 | batch  60/352 | lr 0.00015 | loss 0.2347 | s/batch 7.34\n",
      "INFO:root:| epoch   6 | step 1823 | batch  63/352 | lr 0.00015 | loss 0.2934 | s/batch 13.79\n",
      "INFO:root:| epoch   6 | step 1826 | batch  66/352 | lr 0.00015 | loss 0.2249 | s/batch 12.75\n",
      "INFO:root:| epoch   6 | step 1829 | batch  69/352 | lr 0.00015 | loss 0.2523 | s/batch 5.05\n",
      "INFO:root:| epoch   6 | step 1832 | batch  72/352 | lr 0.00015 | loss 0.2813 | s/batch 5.53\n",
      "INFO:root:| epoch   6 | step 1835 | batch  75/352 | lr 0.00015 | loss 0.2729 | s/batch 9.78\n",
      "INFO:root:| epoch   6 | step 1838 | batch  78/352 | lr 0.00015 | loss 0.2755 | s/batch 2.76\n",
      "INFO:root:| epoch   6 | step 1841 | batch  81/352 | lr 0.00015 | loss 0.2917 | s/batch 12.91\n",
      "INFO:root:| epoch   6 | step 1844 | batch  84/352 | lr 0.00015 | loss 0.2715 | s/batch 5.78\n",
      "INFO:root:| epoch   6 | step 1847 | batch  87/352 | lr 0.00015 | loss 0.2790 | s/batch 10.34\n",
      "INFO:root:| epoch   6 | step 1850 | batch  90/352 | lr 0.00015 | loss 0.3010 | s/batch 8.24\n",
      "INFO:root:| epoch   6 | step 1853 | batch  93/352 | lr 0.00015 | loss 0.2869 | s/batch 4.48\n",
      "INFO:root:| epoch   6 | step 1856 | batch  96/352 | lr 0.00015 | loss 0.3124 | s/batch 10.89\n",
      "INFO:root:| epoch   6 | step 1859 | batch  99/352 | lr 0.00015 | loss 0.3187 | s/batch 10.02\n",
      "INFO:root:| epoch   6 | step 1862 | batch 102/352 | lr 0.00015 | loss 0.2815 | s/batch 8.99\n",
      "INFO:root:| epoch   6 | step 1865 | batch 105/352 | lr 0.00015 | loss 0.2379 | s/batch 4.98\n",
      "INFO:root:| epoch   6 | step 1868 | batch 108/352 | lr 0.00015 | loss 0.3031 | s/batch 4.54\n",
      "INFO:root:| epoch   6 | step 1871 | batch 111/352 | lr 0.00015 | loss 0.2367 | s/batch 5.95\n",
      "INFO:root:| epoch   6 | step 1874 | batch 114/352 | lr 0.00015 | loss 0.2767 | s/batch 5.10\n",
      "INFO:root:| epoch   6 | step 1877 | batch 117/352 | lr 0.00015 | loss 0.2476 | s/batch 3.61\n",
      "INFO:root:| epoch   6 | step 1880 | batch 120/352 | lr 0.00015 | loss 0.2644 | s/batch 8.75\n",
      "INFO:root:| epoch   6 | step 1883 | batch 123/352 | lr 0.00015 | loss 0.3342 | s/batch 13.37\n",
      "INFO:root:| epoch   6 | step 1886 | batch 126/352 | lr 0.00015 | loss 0.2749 | s/batch 5.78\n",
      "INFO:root:| epoch   6 | step 1889 | batch 129/352 | lr 0.00015 | loss 0.2274 | s/batch 6.36\n",
      "INFO:root:| epoch   6 | step 1892 | batch 132/352 | lr 0.00015 | loss 0.2560 | s/batch 13.17\n",
      "INFO:root:| epoch   6 | step 1895 | batch 135/352 | lr 0.00015 | loss 0.2482 | s/batch 5.06\n",
      "INFO:root:| epoch   6 | step 1898 | batch 138/352 | lr 0.00015 | loss 0.2892 | s/batch 3.92\n",
      "INFO:root:| epoch   6 | step 1901 | batch 141/352 | lr 0.00015 | loss 0.2624 | s/batch 10.45\n",
      "INFO:root:| epoch   6 | step 1904 | batch 144/352 | lr 0.00015 | loss 0.2513 | s/batch 9.04\n",
      "INFO:root:| epoch   6 | step 1907 | batch 147/352 | lr 0.00015 | loss 0.2978 | s/batch 9.58\n",
      "INFO:root:| epoch   6 | step 1910 | batch 150/352 | lr 0.00015 | loss 0.2505 | s/batch 9.08\n",
      "INFO:root:| epoch   6 | step 1913 | batch 153/352 | lr 0.00015 | loss 0.2205 | s/batch 5.76\n",
      "INFO:root:| epoch   6 | step 1916 | batch 156/352 | lr 0.00015 | loss 0.2981 | s/batch 8.45\n",
      "INFO:root:| epoch   6 | step 1919 | batch 159/352 | lr 0.00015 | loss 0.2542 | s/batch 7.79\n",
      "INFO:root:| epoch   6 | step 1922 | batch 162/352 | lr 0.00015 | loss 0.2170 | s/batch 7.35\n",
      "INFO:root:| epoch   6 | step 1925 | batch 165/352 | lr 0.00015 | loss 0.2745 | s/batch 12.25\n",
      "INFO:root:| epoch   6 | step 1928 | batch 168/352 | lr 0.00015 | loss 0.3360 | s/batch 11.16\n",
      "INFO:root:| epoch   6 | step 1931 | batch 171/352 | lr 0.00015 | loss 0.2744 | s/batch 13.13\n",
      "INFO:root:| epoch   6 | step 1934 | batch 174/352 | lr 0.00015 | loss 0.2391 | s/batch 7.06\n",
      "INFO:root:| epoch   6 | step 1937 | batch 177/352 | lr 0.00015 | loss 0.3048 | s/batch 8.55\n",
      "INFO:root:| epoch   6 | step 1940 | batch 180/352 | lr 0.00015 | loss 0.2703 | s/batch 2.59\n",
      "INFO:root:| epoch   6 | step 1943 | batch 183/352 | lr 0.00015 | loss 0.2596 | s/batch 6.28\n",
      "INFO:root:| epoch   6 | step 1946 | batch 186/352 | lr 0.00015 | loss 0.2181 | s/batch 5.24\n",
      "INFO:root:| epoch   6 | step 1949 | batch 189/352 | lr 0.00015 | loss 0.2581 | s/batch 10.50\n",
      "INFO:root:| epoch   6 | step 1952 | batch 192/352 | lr 0.00015 | loss 0.2821 | s/batch 14.45\n",
      "INFO:root:| epoch   6 | step 1955 | batch 195/352 | lr 0.00015 | loss 0.2598 | s/batch 12.35\n",
      "INFO:root:| epoch   6 | step 1958 | batch 198/352 | lr 0.00015 | loss 0.2606 | s/batch 5.57\n",
      "INFO:root:| epoch   6 | step 1961 | batch 201/352 | lr 0.00015 | loss 0.2375 | s/batch 3.29\n",
      "INFO:root:| epoch   6 | step 1964 | batch 204/352 | lr 0.00015 | loss 0.2295 | s/batch 8.32\n",
      "INFO:root:| epoch   6 | step 1967 | batch 207/352 | lr 0.00015 | loss 0.2847 | s/batch 3.99\n",
      "INFO:root:| epoch   6 | step 1970 | batch 210/352 | lr 0.00015 | loss 0.2412 | s/batch 8.21\n",
      "INFO:root:| epoch   6 | step 1973 | batch 213/352 | lr 0.00015 | loss 0.2542 | s/batch 3.96\n",
      "INFO:root:| epoch   6 | step 1976 | batch 216/352 | lr 0.00015 | loss 0.2860 | s/batch 12.07\n",
      "INFO:root:| epoch   6 | step 1979 | batch 219/352 | lr 0.00015 | loss 0.2907 | s/batch 9.29\n",
      "INFO:root:| epoch   6 | step 1982 | batch 222/352 | lr 0.00015 | loss 0.2994 | s/batch 8.92\n",
      "INFO:root:| epoch   6 | step 1985 | batch 225/352 | lr 0.00015 | loss 0.2231 | s/batch 6.41\n",
      "INFO:root:| epoch   6 | step 1988 | batch 228/352 | lr 0.00015 | loss 0.2587 | s/batch 4.38\n",
      "INFO:root:| epoch   6 | step 1991 | batch 231/352 | lr 0.00015 | loss 0.2401 | s/batch 3.72\n",
      "INFO:root:| epoch   6 | step 1994 | batch 234/352 | lr 0.00015 | loss 0.1902 | s/batch 6.27\n",
      "INFO:root:| epoch   6 | step 1997 | batch 237/352 | lr 0.00015 | loss 0.2251 | s/batch 6.59\n",
      "INFO:root:| epoch   6 | step 2000 | batch 240/352 | lr 0.00011 | loss 0.2423 | s/batch 8.39\n",
      "INFO:root:| epoch   6 | step 2003 | batch 243/352 | lr 0.00011 | loss 0.2775 | s/batch 4.49\n",
      "INFO:root:| epoch   6 | step 2006 | batch 246/352 | lr 0.00011 | loss 0.2743 | s/batch 14.74\n",
      "INFO:root:| epoch   6 | step 2009 | batch 249/352 | lr 0.00011 | loss 0.2836 | s/batch 4.62\n",
      "INFO:root:| epoch   6 | step 2012 | batch 252/352 | lr 0.00011 | loss 0.2560 | s/batch 2.61\n",
      "INFO:root:| epoch   6 | step 2015 | batch 255/352 | lr 0.00011 | loss 0.2082 | s/batch 8.34\n",
      "INFO:root:| epoch   6 | step 2018 | batch 258/352 | lr 0.00011 | loss 0.2434 | s/batch 7.00\n",
      "INFO:root:| epoch   6 | step 2021 | batch 261/352 | lr 0.00011 | loss 0.2695 | s/batch 7.19\n",
      "INFO:root:| epoch   6 | step 2024 | batch 264/352 | lr 0.00011 | loss 0.2756 | s/batch 9.92\n",
      "INFO:root:| epoch   6 | step 2027 | batch 267/352 | lr 0.00011 | loss 0.2471 | s/batch 5.77\n",
      "INFO:root:| epoch   6 | step 2030 | batch 270/352 | lr 0.00011 | loss 0.3149 | s/batch 9.69\n",
      "INFO:root:| epoch   6 | step 2033 | batch 273/352 | lr 0.00011 | loss 0.2941 | s/batch 9.28\n",
      "INFO:root:| epoch   6 | step 2036 | batch 276/352 | lr 0.00011 | loss 0.2694 | s/batch 4.43\n",
      "INFO:root:| epoch   6 | step 2039 | batch 279/352 | lr 0.00011 | loss 0.2760 | s/batch 7.99\n",
      "INFO:root:| epoch   6 | step 2042 | batch 282/352 | lr 0.00011 | loss 0.2807 | s/batch 3.93\n",
      "INFO:root:| epoch   6 | step 2045 | batch 285/352 | lr 0.00011 | loss 0.2392 | s/batch 15.17\n",
      "INFO:root:| epoch   6 | step 2048 | batch 288/352 | lr 0.00011 | loss 0.2552 | s/batch 4.37\n",
      "INFO:root:| epoch   6 | step 2051 | batch 291/352 | lr 0.00011 | loss 0.2854 | s/batch 10.19\n",
      "INFO:root:| epoch   6 | step 2054 | batch 294/352 | lr 0.00011 | loss 0.2187 | s/batch 7.73\n",
      "INFO:root:| epoch   6 | step 2057 | batch 297/352 | lr 0.00011 | loss 0.2580 | s/batch 5.37\n",
      "INFO:root:| epoch   6 | step 2060 | batch 300/352 | lr 0.00011 | loss 0.2369 | s/batch 6.32\n",
      "INFO:root:| epoch   6 | step 2063 | batch 303/352 | lr 0.00011 | loss 0.2549 | s/batch 4.05\n",
      "INFO:root:| epoch   6 | step 2066 | batch 306/352 | lr 0.00011 | loss 0.2926 | s/batch 13.66\n",
      "INFO:root:| epoch   6 | step 2069 | batch 309/352 | lr 0.00011 | loss 0.2555 | s/batch 6.46\n",
      "INFO:root:| epoch   6 | step 2072 | batch 312/352 | lr 0.00011 | loss 0.2742 | s/batch 14.66\n",
      "INFO:root:| epoch   6 | step 2075 | batch 315/352 | lr 0.00011 | loss 0.2936 | s/batch 5.58\n",
      "INFO:root:| epoch   6 | step 2078 | batch 318/352 | lr 0.00011 | loss 0.2778 | s/batch 5.31\n",
      "INFO:root:| epoch   6 | step 2081 | batch 321/352 | lr 0.00011 | loss 0.2545 | s/batch 9.81\n",
      "INFO:root:| epoch   6 | step 2084 | batch 324/352 | lr 0.00011 | loss 0.2723 | s/batch 2.72\n",
      "INFO:root:| epoch   6 | step 2087 | batch 327/352 | lr 0.00011 | loss 0.2661 | s/batch 9.97\n",
      "INFO:root:| epoch   6 | step 2090 | batch 330/352 | lr 0.00011 | loss 0.2243 | s/batch 9.69\n",
      "INFO:root:| epoch   6 | step 2093 | batch 333/352 | lr 0.00011 | loss 0.2533 | s/batch 4.38\n",
      "INFO:root:| epoch   6 | step 2096 | batch 336/352 | lr 0.00011 | loss 0.2171 | s/batch 11.05\n",
      "INFO:root:| epoch   6 | step 2099 | batch 339/352 | lr 0.00011 | loss 0.2907 | s/batch 4.48\n",
      "INFO:root:| epoch   6 | step 2102 | batch 342/352 | lr 0.00011 | loss 0.3181 | s/batch 10.02\n",
      "INFO:root:| epoch   6 | step 2105 | batch 345/352 | lr 0.00011 | loss 0.2594 | s/batch 11.80\n",
      "INFO:root:| epoch   6 | step 2108 | batch 348/352 | lr 0.00011 | loss 0.2549 | s/batch 5.12\n",
      "INFO:root:| epoch   6 | step 2111 | batch 351/352 | lr 0.00011 | loss 0.2064 | s/batch 8.06\n",
      "INFO:root:| epoch   6 | score (90.07, 88.6, 89.31) | f1 89.31 | loss 0.2658 | time 2766.00\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9187    0.9198    0.9193     35027\n",
      "          股票     0.9232    0.9367    0.9299     33251\n",
      "          体育     0.9778    0.9794    0.9786     28283\n",
      "          娱乐     0.9269    0.9372    0.9320     19920\n",
      "          时政     0.8496    0.8834    0.8661     13515\n",
      "          社会     0.8419    0.8326    0.8372     11009\n",
      "          教育     0.9205    0.9137    0.9171      8987\n",
      "          财经     0.8591    0.7891    0.8226      7957\n",
      "          家居     0.8759    0.8736    0.8747      7063\n",
      "          游戏     0.9013    0.8693    0.8850      5285\n",
      "          房产     0.9538    0.9365    0.9451      4428\n",
      "          时尚     0.8536    0.8336    0.8434      2818\n",
      "          彩票     0.9227    0.8743    0.8979      1639\n",
      "          星座     0.8847    0.8252    0.8539       818\n",
      "\n",
      "    accuracy                         0.9150    180000\n",
      "   macro avg     0.9007    0.8860    0.8931    180000\n",
      "weighted avg     0.9148    0.9150    0.9148    180000\n",
      "\n",
      "INFO:root:| epoch   6 | dev | score (92.74, 92.64, 92.67) | f1 92.67 | time 256.69\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9503    0.9232    0.9365      3891\n",
      "          股票     0.9481    0.9388    0.9434      3694\n",
      "          体育     0.9758    0.9870    0.9813      3142\n",
      "          娱乐     0.9474    0.9526    0.9500      2213\n",
      "          时政     0.8531    0.9134    0.8822      1501\n",
      "          社会     0.8589    0.8757    0.8672      1223\n",
      "          教育     0.9493    0.9198    0.9344       998\n",
      "          财经     0.8579    0.8744    0.8661       884\n",
      "          家居     0.9409    0.9133    0.9269       784\n",
      "          游戏     0.9120    0.9258    0.9188       593\n",
      "          房产     0.9779    0.9898    0.9838       492\n",
      "          时尚     0.9194    0.9105    0.9149       313\n",
      "          彩票     0.9385    0.9231    0.9307       182\n",
      "          星座     0.9540    0.9222    0.9379        90\n",
      "\n",
      "    accuracy                         0.9345     20000\n",
      "   macro avg     0.9274    0.9264    0.9267     20000\n",
      "weighted avg     0.9351    0.9345    0.9347     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 92.12, current dev = 92.67\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch   7 | step 2115 | batch   3/352 | lr 0.00011 | loss 0.2551 | s/batch 3.46\n",
      "INFO:root:| epoch   7 | step 2118 | batch   6/352 | lr 0.00011 | loss 0.2876 | s/batch 9.31\n",
      "INFO:root:| epoch   7 | step 2121 | batch   9/352 | lr 0.00011 | loss 0.2285 | s/batch 7.92\n",
      "INFO:root:| epoch   7 | step 2124 | batch  12/352 | lr 0.00011 | loss 0.2679 | s/batch 5.28\n",
      "INFO:root:| epoch   7 | step 2127 | batch  15/352 | lr 0.00011 | loss 0.2353 | s/batch 5.71\n",
      "INFO:root:| epoch   7 | step 2130 | batch  18/352 | lr 0.00011 | loss 0.3185 | s/batch 15.43\n",
      "INFO:root:| epoch   7 | step 2133 | batch  21/352 | lr 0.00011 | loss 0.2575 | s/batch 3.49\n",
      "INFO:root:| epoch   7 | step 2136 | batch  24/352 | lr 0.00011 | loss 0.2343 | s/batch 6.70\n",
      "INFO:root:| epoch   7 | step 2139 | batch  27/352 | lr 0.00011 | loss 0.2911 | s/batch 4.72\n",
      "INFO:root:| epoch   7 | step 2142 | batch  30/352 | lr 0.00011 | loss 0.2771 | s/batch 7.70\n",
      "INFO:root:| epoch   7 | step 2145 | batch  33/352 | lr 0.00011 | loss 0.2615 | s/batch 3.76\n",
      "INFO:root:| epoch   7 | step 2148 | batch  36/352 | lr 0.00011 | loss 0.2806 | s/batch 10.99\n",
      "INFO:root:| epoch   7 | step 2151 | batch  39/352 | lr 0.00011 | loss 0.2629 | s/batch 10.25\n",
      "INFO:root:| epoch   7 | step 2154 | batch  42/352 | lr 0.00011 | loss 0.3042 | s/batch 7.13\n",
      "INFO:root:| epoch   7 | step 2157 | batch  45/352 | lr 0.00011 | loss 0.2167 | s/batch 5.01\n",
      "INFO:root:| epoch   7 | step 2160 | batch  48/352 | lr 0.00011 | loss 0.2675 | s/batch 4.13\n",
      "INFO:root:| epoch   7 | step 2163 | batch  51/352 | lr 0.00011 | loss 0.2634 | s/batch 11.70\n",
      "INFO:root:| epoch   7 | step 2166 | batch  54/352 | lr 0.00011 | loss 0.2862 | s/batch 3.18\n",
      "INFO:root:| epoch   7 | step 2169 | batch  57/352 | lr 0.00011 | loss 0.2609 | s/batch 5.62\n",
      "INFO:root:| epoch   7 | step 2172 | batch  60/352 | lr 0.00011 | loss 0.2723 | s/batch 9.27\n",
      "INFO:root:| epoch   7 | step 2175 | batch  63/352 | lr 0.00011 | loss 0.2041 | s/batch 6.63\n",
      "INFO:root:| epoch   7 | step 2178 | batch  66/352 | lr 0.00011 | loss 0.2553 | s/batch 6.26\n",
      "INFO:root:| epoch   7 | step 2181 | batch  69/352 | lr 0.00011 | loss 0.2377 | s/batch 5.96\n",
      "INFO:root:| epoch   7 | step 2184 | batch  72/352 | lr 0.00011 | loss 0.2654 | s/batch 12.45\n",
      "INFO:root:| epoch   7 | step 2187 | batch  75/352 | lr 0.00011 | loss 0.2674 | s/batch 10.75\n",
      "INFO:root:| epoch   7 | step 2190 | batch  78/352 | lr 0.00011 | loss 0.2957 | s/batch 10.39\n",
      "INFO:root:| epoch   7 | step 2193 | batch  81/352 | lr 0.00011 | loss 0.2469 | s/batch 5.05\n",
      "INFO:root:| epoch   7 | step 2196 | batch  84/352 | lr 0.00011 | loss 0.2419 | s/batch 11.89\n",
      "INFO:root:| epoch   7 | step 2199 | batch  87/352 | lr 0.00011 | loss 0.2533 | s/batch 4.06\n",
      "INFO:root:| epoch   7 | step 2202 | batch  90/352 | lr 0.00011 | loss 0.2716 | s/batch 8.71\n",
      "INFO:root:| epoch   7 | step 2205 | batch  93/352 | lr 0.00011 | loss 0.2572 | s/batch 7.64\n",
      "INFO:root:| epoch   7 | step 2208 | batch  96/352 | lr 0.00011 | loss 0.2507 | s/batch 6.29\n",
      "INFO:root:| epoch   7 | step 2211 | batch  99/352 | lr 0.00011 | loss 0.2341 | s/batch 5.80\n",
      "INFO:root:| epoch   7 | step 2214 | batch 102/352 | lr 0.00011 | loss 0.2684 | s/batch 17.59\n",
      "INFO:root:| epoch   7 | step 2217 | batch 105/352 | lr 0.00011 | loss 0.2443 | s/batch 11.00\n",
      "INFO:root:| epoch   7 | step 2220 | batch 108/352 | lr 0.00011 | loss 0.2526 | s/batch 10.03\n",
      "INFO:root:| epoch   7 | step 2223 | batch 111/352 | lr 0.00011 | loss 0.2551 | s/batch 8.29\n",
      "INFO:root:| epoch   7 | step 2226 | batch 114/352 | lr 0.00011 | loss 0.2523 | s/batch 12.90\n",
      "INFO:root:| epoch   7 | step 2229 | batch 117/352 | lr 0.00011 | loss 0.2685 | s/batch 4.06\n",
      "INFO:root:| epoch   7 | step 2232 | batch 120/352 | lr 0.00011 | loss 0.2473 | s/batch 4.64\n",
      "INFO:root:| epoch   7 | step 2235 | batch 123/352 | lr 0.00011 | loss 0.2548 | s/batch 3.86\n",
      "INFO:root:| epoch   7 | step 2238 | batch 126/352 | lr 0.00011 | loss 0.2395 | s/batch 13.22\n",
      "INFO:root:| epoch   7 | step 2241 | batch 129/352 | lr 0.00011 | loss 0.2710 | s/batch 9.21\n",
      "INFO:root:| epoch   7 | step 2244 | batch 132/352 | lr 0.00011 | loss 0.2756 | s/batch 10.51\n",
      "INFO:root:| epoch   7 | step 2247 | batch 135/352 | lr 0.00011 | loss 0.2065 | s/batch 7.58\n",
      "INFO:root:| epoch   7 | step 2250 | batch 138/352 | lr 0.00011 | loss 0.2572 | s/batch 5.62\n",
      "INFO:root:| epoch   7 | step 2253 | batch 141/352 | lr 0.00011 | loss 0.2112 | s/batch 9.52\n",
      "INFO:root:| epoch   7 | step 2256 | batch 144/352 | lr 0.00011 | loss 0.2626 | s/batch 5.09\n",
      "INFO:root:| epoch   7 | step 2259 | batch 147/352 | lr 0.00011 | loss 0.2515 | s/batch 10.15\n",
      "INFO:root:| epoch   7 | step 2262 | batch 150/352 | lr 0.00011 | loss 0.2510 | s/batch 3.89\n",
      "INFO:root:| epoch   7 | step 2265 | batch 153/352 | lr 0.00011 | loss 0.2009 | s/batch 6.83\n",
      "INFO:root:| epoch   7 | step 2268 | batch 156/352 | lr 0.00011 | loss 0.2192 | s/batch 5.71\n",
      "INFO:root:| epoch   7 | step 2271 | batch 159/352 | lr 0.00011 | loss 0.2557 | s/batch 8.61\n",
      "INFO:root:| epoch   7 | step 2274 | batch 162/352 | lr 0.00011 | loss 0.2434 | s/batch 4.09\n",
      "INFO:root:| epoch   7 | step 2277 | batch 165/352 | lr 0.00011 | loss 0.2398 | s/batch 9.41\n",
      "INFO:root:| epoch   7 | step 2280 | batch 168/352 | lr 0.00011 | loss 0.2386 | s/batch 4.42\n",
      "INFO:root:| epoch   7 | step 2283 | batch 171/352 | lr 0.00011 | loss 0.2406 | s/batch 12.94\n",
      "INFO:root:| epoch   7 | step 2286 | batch 174/352 | lr 0.00011 | loss 0.2426 | s/batch 13.06\n",
      "INFO:root:| epoch   7 | step 2289 | batch 177/352 | lr 0.00011 | loss 0.2709 | s/batch 3.33\n",
      "INFO:root:| epoch   7 | step 2292 | batch 180/352 | lr 0.00011 | loss 0.2503 | s/batch 9.20\n",
      "INFO:root:| epoch   7 | step 2295 | batch 183/352 | lr 0.00011 | loss 0.2359 | s/batch 7.80\n",
      "INFO:root:| epoch   7 | step 2298 | batch 186/352 | lr 0.00011 | loss 0.3024 | s/batch 8.11\n",
      "INFO:root:| epoch   7 | step 2301 | batch 189/352 | lr 0.00011 | loss 0.2887 | s/batch 5.32\n",
      "INFO:root:| epoch   7 | step 2304 | batch 192/352 | lr 0.00011 | loss 0.2388 | s/batch 10.02\n",
      "INFO:root:| epoch   7 | step 2307 | batch 195/352 | lr 0.00011 | loss 0.2426 | s/batch 11.76\n",
      "INFO:root:| epoch   7 | step 2310 | batch 198/352 | lr 0.00011 | loss 0.2405 | s/batch 8.46\n",
      "INFO:root:| epoch   7 | step 2313 | batch 201/352 | lr 0.00011 | loss 0.3049 | s/batch 16.37\n",
      "INFO:root:| epoch   7 | step 2316 | batch 204/352 | lr 0.00011 | loss 0.2410 | s/batch 8.66\n",
      "INFO:root:| epoch   7 | step 2319 | batch 207/352 | lr 0.00011 | loss 0.2301 | s/batch 4.75\n",
      "INFO:root:| epoch   7 | step 2322 | batch 210/352 | lr 0.00011 | loss 0.2318 | s/batch 6.45\n",
      "INFO:root:| epoch   7 | step 2325 | batch 213/352 | lr 0.00011 | loss 0.2303 | s/batch 8.16\n",
      "INFO:root:| epoch   7 | step 2328 | batch 216/352 | lr 0.00011 | loss 0.2487 | s/batch 9.15\n",
      "INFO:root:| epoch   7 | step 2331 | batch 219/352 | lr 0.00011 | loss 0.2350 | s/batch 3.68\n",
      "INFO:root:| epoch   7 | step 2334 | batch 222/352 | lr 0.00011 | loss 0.2471 | s/batch 3.83\n",
      "INFO:root:| epoch   7 | step 2337 | batch 225/352 | lr 0.00011 | loss 0.2831 | s/batch 11.21\n",
      "INFO:root:| epoch   7 | step 2340 | batch 228/352 | lr 0.00011 | loss 0.2482 | s/batch 9.96\n",
      "INFO:root:| epoch   7 | step 2343 | batch 231/352 | lr 0.00011 | loss 0.2234 | s/batch 9.87\n",
      "INFO:root:| epoch   7 | step 2346 | batch 234/352 | lr 0.00011 | loss 0.2421 | s/batch 5.81\n",
      "INFO:root:| epoch   7 | step 2349 | batch 237/352 | lr 0.00011 | loss 0.2425 | s/batch 5.69\n",
      "INFO:root:| epoch   7 | step 2352 | batch 240/352 | lr 0.00011 | loss 0.2387 | s/batch 5.76\n",
      "INFO:root:| epoch   7 | step 2355 | batch 243/352 | lr 0.00011 | loss 0.2592 | s/batch 4.42\n",
      "INFO:root:| epoch   7 | step 2358 | batch 246/352 | lr 0.00011 | loss 0.2594 | s/batch 13.33\n",
      "INFO:root:| epoch   7 | step 2361 | batch 249/352 | lr 0.00011 | loss 0.2696 | s/batch 12.51\n",
      "INFO:root:| epoch   7 | step 2364 | batch 252/352 | lr 0.00011 | loss 0.2459 | s/batch 3.27\n",
      "INFO:root:| epoch   7 | step 2367 | batch 255/352 | lr 0.00011 | loss 0.2593 | s/batch 5.04\n",
      "INFO:root:| epoch   7 | step 2370 | batch 258/352 | lr 0.00011 | loss 0.2442 | s/batch 12.41\n",
      "INFO:root:| epoch   7 | step 2373 | batch 261/352 | lr 0.00011 | loss 0.2605 | s/batch 5.04\n",
      "INFO:root:| epoch   7 | step 2376 | batch 264/352 | lr 0.00011 | loss 0.2662 | s/batch 4.39\n",
      "INFO:root:| epoch   7 | step 2379 | batch 267/352 | lr 0.00011 | loss 0.2731 | s/batch 12.76\n",
      "INFO:root:| epoch   7 | step 2382 | batch 270/352 | lr 0.00011 | loss 0.2505 | s/batch 3.95\n",
      "INFO:root:| epoch   7 | step 2385 | batch 273/352 | lr 0.00011 | loss 0.2453 | s/batch 8.29\n",
      "INFO:root:| epoch   7 | step 2388 | batch 276/352 | lr 0.00011 | loss 0.2470 | s/batch 6.31\n",
      "INFO:root:| epoch   7 | step 2391 | batch 279/352 | lr 0.00011 | loss 0.2903 | s/batch 12.22\n",
      "INFO:root:| epoch   7 | step 2394 | batch 282/352 | lr 0.00011 | loss 0.2666 | s/batch 5.33\n",
      "INFO:root:| epoch   7 | step 2397 | batch 285/352 | lr 0.00011 | loss 0.2581 | s/batch 10.08\n",
      "INFO:root:| epoch   7 | step 2400 | batch 288/352 | lr 0.00011 | loss 0.2774 | s/batch 8.97\n",
      "INFO:root:| epoch   7 | step 2403 | batch 291/352 | lr 0.00011 | loss 0.2534 | s/batch 6.03\n",
      "INFO:root:| epoch   7 | step 2406 | batch 294/352 | lr 0.00011 | loss 0.2778 | s/batch 12.50\n",
      "INFO:root:| epoch   7 | step 2409 | batch 297/352 | lr 0.00011 | loss 0.2506 | s/batch 10.45\n",
      "INFO:root:| epoch   7 | step 2412 | batch 300/352 | lr 0.00011 | loss 0.2784 | s/batch 7.11\n",
      "INFO:root:| epoch   7 | step 2415 | batch 303/352 | lr 0.00011 | loss 0.2992 | s/batch 9.85\n",
      "INFO:root:| epoch   7 | step 2418 | batch 306/352 | lr 0.00011 | loss 0.2301 | s/batch 7.46\n",
      "INFO:root:| epoch   7 | step 2421 | batch 309/352 | lr 0.00011 | loss 0.2552 | s/batch 10.96\n",
      "INFO:root:| epoch   7 | step 2424 | batch 312/352 | lr 0.00011 | loss 0.2621 | s/batch 3.34\n",
      "INFO:root:| epoch   7 | step 2427 | batch 315/352 | lr 0.00011 | loss 0.2658 | s/batch 9.75\n",
      "INFO:root:| epoch   7 | step 2430 | batch 318/352 | lr 0.00011 | loss 0.2727 | s/batch 2.91\n",
      "INFO:root:| epoch   7 | step 2433 | batch 321/352 | lr 0.00011 | loss 0.2084 | s/batch 5.56\n",
      "INFO:root:| epoch   7 | step 2436 | batch 324/352 | lr 0.00011 | loss 0.2689 | s/batch 11.35\n",
      "INFO:root:| epoch   7 | step 2439 | batch 327/352 | lr 0.00011 | loss 0.2474 | s/batch 4.35\n",
      "INFO:root:| epoch   7 | step 2442 | batch 330/352 | lr 0.00011 | loss 0.2014 | s/batch 9.50\n",
      "INFO:root:| epoch   7 | step 2445 | batch 333/352 | lr 0.00011 | loss 0.2751 | s/batch 7.62\n",
      "INFO:root:| epoch   7 | step 2448 | batch 336/352 | lr 0.00011 | loss 0.3051 | s/batch 13.76\n",
      "INFO:root:| epoch   7 | step 2451 | batch 339/352 | lr 0.00011 | loss 0.2137 | s/batch 7.05\n",
      "INFO:root:| epoch   7 | step 2454 | batch 342/352 | lr 0.00011 | loss 0.2058 | s/batch 4.47\n",
      "INFO:root:| epoch   7 | step 2457 | batch 345/352 | lr 0.00011 | loss 0.2936 | s/batch 5.50\n",
      "INFO:root:| epoch   7 | step 2460 | batch 348/352 | lr 0.00011 | loss 0.2414 | s/batch 8.28\n",
      "INFO:root:| epoch   7 | step 2463 | batch 351/352 | lr 0.00011 | loss 0.2296 | s/batch 5.69\n",
      "INFO:root:| epoch   7 | score (90.39, 89.14, 89.74) | f1 89.74 | loss 0.2546 | time 2766.48\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9219    0.9235    0.9227     35027\n",
      "          股票     0.9281    0.9386    0.9334     33251\n",
      "          体育     0.9787    0.9802    0.9794     28283\n",
      "          娱乐     0.9286    0.9421    0.9353     19920\n",
      "          时政     0.8551    0.8855    0.8700     13515\n",
      "          社会     0.8517    0.8386    0.8451     11009\n",
      "          教育     0.9270    0.9179    0.9224      8987\n",
      "          财经     0.8594    0.8021    0.8297      7957\n",
      "          家居     0.8791    0.8768    0.8779      7063\n",
      "          游戏     0.9029    0.8747    0.8886      5285\n",
      "          房产     0.9625    0.9442    0.9533      4428\n",
      "          时尚     0.8580    0.8364    0.8471      2818\n",
      "          彩票     0.9313    0.8761    0.9029      1639\n",
      "          星座     0.8699    0.8423    0.8559       818\n",
      "\n",
      "    accuracy                         0.9186    180000\n",
      "   macro avg     0.9039    0.8914    0.8974    180000\n",
      "weighted avg     0.9185    0.9186    0.9185    180000\n",
      "\n",
      "INFO:root:| epoch   7 | dev | score (92.87, 92.66, 92.68) | f1 92.68 | time 257.49\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9468    0.9322    0.9394      3891\n",
      "          股票     0.9694    0.9077    0.9375      3694\n",
      "          体育     0.9841    0.9850    0.9846      3142\n",
      "          娱乐     0.9580    0.9381    0.9479      2213\n",
      "          时政     0.8366    0.9241    0.8781      1501\n",
      "          社会     0.8269    0.8904    0.8575      1223\n",
      "          教育     0.9337    0.9449    0.9392       998\n",
      "          财经     0.8036    0.9072    0.8523       884\n",
      "          家居     0.9212    0.9388    0.9299       784\n",
      "          游戏     0.9388    0.9056    0.9219       593\n",
      "          房产     0.9878    0.9878    0.9878       492\n",
      "          时尚     0.9367    0.8978    0.9168       313\n",
      "          彩票     0.9704    0.9011    0.9345       182\n",
      "          星座     0.9880    0.9111    0.9480        90\n",
      "\n",
      "    accuracy                         0.9329     20000\n",
      "   macro avg     0.9287    0.9266    0.9268     20000\n",
      "weighted avg     0.9355    0.9329    0.9336     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 92.67, current dev = 92.68\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch   8 | step 2467 | batch   3/352 | lr 0.00011 | loss 0.2395 | s/batch 6.53\n",
      "INFO:root:| epoch   8 | step 2470 | batch   6/352 | lr 0.00011 | loss 0.2617 | s/batch 6.39\n",
      "INFO:root:| epoch   8 | step 2473 | batch   9/352 | lr 0.00011 | loss 0.2612 | s/batch 12.28\n",
      "INFO:root:| epoch   8 | step 2476 | batch  12/352 | lr 0.00011 | loss 0.2162 | s/batch 4.58\n",
      "INFO:root:| epoch   8 | step 2479 | batch  15/352 | lr 0.00011 | loss 0.2450 | s/batch 5.06\n",
      "INFO:root:| epoch   8 | step 2482 | batch  18/352 | lr 0.00011 | loss 0.2860 | s/batch 11.01\n",
      "INFO:root:| epoch   8 | step 2485 | batch  21/352 | lr 0.00011 | loss 0.2507 | s/batch 3.92\n",
      "INFO:root:| epoch   8 | step 2488 | batch  24/352 | lr 0.00011 | loss 0.3030 | s/batch 14.68\n",
      "INFO:root:| epoch   8 | step 2491 | batch  27/352 | lr 0.00011 | loss 0.2422 | s/batch 3.17\n",
      "INFO:root:| epoch   8 | step 2494 | batch  30/352 | lr 0.00011 | loss 0.2416 | s/batch 3.24\n",
      "INFO:root:| epoch   8 | step 2497 | batch  33/352 | lr 0.00011 | loss 0.2924 | s/batch 9.05\n",
      "INFO:root:| epoch   8 | step 2500 | batch  36/352 | lr 0.00011 | loss 0.2810 | s/batch 4.99\n",
      "INFO:root:| epoch   8 | step 2503 | batch  39/352 | lr 0.00011 | loss 0.2997 | s/batch 9.04\n",
      "INFO:root:| epoch   8 | step 2506 | batch  42/352 | lr 0.00011 | loss 0.2599 | s/batch 2.46\n",
      "INFO:root:| epoch   8 | step 2509 | batch  45/352 | lr 0.00011 | loss 0.3052 | s/batch 8.48\n",
      "INFO:root:| epoch   8 | step 2512 | batch  48/352 | lr 0.00011 | loss 0.2689 | s/batch 9.82\n",
      "INFO:root:| epoch   8 | step 2515 | batch  51/352 | lr 0.00011 | loss 0.2638 | s/batch 11.58\n",
      "INFO:root:| epoch   8 | step 2518 | batch  54/352 | lr 0.00011 | loss 0.2820 | s/batch 7.76\n",
      "INFO:root:| epoch   8 | step 2521 | batch  57/352 | lr 0.00011 | loss 0.2959 | s/batch 8.36\n",
      "INFO:root:| epoch   8 | step 2524 | batch  60/352 | lr 0.00011 | loss 0.2478 | s/batch 5.93\n",
      "INFO:root:| epoch   8 | step 2527 | batch  63/352 | lr 0.00011 | loss 0.2119 | s/batch 8.31\n",
      "INFO:root:| epoch   8 | step 2530 | batch  66/352 | lr 0.00011 | loss 0.2808 | s/batch 18.78\n",
      "INFO:root:| epoch   8 | step 2533 | batch  69/352 | lr 0.00011 | loss 0.2394 | s/batch 5.92\n",
      "INFO:root:| epoch   8 | step 2536 | batch  72/352 | lr 0.00011 | loss 0.3037 | s/batch 9.67\n",
      "INFO:root:| epoch   8 | step 2539 | batch  75/352 | lr 0.00011 | loss 0.2435 | s/batch 4.13\n",
      "INFO:root:| epoch   8 | step 2542 | batch  78/352 | lr 0.00011 | loss 0.2481 | s/batch 4.88\n",
      "INFO:root:| epoch   8 | step 2545 | batch  81/352 | lr 0.00011 | loss 0.2227 | s/batch 5.74\n",
      "INFO:root:| epoch   8 | step 2548 | batch  84/352 | lr 0.00011 | loss 0.2469 | s/batch 4.47\n",
      "INFO:root:| epoch   8 | step 2551 | batch  87/352 | lr 0.00011 | loss 0.2183 | s/batch 7.92\n",
      "INFO:root:| epoch   8 | step 2554 | batch  90/352 | lr 0.00011 | loss 0.2302 | s/batch 7.89\n",
      "INFO:root:| epoch   8 | step 2557 | batch  93/352 | lr 0.00011 | loss 0.2488 | s/batch 7.69\n",
      "INFO:root:| epoch   8 | step 2560 | batch  96/352 | lr 0.00011 | loss 0.2253 | s/batch 5.61\n",
      "INFO:root:| epoch   8 | step 2563 | batch  99/352 | lr 0.00011 | loss 0.2267 | s/batch 7.62\n",
      "INFO:root:| epoch   8 | step 2566 | batch 102/352 | lr 0.00011 | loss 0.2472 | s/batch 4.03\n",
      "INFO:root:| epoch   8 | step 2569 | batch 105/352 | lr 0.00011 | loss 0.2130 | s/batch 6.27\n",
      "INFO:root:| epoch   8 | step 2572 | batch 108/352 | lr 0.00011 | loss 0.2266 | s/batch 4.47\n",
      "INFO:root:| epoch   8 | step 2575 | batch 111/352 | lr 0.00011 | loss 0.2288 | s/batch 5.75\n",
      "INFO:root:| epoch   8 | step 2578 | batch 114/352 | lr 0.00011 | loss 0.2873 | s/batch 12.90\n",
      "INFO:root:| epoch   8 | step 2581 | batch 117/352 | lr 0.00011 | loss 0.2665 | s/batch 13.87\n",
      "INFO:root:| epoch   8 | step 2584 | batch 120/352 | lr 0.00011 | loss 0.2662 | s/batch 10.07\n",
      "INFO:root:| epoch   8 | step 2587 | batch 123/352 | lr 0.00011 | loss 0.2216 | s/batch 10.68\n",
      "INFO:root:| epoch   8 | step 2590 | batch 126/352 | lr 0.00011 | loss 0.2823 | s/batch 12.65\n",
      "INFO:root:| epoch   8 | step 2593 | batch 129/352 | lr 0.00011 | loss 0.2523 | s/batch 7.35\n",
      "INFO:root:| epoch   8 | step 2596 | batch 132/352 | lr 0.00011 | loss 0.2273 | s/batch 4.11\n",
      "INFO:root:| epoch   8 | step 2599 | batch 135/352 | lr 0.00011 | loss 0.2236 | s/batch 11.86\n",
      "INFO:root:| epoch   8 | step 2602 | batch 138/352 | lr 0.00011 | loss 0.2264 | s/batch 6.33\n",
      "INFO:root:| epoch   8 | step 2605 | batch 141/352 | lr 0.00011 | loss 0.2168 | s/batch 12.85\n",
      "INFO:root:| epoch   8 | step 2608 | batch 144/352 | lr 0.00011 | loss 0.2341 | s/batch 5.12\n",
      "INFO:root:| epoch   8 | step 2611 | batch 147/352 | lr 0.00011 | loss 0.2451 | s/batch 8.92\n",
      "INFO:root:| epoch   8 | step 2614 | batch 150/352 | lr 0.00011 | loss 0.2371 | s/batch 4.49\n",
      "INFO:root:| epoch   8 | step 2617 | batch 153/352 | lr 0.00011 | loss 0.2200 | s/batch 10.02\n",
      "INFO:root:| epoch   8 | step 2620 | batch 156/352 | lr 0.00011 | loss 0.2069 | s/batch 5.12\n",
      "INFO:root:| epoch   8 | step 2623 | batch 159/352 | lr 0.00011 | loss 0.2686 | s/batch 16.24\n",
      "INFO:root:| epoch   8 | step 2626 | batch 162/352 | lr 0.00011 | loss 0.2555 | s/batch 14.75\n",
      "INFO:root:| epoch   8 | step 2629 | batch 165/352 | lr 0.00011 | loss 0.2595 | s/batch 12.50\n",
      "INFO:root:| epoch   8 | step 2632 | batch 168/352 | lr 0.00011 | loss 0.2577 | s/batch 11.47\n",
      "INFO:root:| epoch   8 | step 2635 | batch 171/352 | lr 0.00011 | loss 0.2577 | s/batch 8.20\n",
      "INFO:root:| epoch   8 | step 2638 | batch 174/352 | lr 0.00011 | loss 0.2248 | s/batch 13.02\n",
      "INFO:root:| epoch   8 | step 2641 | batch 177/352 | lr 0.00011 | loss 0.2733 | s/batch 3.18\n",
      "INFO:root:| epoch   8 | step 2644 | batch 180/352 | lr 0.00011 | loss 0.2606 | s/batch 7.60\n",
      "INFO:root:| epoch   8 | step 2647 | batch 183/352 | lr 0.00011 | loss 0.1863 | s/batch 6.31\n",
      "INFO:root:| epoch   8 | step 2650 | batch 186/352 | lr 0.00011 | loss 0.2617 | s/batch 14.87\n",
      "INFO:root:| epoch   8 | step 2653 | batch 189/352 | lr 0.00011 | loss 0.2471 | s/batch 5.05\n",
      "INFO:root:| epoch   8 | step 2656 | batch 192/352 | lr 0.00011 | loss 0.2314 | s/batch 3.65\n",
      "INFO:root:| epoch   8 | step 2659 | batch 195/352 | lr 0.00011 | loss 0.2498 | s/batch 2.77\n",
      "INFO:root:| epoch   8 | step 2662 | batch 198/352 | lr 0.00011 | loss 0.2663 | s/batch 3.33\n",
      "INFO:root:| epoch   8 | step 2665 | batch 201/352 | lr 0.00011 | loss 0.2307 | s/batch 8.80\n",
      "INFO:root:| epoch   8 | step 2668 | batch 204/352 | lr 0.00011 | loss 0.2416 | s/batch 8.90\n",
      "INFO:root:| epoch   8 | step 2671 | batch 207/352 | lr 0.00011 | loss 0.2408 | s/batch 5.01\n",
      "INFO:root:| epoch   8 | step 2674 | batch 210/352 | lr 0.00011 | loss 0.2868 | s/batch 9.30\n",
      "INFO:root:| epoch   8 | step 2677 | batch 213/352 | lr 0.00011 | loss 0.2586 | s/batch 4.05\n",
      "INFO:root:| epoch   8 | step 2680 | batch 216/352 | lr 0.00011 | loss 0.2554 | s/batch 7.05\n",
      "INFO:root:| epoch   8 | step 2683 | batch 219/352 | lr 0.00011 | loss 0.2320 | s/batch 9.59\n",
      "INFO:root:| epoch   8 | step 2686 | batch 222/352 | lr 0.00011 | loss 0.2094 | s/batch 6.78\n",
      "INFO:root:| epoch   8 | step 2689 | batch 225/352 | lr 0.00011 | loss 0.2667 | s/batch 3.40\n",
      "INFO:root:| epoch   8 | step 2692 | batch 228/352 | lr 0.00011 | loss 0.2606 | s/batch 6.44\n",
      "INFO:root:| epoch   8 | step 2695 | batch 231/352 | lr 0.00011 | loss 0.2153 | s/batch 5.56\n",
      "INFO:root:| epoch   8 | step 2698 | batch 234/352 | lr 0.00011 | loss 0.3071 | s/batch 13.82\n",
      "INFO:root:| epoch   8 | step 2701 | batch 237/352 | lr 0.00011 | loss 0.2342 | s/batch 6.27\n",
      "INFO:root:| epoch   8 | step 2704 | batch 240/352 | lr 0.00011 | loss 0.2460 | s/batch 12.10\n",
      "INFO:root:| epoch   8 | step 2707 | batch 243/352 | lr 0.00011 | loss 0.2674 | s/batch 13.86\n",
      "INFO:root:| epoch   8 | step 2710 | batch 246/352 | lr 0.00011 | loss 0.2317 | s/batch 12.62\n",
      "INFO:root:| epoch   8 | step 2713 | batch 249/352 | lr 0.00011 | loss 0.2519 | s/batch 5.63\n",
      "INFO:root:| epoch   8 | step 2716 | batch 252/352 | lr 0.00011 | loss 0.1844 | s/batch 10.54\n",
      "INFO:root:| epoch   8 | step 2719 | batch 255/352 | lr 0.00011 | loss 0.2195 | s/batch 7.44\n",
      "INFO:root:| epoch   8 | step 2722 | batch 258/352 | lr 0.00011 | loss 0.2278 | s/batch 8.72\n",
      "INFO:root:| epoch   8 | step 2725 | batch 261/352 | lr 0.00011 | loss 0.2462 | s/batch 3.41\n",
      "INFO:root:| epoch   8 | step 2728 | batch 264/352 | lr 0.00011 | loss 0.2214 | s/batch 9.81\n",
      "INFO:root:| epoch   8 | step 2731 | batch 267/352 | lr 0.00011 | loss 0.2167 | s/batch 4.37\n",
      "INFO:root:| epoch   8 | step 2734 | batch 270/352 | lr 0.00011 | loss 0.2445 | s/batch 2.69\n",
      "INFO:root:| epoch   8 | step 2737 | batch 273/352 | lr 0.00011 | loss 0.2537 | s/batch 2.27\n",
      "INFO:root:| epoch   8 | step 2740 | batch 276/352 | lr 0.00011 | loss 0.2315 | s/batch 9.53\n",
      "INFO:root:| epoch   8 | step 2743 | batch 279/352 | lr 0.00011 | loss 0.2507 | s/batch 12.91\n",
      "INFO:root:| epoch   8 | step 2746 | batch 282/352 | lr 0.00011 | loss 0.2480 | s/batch 7.22\n",
      "INFO:root:| epoch   8 | step 2749 | batch 285/352 | lr 0.00011 | loss 0.2676 | s/batch 8.91\n",
      "INFO:root:| epoch   8 | step 2752 | batch 288/352 | lr 0.00011 | loss 0.2424 | s/batch 6.95\n",
      "INFO:root:| epoch   8 | step 2755 | batch 291/352 | lr 0.00011 | loss 0.1964 | s/batch 4.95\n",
      "INFO:root:| epoch   8 | step 2758 | batch 294/352 | lr 0.00011 | loss 0.2539 | s/batch 5.11\n",
      "INFO:root:| epoch   8 | step 2761 | batch 297/352 | lr 0.00011 | loss 0.2519 | s/batch 4.34\n",
      "INFO:root:| epoch   8 | step 2764 | batch 300/352 | lr 0.00011 | loss 0.2278 | s/batch 4.89\n",
      "INFO:root:| epoch   8 | step 2767 | batch 303/352 | lr 0.00011 | loss 0.2665 | s/batch 13.74\n",
      "INFO:root:| epoch   8 | step 2770 | batch 306/352 | lr 0.00011 | loss 0.2697 | s/batch 8.53\n",
      "INFO:root:| epoch   8 | step 2773 | batch 309/352 | lr 0.00011 | loss 0.2295 | s/batch 6.39\n",
      "INFO:root:| epoch   8 | step 2776 | batch 312/352 | lr 0.00011 | loss 0.2684 | s/batch 14.85\n",
      "INFO:root:| epoch   8 | step 2779 | batch 315/352 | lr 0.00011 | loss 0.3336 | s/batch 13.68\n",
      "INFO:root:| epoch   8 | step 2782 | batch 318/352 | lr 0.00011 | loss 0.2081 | s/batch 8.91\n",
      "INFO:root:| epoch   8 | step 2785 | batch 321/352 | lr 0.00011 | loss 0.2691 | s/batch 9.93\n",
      "INFO:root:| epoch   8 | step 2788 | batch 324/352 | lr 0.00011 | loss 0.2564 | s/batch 4.48\n",
      "INFO:root:| epoch   8 | step 2791 | batch 327/352 | lr 0.00011 | loss 0.2275 | s/batch 5.03\n",
      "INFO:root:| epoch   8 | step 2794 | batch 330/352 | lr 0.00011 | loss 0.2933 | s/batch 9.29\n",
      "INFO:root:| epoch   8 | step 2797 | batch 333/352 | lr 0.00011 | loss 0.2274 | s/batch 8.08\n",
      "INFO:root:| epoch   8 | step 2800 | batch 336/352 | lr 0.00011 | loss 0.2428 | s/batch 5.67\n",
      "INFO:root:| epoch   8 | step 2803 | batch 339/352 | lr 0.00011 | loss 0.2234 | s/batch 6.91\n",
      "INFO:root:| epoch   8 | step 2806 | batch 342/352 | lr 0.00011 | loss 0.2881 | s/batch 11.65\n",
      "INFO:root:| epoch   8 | step 2809 | batch 345/352 | lr 0.00011 | loss 0.2091 | s/batch 5.30\n",
      "INFO:root:| epoch   8 | step 2812 | batch 348/352 | lr 0.00011 | loss 0.2147 | s/batch 4.21\n",
      "INFO:root:| epoch   8 | step 2815 | batch 351/352 | lr 0.00011 | loss 0.2816 | s/batch 1.74\n",
      "INFO:root:| epoch   8 | score (90.74, 89.5, 90.1) | f1 90.1 | loss 0.2482 | time 2759.69\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9238    0.9242    0.9240     35027\n",
      "          股票     0.9289    0.9383    0.9336     33251\n",
      "          体育     0.9798    0.9814    0.9806     28283\n",
      "          娱乐     0.9314    0.9424    0.9369     19920\n",
      "          时政     0.8554    0.8896    0.8721     13515\n",
      "          社会     0.8488    0.8376    0.8432     11009\n",
      "          教育     0.9235    0.9150    0.9192      8987\n",
      "          财经     0.8591    0.8052    0.8313      7957\n",
      "          家居     0.8891    0.8823    0.8857      7063\n",
      "          游戏     0.9079    0.8827    0.8951      5285\n",
      "          房产     0.9604    0.9413    0.9507      4428\n",
      "          时尚     0.8645    0.8538    0.8591      2818\n",
      "          彩票     0.9329    0.8908    0.9114      1639\n",
      "          星座     0.8986    0.8447    0.8708       818\n",
      "\n",
      "    accuracy                         0.9200    180000\n",
      "   macro avg     0.9074    0.8950    0.9010    180000\n",
      "weighted avg     0.9199    0.9200    0.9198    180000\n",
      "\n",
      "INFO:root:| epoch   8 | dev | score (92.73, 92.64, 92.64) | f1 92.64 | time 255.56\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9298    0.9463    0.9380      3891\n",
      "          股票     0.9470    0.9478    0.9474      3694\n",
      "          体育     0.9850    0.9825    0.9837      3142\n",
      "          娱乐     0.9588    0.9462    0.9525      2213\n",
      "          时政     0.8664    0.8987    0.8823      1501\n",
      "          社会     0.8681    0.8667    0.8674      1223\n",
      "          教育     0.9364    0.9439    0.9401       998\n",
      "          财经     0.9014    0.8484    0.8741       884\n",
      "          家居     0.9692    0.8839    0.9246       784\n",
      "          游戏     0.9392    0.9123    0.9256       593\n",
      "          房产     0.9939    0.9919    0.9929       492\n",
      "          时尚     0.8423    0.9553    0.8952       313\n",
      "          彩票     0.9341    0.9341    0.9341       182\n",
      "          星座     0.9111    0.9111    0.9111        90\n",
      "\n",
      "    accuracy                         0.9369     20000\n",
      "   macro avg     0.9273    0.9264    0.9264     20000\n",
      "weighted avg     0.9374    0.9369    0.9369     20000\n",
      "\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch   9 | step 2819 | batch   3/352 | lr 0.00011 | loss 0.2166 | s/batch 6.09\n",
      "INFO:root:| epoch   9 | step 2822 | batch   6/352 | lr 0.00011 | loss 0.2374 | s/batch 4.30\n",
      "INFO:root:| epoch   9 | step 2825 | batch   9/352 | lr 0.00011 | loss 0.2601 | s/batch 3.28\n",
      "INFO:root:| epoch   9 | step 2828 | batch  12/352 | lr 0.00011 | loss 0.2314 | s/batch 7.34\n",
      "INFO:root:| epoch   9 | step 2831 | batch  15/352 | lr 0.00011 | loss 0.1994 | s/batch 5.81\n",
      "INFO:root:| epoch   9 | step 2834 | batch  18/352 | lr 0.00011 | loss 0.2330 | s/batch 18.38\n",
      "INFO:root:| epoch   9 | step 2837 | batch  21/352 | lr 0.00011 | loss 0.2926 | s/batch 13.51\n",
      "INFO:root:| epoch   9 | step 2840 | batch  24/352 | lr 0.00011 | loss 0.2672 | s/batch 10.49\n",
      "INFO:root:| epoch   9 | step 2843 | batch  27/352 | lr 0.00011 | loss 0.2762 | s/batch 5.35\n",
      "INFO:root:| epoch   9 | step 2846 | batch  30/352 | lr 0.00011 | loss 0.2614 | s/batch 4.44\n",
      "INFO:root:| epoch   9 | step 2849 | batch  33/352 | lr 0.00011 | loss 0.2129 | s/batch 6.95\n",
      "INFO:root:| epoch   9 | step 2852 | batch  36/352 | lr 0.00011 | loss 0.2246 | s/batch 3.79\n",
      "INFO:root:| epoch   9 | step 2855 | batch  39/352 | lr 0.00011 | loss 0.2207 | s/batch 5.72\n",
      "INFO:root:| epoch   9 | step 2858 | batch  42/352 | lr 0.00011 | loss 0.2451 | s/batch 9.75\n",
      "INFO:root:| epoch   9 | step 2861 | batch  45/352 | lr 0.00011 | loss 0.2566 | s/batch 2.69\n",
      "INFO:root:| epoch   9 | step 2864 | batch  48/352 | lr 0.00011 | loss 0.2656 | s/batch 9.58\n",
      "INFO:root:| epoch   9 | step 2867 | batch  51/352 | lr 0.00011 | loss 0.2364 | s/batch 10.61\n",
      "INFO:root:| epoch   9 | step 2870 | batch  54/352 | lr 0.00011 | loss 0.2316 | s/batch 11.25\n",
      "INFO:root:| epoch   9 | step 2873 | batch  57/352 | lr 0.00011 | loss 0.2315 | s/batch 4.02\n",
      "INFO:root:| epoch   9 | step 2876 | batch  60/352 | lr 0.00011 | loss 0.2388 | s/batch 5.58\n",
      "INFO:root:| epoch   9 | step 2879 | batch  63/352 | lr 0.00011 | loss 0.2308 | s/batch 9.40\n",
      "INFO:root:| epoch   9 | step 2882 | batch  66/352 | lr 0.00011 | loss 0.2713 | s/batch 9.94\n",
      "INFO:root:| epoch   9 | step 2885 | batch  69/352 | lr 0.00011 | loss 0.2430 | s/batch 8.72\n",
      "INFO:root:| epoch   9 | step 2888 | batch  72/352 | lr 0.00011 | loss 0.2641 | s/batch 3.42\n",
      "INFO:root:| epoch   9 | step 2891 | batch  75/352 | lr 0.00011 | loss 0.2378 | s/batch 5.65\n",
      "INFO:root:| epoch   9 | step 2894 | batch  78/352 | lr 0.00011 | loss 0.2455 | s/batch 9.67\n",
      "INFO:root:| epoch   9 | step 2897 | batch  81/352 | lr 0.00011 | loss 0.2015 | s/batch 9.47\n",
      "INFO:root:| epoch   9 | step 2900 | batch  84/352 | lr 0.00011 | loss 0.2361 | s/batch 12.98\n",
      "INFO:root:| epoch   9 | step 2903 | batch  87/352 | lr 0.00011 | loss 0.2607 | s/batch 5.29\n",
      "INFO:root:| epoch   9 | step 2906 | batch  90/352 | lr 0.00011 | loss 0.2242 | s/batch 8.18\n",
      "INFO:root:| epoch   9 | step 2909 | batch  93/352 | lr 0.00011 | loss 0.2116 | s/batch 3.41\n",
      "INFO:root:| epoch   9 | step 2912 | batch  96/352 | lr 0.00011 | loss 0.2347 | s/batch 5.29\n",
      "INFO:root:| epoch   9 | step 2915 | batch  99/352 | lr 0.00011 | loss 0.2344 | s/batch 3.85\n",
      "INFO:root:| epoch   9 | step 2918 | batch 102/352 | lr 0.00011 | loss 0.2478 | s/batch 5.33\n",
      "INFO:root:| epoch   9 | step 2921 | batch 105/352 | lr 0.00011 | loss 0.3059 | s/batch 12.79\n",
      "INFO:root:| epoch   9 | step 2924 | batch 108/352 | lr 0.00011 | loss 0.2504 | s/batch 8.81\n",
      "INFO:root:| epoch   9 | step 2927 | batch 111/352 | lr 0.00011 | loss 0.2454 | s/batch 4.01\n",
      "INFO:root:| epoch   9 | step 2930 | batch 114/352 | lr 0.00011 | loss 0.2087 | s/batch 6.22\n",
      "INFO:root:| epoch   9 | step 2933 | batch 117/352 | lr 0.00011 | loss 0.2864 | s/batch 15.25\n",
      "INFO:root:| epoch   9 | step 2936 | batch 120/352 | lr 0.00011 | loss 0.1978 | s/batch 8.85\n",
      "INFO:root:| epoch   9 | step 2939 | batch 123/352 | lr 0.00011 | loss 0.2792 | s/batch 10.17\n",
      "INFO:root:| epoch   9 | step 2942 | batch 126/352 | lr 0.00011 | loss 0.2296 | s/batch 7.18\n",
      "INFO:root:| epoch   9 | step 2945 | batch 129/352 | lr 0.00011 | loss 0.2289 | s/batch 8.86\n",
      "INFO:root:| epoch   9 | step 2948 | batch 132/352 | lr 0.00011 | loss 0.2783 | s/batch 9.71\n",
      "INFO:root:| epoch   9 | step 2951 | batch 135/352 | lr 0.00011 | loss 0.2374 | s/batch 8.28\n",
      "INFO:root:| epoch   9 | step 2954 | batch 138/352 | lr 0.00011 | loss 0.2481 | s/batch 5.75\n",
      "INFO:root:| epoch   9 | step 2957 | batch 141/352 | lr 0.00011 | loss 0.2567 | s/batch 4.61\n",
      "INFO:root:| epoch   9 | step 2960 | batch 144/352 | lr 0.00011 | loss 0.2210 | s/batch 14.08\n",
      "INFO:root:| epoch   9 | step 2963 | batch 147/352 | lr 0.00011 | loss 0.2613 | s/batch 3.49\n",
      "INFO:root:| epoch   9 | step 2966 | batch 150/352 | lr 0.00011 | loss 0.2058 | s/batch 5.37\n",
      "INFO:root:| epoch   9 | step 2969 | batch 153/352 | lr 0.00011 | loss 0.2502 | s/batch 11.70\n",
      "INFO:root:| epoch   9 | step 2972 | batch 156/352 | lr 0.00011 | loss 0.2268 | s/batch 4.61\n",
      "INFO:root:| epoch   9 | step 2975 | batch 159/352 | lr 0.00011 | loss 0.2281 | s/batch 5.05\n",
      "INFO:root:| epoch   9 | step 2978 | batch 162/352 | lr 0.00011 | loss 0.2534 | s/batch 10.90\n",
      "INFO:root:| epoch   9 | step 2981 | batch 165/352 | lr 0.00011 | loss 0.2592 | s/batch 15.42\n",
      "INFO:root:| epoch   9 | step 2984 | batch 168/352 | lr 0.00011 | loss 0.2442 | s/batch 10.36\n",
      "INFO:root:| epoch   9 | step 2987 | batch 171/352 | lr 0.00011 | loss 0.2155 | s/batch 9.14\n",
      "INFO:root:| epoch   9 | step 2990 | batch 174/352 | lr 0.00011 | loss 0.2311 | s/batch 3.83\n",
      "INFO:root:| epoch   9 | step 2993 | batch 177/352 | lr 0.00011 | loss 0.2382 | s/batch 9.86\n",
      "INFO:root:| epoch   9 | step 2996 | batch 180/352 | lr 0.00011 | loss 0.2131 | s/batch 4.40\n",
      "INFO:root:| epoch   9 | step 2999 | batch 183/352 | lr 0.00011 | loss 0.2265 | s/batch 11.69\n",
      "INFO:root:| epoch   9 | step 3002 | batch 186/352 | lr 0.00008 | loss 0.1974 | s/batch 8.96\n",
      "INFO:root:| epoch   9 | step 3005 | batch 189/352 | lr 0.00008 | loss 0.2766 | s/batch 10.04\n",
      "INFO:root:| epoch   9 | step 3008 | batch 192/352 | lr 0.00008 | loss 0.2568 | s/batch 4.07\n",
      "INFO:root:| epoch   9 | step 3011 | batch 195/352 | lr 0.00008 | loss 0.2752 | s/batch 9.98\n",
      "INFO:root:| epoch   9 | step 3014 | batch 198/352 | lr 0.00008 | loss 0.2753 | s/batch 12.65\n",
      "INFO:root:| epoch   9 | step 3017 | batch 201/352 | lr 0.00008 | loss 0.2157 | s/batch 6.48\n",
      "INFO:root:| epoch   9 | step 3020 | batch 204/352 | lr 0.00008 | loss 0.2422 | s/batch 7.26\n",
      "INFO:root:| epoch   9 | step 3023 | batch 207/352 | lr 0.00008 | loss 0.1990 | s/batch 5.63\n",
      "INFO:root:| epoch   9 | step 3026 | batch 210/352 | lr 0.00008 | loss 0.2629 | s/batch 10.76\n",
      "INFO:root:| epoch   9 | step 3029 | batch 213/352 | lr 0.00008 | loss 0.2511 | s/batch 5.34\n",
      "INFO:root:| epoch   9 | step 3032 | batch 216/352 | lr 0.00008 | loss 0.2226 | s/batch 8.40\n",
      "INFO:root:| epoch   9 | step 3035 | batch 219/352 | lr 0.00008 | loss 0.2503 | s/batch 10.66\n",
      "INFO:root:| epoch   9 | step 3038 | batch 222/352 | lr 0.00008 | loss 0.2483 | s/batch 4.37\n",
      "INFO:root:| epoch   9 | step 3041 | batch 225/352 | lr 0.00008 | loss 0.2674 | s/batch 3.64\n",
      "INFO:root:| epoch   9 | step 3044 | batch 228/352 | lr 0.00008 | loss 0.2122 | s/batch 3.76\n",
      "INFO:root:| epoch   9 | step 3047 | batch 231/352 | lr 0.00008 | loss 0.2634 | s/batch 9.43\n",
      "INFO:root:| epoch   9 | step 3050 | batch 234/352 | lr 0.00008 | loss 0.2136 | s/batch 8.46\n",
      "INFO:root:| epoch   9 | step 3053 | batch 237/352 | lr 0.00008 | loss 0.2438 | s/batch 8.62\n",
      "INFO:root:| epoch   9 | step 3056 | batch 240/352 | lr 0.00008 | loss 0.2263 | s/batch 2.74\n",
      "INFO:root:| epoch   9 | step 3059 | batch 243/352 | lr 0.00008 | loss 0.1958 | s/batch 6.92\n",
      "INFO:root:| epoch   9 | step 3062 | batch 246/352 | lr 0.00008 | loss 0.2223 | s/batch 6.47\n",
      "INFO:root:| epoch   9 | step 3065 | batch 249/352 | lr 0.00008 | loss 0.2648 | s/batch 8.07\n",
      "INFO:root:| epoch   9 | step 3068 | batch 252/352 | lr 0.00008 | loss 0.2603 | s/batch 3.94\n",
      "INFO:root:| epoch   9 | step 3071 | batch 255/352 | lr 0.00008 | loss 0.2496 | s/batch 10.48\n",
      "INFO:root:| epoch   9 | step 3074 | batch 258/352 | lr 0.00008 | loss 0.2034 | s/batch 6.87\n",
      "INFO:root:| epoch   9 | step 3077 | batch 261/352 | lr 0.00008 | loss 0.2281 | s/batch 5.68\n",
      "INFO:root:| epoch   9 | step 3080 | batch 264/352 | lr 0.00008 | loss 0.2533 | s/batch 11.47\n",
      "INFO:root:| epoch   9 | step 3083 | batch 267/352 | lr 0.00008 | loss 0.2398 | s/batch 11.81\n",
      "INFO:root:| epoch   9 | step 3086 | batch 270/352 | lr 0.00008 | loss 0.2309 | s/batch 3.86\n",
      "INFO:root:| epoch   9 | step 3089 | batch 273/352 | lr 0.00008 | loss 0.2566 | s/batch 8.05\n",
      "INFO:root:| epoch   9 | step 3092 | batch 276/352 | lr 0.00008 | loss 0.2251 | s/batch 5.12\n",
      "INFO:root:| epoch   9 | step 3095 | batch 279/352 | lr 0.00008 | loss 0.2487 | s/batch 3.93\n",
      "INFO:root:| epoch   9 | step 3098 | batch 282/352 | lr 0.00008 | loss 0.2743 | s/batch 9.69\n",
      "INFO:root:| epoch   9 | step 3101 | batch 285/352 | lr 0.00008 | loss 0.2101 | s/batch 7.11\n",
      "INFO:root:| epoch   9 | step 3104 | batch 288/352 | lr 0.00008 | loss 0.2357 | s/batch 4.99\n",
      "INFO:root:| epoch   9 | step 3107 | batch 291/352 | lr 0.00008 | loss 0.2243 | s/batch 5.68\n",
      "INFO:root:| epoch   9 | step 3110 | batch 294/352 | lr 0.00008 | loss 0.2147 | s/batch 5.11\n",
      "INFO:root:| epoch   9 | step 3113 | batch 297/352 | lr 0.00008 | loss 0.2523 | s/batch 4.02\n",
      "INFO:root:| epoch   9 | step 3116 | batch 300/352 | lr 0.00008 | loss 0.3024 | s/batch 8.14\n",
      "INFO:root:| epoch   9 | step 3119 | batch 303/352 | lr 0.00008 | loss 0.2638 | s/batch 13.37\n",
      "INFO:root:| epoch   9 | step 3122 | batch 306/352 | lr 0.00008 | loss 0.2572 | s/batch 3.85\n",
      "INFO:root:| epoch   9 | step 3125 | batch 309/352 | lr 0.00008 | loss 0.2011 | s/batch 5.63\n",
      "INFO:root:| epoch   9 | step 3128 | batch 312/352 | lr 0.00008 | loss 0.2525 | s/batch 5.59\n",
      "INFO:root:| epoch   9 | step 3131 | batch 315/352 | lr 0.00008 | loss 0.2513 | s/batch 7.12\n",
      "INFO:root:| epoch   9 | step 3134 | batch 318/352 | lr 0.00008 | loss 0.2266 | s/batch 14.79\n",
      "INFO:root:| epoch   9 | step 3137 | batch 321/352 | lr 0.00008 | loss 0.2131 | s/batch 9.23\n",
      "INFO:root:| epoch   9 | step 3140 | batch 324/352 | lr 0.00008 | loss 0.2530 | s/batch 11.54\n",
      "INFO:root:| epoch   9 | step 3143 | batch 327/352 | lr 0.00008 | loss 0.2817 | s/batch 14.85\n",
      "INFO:root:| epoch   9 | step 3146 | batch 330/352 | lr 0.00008 | loss 0.2117 | s/batch 7.34\n",
      "INFO:root:| epoch   9 | step 3149 | batch 333/352 | lr 0.00008 | loss 0.2868 | s/batch 11.37\n",
      "INFO:root:| epoch   9 | step 3152 | batch 336/352 | lr 0.00008 | loss 0.2165 | s/batch 10.56\n",
      "INFO:root:| epoch   9 | step 3155 | batch 339/352 | lr 0.00008 | loss 0.1980 | s/batch 6.38\n",
      "INFO:root:| epoch   9 | step 3158 | batch 342/352 | lr 0.00008 | loss 0.2307 | s/batch 9.09\n",
      "INFO:root:| epoch   9 | step 3161 | batch 345/352 | lr 0.00008 | loss 0.2100 | s/batch 4.49\n",
      "INFO:root:| epoch   9 | step 3164 | batch 348/352 | lr 0.00008 | loss 0.2394 | s/batch 3.68\n",
      "INFO:root:| epoch   9 | step 3167 | batch 351/352 | lr 0.00008 | loss 0.2146 | s/batch 11.88\n",
      "INFO:root:| epoch   9 | score (91.11, 89.94, 90.51) | f1 90.51 | loss 0.2403 | time 2734.40\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9245    0.9251    0.9248     35027\n",
      "          股票     0.9320    0.9391    0.9355     33251\n",
      "          体育     0.9801    0.9819    0.9810     28283\n",
      "          娱乐     0.9331    0.9435    0.9383     19920\n",
      "          时政     0.8602    0.8923    0.8760     13515\n",
      "          社会     0.8536    0.8436    0.8486     11009\n",
      "          教育     0.9309    0.9236    0.9272      8987\n",
      "          财经     0.8662    0.8174    0.8411      7957\n",
      "          家居     0.8865    0.8872    0.8868      7063\n",
      "          游戏     0.9079    0.8808    0.8942      5285\n",
      "          房产     0.9669    0.9494    0.9581      4428\n",
      "          时尚     0.8753    0.8591    0.8671      2818\n",
      "          彩票     0.9331    0.8853    0.9086      1639\n",
      "          星座     0.9051    0.8631    0.8836       818\n",
      "\n",
      "    accuracy                         0.9225    180000\n",
      "   macro avg     0.9111    0.8994    0.9051    180000\n",
      "weighted avg     0.9224    0.9225    0.9224    180000\n",
      "\n",
      "INFO:root:| epoch   9 | dev | score (93.34, 92.74, 93.01) | f1 93.01 | time 259.55\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9397    0.9373    0.9385      3891\n",
      "          股票     0.9483    0.9437    0.9460      3694\n",
      "          体育     0.9847    0.9850    0.9849      3142\n",
      "          娱乐     0.9652    0.9404    0.9526      2213\n",
      "          时政     0.8722    0.8954    0.8836      1501\n",
      "          社会     0.8221    0.8953    0.8571      1223\n",
      "          教育     0.9508    0.9299    0.9402       998\n",
      "          财经     0.8693    0.8801    0.8746       884\n",
      "          家居     0.9409    0.9145    0.9276       784\n",
      "          游戏     0.9540    0.9089    0.9309       593\n",
      "          房产     0.9919    0.9919    0.9919       492\n",
      "          时尚     0.9034    0.9265    0.9148       313\n",
      "          彩票     0.9600    0.9231    0.9412       182\n",
      "          星座     0.9647    0.9111    0.9371        90\n",
      "\n",
      "    accuracy                         0.9369     20000\n",
      "   macro avg     0.9334    0.9274    0.9301     20000\n",
      "weighted avg     0.9379    0.9369    0.9372     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 92.68, current dev = 93.01\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch  10 | step 3171 | batch   3/352 | lr 0.00008 | loss 0.2849 | s/batch 16.79\n",
      "INFO:root:| epoch  10 | step 3174 | batch   6/352 | lr 0.00008 | loss 0.2244 | s/batch 5.23\n",
      "INFO:root:| epoch  10 | step 3177 | batch   9/352 | lr 0.00008 | loss 0.2261 | s/batch 9.60\n",
      "INFO:root:| epoch  10 | step 3180 | batch  12/352 | lr 0.00008 | loss 0.2479 | s/batch 15.56\n",
      "INFO:root:| epoch  10 | step 3183 | batch  15/352 | lr 0.00008 | loss 0.2461 | s/batch 9.55\n",
      "INFO:root:| epoch  10 | step 3186 | batch  18/352 | lr 0.00008 | loss 0.2197 | s/batch 8.00\n",
      "INFO:root:| epoch  10 | step 3189 | batch  21/352 | lr 0.00008 | loss 0.2062 | s/batch 3.06\n",
      "INFO:root:| epoch  10 | step 3192 | batch  24/352 | lr 0.00008 | loss 0.2206 | s/batch 12.76\n",
      "INFO:root:| epoch  10 | step 3195 | batch  27/352 | lr 0.00008 | loss 0.2444 | s/batch 11.75\n",
      "INFO:root:| epoch  10 | step 3198 | batch  30/352 | lr 0.00008 | loss 0.2352 | s/batch 5.44\n",
      "INFO:root:| epoch  10 | step 3201 | batch  33/352 | lr 0.00008 | loss 0.2114 | s/batch 10.89\n",
      "INFO:root:| epoch  10 | step 3204 | batch  36/352 | lr 0.00008 | loss 0.2588 | s/batch 3.47\n",
      "INFO:root:| epoch  10 | step 3207 | batch  39/352 | lr 0.00008 | loss 0.2678 | s/batch 13.07\n",
      "INFO:root:| epoch  10 | step 3210 | batch  42/352 | lr 0.00008 | loss 0.2521 | s/batch 9.86\n",
      "INFO:root:| epoch  10 | step 3213 | batch  45/352 | lr 0.00008 | loss 0.2223 | s/batch 3.96\n",
      "INFO:root:| epoch  10 | step 3216 | batch  48/352 | lr 0.00008 | loss 0.2369 | s/batch 4.42\n",
      "INFO:root:| epoch  10 | step 3219 | batch  51/352 | lr 0.00008 | loss 0.2176 | s/batch 5.25\n",
      "INFO:root:| epoch  10 | step 3222 | batch  54/352 | lr 0.00008 | loss 0.2664 | s/batch 8.77\n",
      "INFO:root:| epoch  10 | step 3225 | batch  57/352 | lr 0.00008 | loss 0.2586 | s/batch 2.75\n",
      "INFO:root:| epoch  10 | step 3228 | batch  60/352 | lr 0.00008 | loss 0.2322 | s/batch 3.70\n",
      "INFO:root:| epoch  10 | step 3231 | batch  63/352 | lr 0.00008 | loss 0.2392 | s/batch 7.65\n",
      "INFO:root:| epoch  10 | step 3234 | batch  66/352 | lr 0.00008 | loss 0.2432 | s/batch 3.69\n",
      "INFO:root:| epoch  10 | step 3237 | batch  69/352 | lr 0.00008 | loss 0.2338 | s/batch 3.17\n",
      "INFO:root:| epoch  10 | step 3240 | batch  72/352 | lr 0.00008 | loss 0.1922 | s/batch 9.56\n",
      "INFO:root:| epoch  10 | step 3243 | batch  75/352 | lr 0.00008 | loss 0.2236 | s/batch 9.38\n",
      "INFO:root:| epoch  10 | step 3246 | batch  78/352 | lr 0.00008 | loss 0.2101 | s/batch 6.26\n",
      "INFO:root:| epoch  10 | step 3249 | batch  81/352 | lr 0.00008 | loss 0.1904 | s/batch 6.17\n",
      "INFO:root:| epoch  10 | step 3252 | batch  84/352 | lr 0.00008 | loss 0.2357 | s/batch 6.04\n",
      "INFO:root:| epoch  10 | step 3255 | batch  87/352 | lr 0.00008 | loss 0.1963 | s/batch 6.37\n",
      "INFO:root:| epoch  10 | step 3258 | batch  90/352 | lr 0.00008 | loss 0.2175 | s/batch 3.99\n",
      "INFO:root:| epoch  10 | step 3261 | batch  93/352 | lr 0.00008 | loss 0.2278 | s/batch 6.96\n",
      "INFO:root:| epoch  10 | step 3264 | batch  96/352 | lr 0.00008 | loss 0.2360 | s/batch 3.79\n",
      "INFO:root:| epoch  10 | step 3267 | batch  99/352 | lr 0.00008 | loss 0.2233 | s/batch 4.98\n",
      "INFO:root:| epoch  10 | step 3270 | batch 102/352 | lr 0.00008 | loss 0.2968 | s/batch 13.71\n",
      "INFO:root:| epoch  10 | step 3273 | batch 105/352 | lr 0.00008 | loss 0.2633 | s/batch 3.90\n",
      "INFO:root:| epoch  10 | step 3276 | batch 108/352 | lr 0.00008 | loss 0.1999 | s/batch 8.50\n",
      "INFO:root:| epoch  10 | step 3279 | batch 111/352 | lr 0.00008 | loss 0.2183 | s/batch 5.03\n",
      "INFO:root:| epoch  10 | step 3282 | batch 114/352 | lr 0.00008 | loss 0.2755 | s/batch 11.06\n",
      "INFO:root:| epoch  10 | step 3285 | batch 117/352 | lr 0.00008 | loss 0.2229 | s/batch 9.84\n",
      "INFO:root:| epoch  10 | step 3288 | batch 120/352 | lr 0.00008 | loss 0.2213 | s/batch 5.28\n",
      "INFO:root:| epoch  10 | step 3291 | batch 123/352 | lr 0.00008 | loss 0.2174 | s/batch 7.83\n",
      "INFO:root:| epoch  10 | step 3294 | batch 126/352 | lr 0.00008 | loss 0.2174 | s/batch 9.65\n",
      "INFO:root:| epoch  10 | step 3297 | batch 129/352 | lr 0.00008 | loss 0.2153 | s/batch 5.02\n",
      "INFO:root:| epoch  10 | step 3300 | batch 132/352 | lr 0.00008 | loss 0.2420 | s/batch 3.38\n",
      "INFO:root:| epoch  10 | step 3303 | batch 135/352 | lr 0.00008 | loss 0.2369 | s/batch 3.20\n",
      "INFO:root:| epoch  10 | step 3306 | batch 138/352 | lr 0.00008 | loss 0.2592 | s/batch 10.50\n",
      "INFO:root:| epoch  10 | step 3309 | batch 141/352 | lr 0.00008 | loss 0.2194 | s/batch 7.14\n",
      "INFO:root:| epoch  10 | step 3312 | batch 144/352 | lr 0.00008 | loss 0.2378 | s/batch 5.68\n",
      "INFO:root:| epoch  10 | step 3315 | batch 147/352 | lr 0.00008 | loss 0.2435 | s/batch 9.29\n",
      "INFO:root:| epoch  10 | step 3318 | batch 150/352 | lr 0.00008 | loss 0.2396 | s/batch 2.18\n",
      "INFO:root:| epoch  10 | step 3321 | batch 153/352 | lr 0.00008 | loss 0.2522 | s/batch 3.98\n",
      "INFO:root:| epoch  10 | step 3324 | batch 156/352 | lr 0.00008 | loss 0.2478 | s/batch 4.99\n",
      "INFO:root:| epoch  10 | step 3327 | batch 159/352 | lr 0.00008 | loss 0.2958 | s/batch 18.37\n",
      "INFO:root:| epoch  10 | step 3330 | batch 162/352 | lr 0.00008 | loss 0.2274 | s/batch 12.50\n",
      "INFO:root:| epoch  10 | step 3333 | batch 165/352 | lr 0.00008 | loss 0.2315 | s/batch 4.98\n",
      "INFO:root:| epoch  10 | step 3336 | batch 168/352 | lr 0.00008 | loss 0.1968 | s/batch 7.33\n",
      "INFO:root:| epoch  10 | step 3339 | batch 171/352 | lr 0.00008 | loss 0.2605 | s/batch 9.83\n",
      "INFO:root:| epoch  10 | step 3342 | batch 174/352 | lr 0.00008 | loss 0.2405 | s/batch 4.03\n",
      "INFO:root:| epoch  10 | step 3345 | batch 177/352 | lr 0.00008 | loss 0.2699 | s/batch 8.35\n",
      "INFO:root:| epoch  10 | step 3348 | batch 180/352 | lr 0.00008 | loss 0.2428 | s/batch 14.04\n",
      "INFO:root:| epoch  10 | step 3351 | batch 183/352 | lr 0.00008 | loss 0.2507 | s/batch 11.43\n",
      "INFO:root:| epoch  10 | step 3354 | batch 186/352 | lr 0.00008 | loss 0.2815 | s/batch 4.04\n",
      "INFO:root:| epoch  10 | step 3357 | batch 189/352 | lr 0.00008 | loss 0.2190 | s/batch 5.63\n",
      "INFO:root:| epoch  10 | step 3360 | batch 192/352 | lr 0.00008 | loss 0.2301 | s/batch 5.48\n",
      "INFO:root:| epoch  10 | step 3363 | batch 195/352 | lr 0.00008 | loss 0.2097 | s/batch 4.34\n",
      "INFO:root:| epoch  10 | step 3366 | batch 198/352 | lr 0.00008 | loss 0.2470 | s/batch 5.19\n",
      "INFO:root:| epoch  10 | step 3369 | batch 201/352 | lr 0.00008 | loss 0.2739 | s/batch 3.80\n",
      "INFO:root:| epoch  10 | step 3372 | batch 204/352 | lr 0.00008 | loss 0.2725 | s/batch 8.96\n",
      "INFO:root:| epoch  10 | step 3375 | batch 207/352 | lr 0.00008 | loss 0.2576 | s/batch 3.69\n",
      "INFO:root:| epoch  10 | step 3378 | batch 210/352 | lr 0.00008 | loss 0.2364 | s/batch 4.48\n",
      "INFO:root:| epoch  10 | step 3381 | batch 213/352 | lr 0.00008 | loss 0.2587 | s/batch 18.72\n",
      "INFO:root:| epoch  10 | step 3384 | batch 216/352 | lr 0.00008 | loss 0.2400 | s/batch 5.32\n",
      "INFO:root:| epoch  10 | step 3387 | batch 219/352 | lr 0.00008 | loss 0.2388 | s/batch 12.86\n",
      "INFO:root:| epoch  10 | step 3390 | batch 222/352 | lr 0.00008 | loss 0.2398 | s/batch 10.08\n",
      "INFO:root:| epoch  10 | step 3393 | batch 225/352 | lr 0.00008 | loss 0.2619 | s/batch 9.72\n",
      "INFO:root:| epoch  10 | step 3396 | batch 228/352 | lr 0.00008 | loss 0.2396 | s/batch 10.78\n",
      "INFO:root:| epoch  10 | step 3399 | batch 231/352 | lr 0.00008 | loss 0.2438 | s/batch 10.20\n",
      "INFO:root:| epoch  10 | step 3402 | batch 234/352 | lr 0.00008 | loss 0.2169 | s/batch 5.77\n",
      "INFO:root:| epoch  10 | step 3405 | batch 237/352 | lr 0.00008 | loss 0.2462 | s/batch 12.37\n",
      "INFO:root:| epoch  10 | step 3408 | batch 240/352 | lr 0.00008 | loss 0.2300 | s/batch 5.72\n",
      "INFO:root:| epoch  10 | step 3411 | batch 243/352 | lr 0.00008 | loss 0.2045 | s/batch 7.78\n",
      "INFO:root:| epoch  10 | step 3414 | batch 246/352 | lr 0.00008 | loss 0.2424 | s/batch 2.76\n",
      "INFO:root:| epoch  10 | step 3417 | batch 249/352 | lr 0.00008 | loss 0.2257 | s/batch 13.49\n",
      "INFO:root:| epoch  10 | step 3420 | batch 252/352 | lr 0.00008 | loss 0.2094 | s/batch 13.36\n",
      "INFO:root:| epoch  10 | step 3423 | batch 255/352 | lr 0.00008 | loss 0.2578 | s/batch 8.17\n",
      "INFO:root:| epoch  10 | step 3426 | batch 258/352 | lr 0.00008 | loss 0.2046 | s/batch 5.05\n",
      "INFO:root:| epoch  10 | step 3429 | batch 261/352 | lr 0.00008 | loss 0.2567 | s/batch 8.91\n",
      "INFO:root:| epoch  10 | step 3432 | batch 264/352 | lr 0.00008 | loss 0.2207 | s/batch 8.51\n",
      "INFO:root:| epoch  10 | step 3435 | batch 267/352 | lr 0.00008 | loss 0.2217 | s/batch 8.24\n",
      "INFO:root:| epoch  10 | step 3438 | batch 270/352 | lr 0.00008 | loss 0.2204 | s/batch 6.29\n",
      "INFO:root:| epoch  10 | step 3441 | batch 273/352 | lr 0.00008 | loss 0.2707 | s/batch 8.91\n",
      "INFO:root:| epoch  10 | step 3444 | batch 276/352 | lr 0.00008 | loss 0.2509 | s/batch 10.05\n",
      "INFO:root:| epoch  10 | step 3447 | batch 279/352 | lr 0.00008 | loss 0.2448 | s/batch 6.71\n",
      "INFO:root:| epoch  10 | step 3450 | batch 282/352 | lr 0.00008 | loss 0.1911 | s/batch 5.68\n",
      "INFO:root:| epoch  10 | step 3453 | batch 285/352 | lr 0.00008 | loss 0.2548 | s/batch 10.50\n",
      "INFO:root:| epoch  10 | step 3456 | batch 288/352 | lr 0.00008 | loss 0.2422 | s/batch 6.10\n",
      "INFO:root:| epoch  10 | step 3459 | batch 291/352 | lr 0.00008 | loss 0.2509 | s/batch 4.67\n",
      "INFO:root:| epoch  10 | step 3462 | batch 294/352 | lr 0.00008 | loss 0.1886 | s/batch 6.42\n",
      "INFO:root:| epoch  10 | step 3465 | batch 297/352 | lr 0.00008 | loss 0.2095 | s/batch 6.07\n",
      "INFO:root:| epoch  10 | step 3468 | batch 300/352 | lr 0.00008 | loss 0.2188 | s/batch 5.10\n",
      "INFO:root:| epoch  10 | step 3471 | batch 303/352 | lr 0.00008 | loss 0.2265 | s/batch 10.81\n",
      "INFO:root:| epoch  10 | step 3474 | batch 306/352 | lr 0.00008 | loss 0.2475 | s/batch 12.44\n",
      "INFO:root:| epoch  10 | step 3477 | batch 309/352 | lr 0.00008 | loss 0.2127 | s/batch 10.16\n",
      "INFO:root:| epoch  10 | step 3480 | batch 312/352 | lr 0.00008 | loss 0.2519 | s/batch 3.46\n",
      "INFO:root:| epoch  10 | step 3483 | batch 315/352 | lr 0.00008 | loss 0.2535 | s/batch 8.24\n",
      "INFO:root:| epoch  10 | step 3486 | batch 318/352 | lr 0.00008 | loss 0.2590 | s/batch 11.40\n",
      "INFO:root:| epoch  10 | step 3489 | batch 321/352 | lr 0.00008 | loss 0.3052 | s/batch 20.88\n",
      "INFO:root:| epoch  10 | step 3492 | batch 324/352 | lr 0.00008 | loss 0.2140 | s/batch 7.67\n",
      "INFO:root:| epoch  10 | step 3495 | batch 327/352 | lr 0.00008 | loss 0.2170 | s/batch 7.79\n",
      "INFO:root:| epoch  10 | step 3498 | batch 330/352 | lr 0.00008 | loss 0.2339 | s/batch 10.60\n",
      "INFO:root:| epoch  10 | step 3501 | batch 333/352 | lr 0.00008 | loss 0.2117 | s/batch 5.10\n",
      "INFO:root:| epoch  10 | step 3504 | batch 336/352 | lr 0.00008 | loss 0.2498 | s/batch 10.61\n",
      "INFO:root:| epoch  10 | step 3507 | batch 339/352 | lr 0.00008 | loss 0.2064 | s/batch 5.14\n",
      "INFO:root:| epoch  10 | step 3510 | batch 342/352 | lr 0.00008 | loss 0.2344 | s/batch 4.38\n",
      "INFO:root:| epoch  10 | step 3513 | batch 345/352 | lr 0.00008 | loss 0.2243 | s/batch 4.60\n",
      "INFO:root:| epoch  10 | step 3516 | batch 348/352 | lr 0.00008 | loss 0.2509 | s/batch 5.23\n",
      "INFO:root:| epoch  10 | step 3519 | batch 351/352 | lr 0.00008 | loss 0.2612 | s/batch 13.28\n",
      "INFO:root:| epoch  10 | score (91.05, 90.02, 90.52) | f1 90.52 | loss 0.2368 | time 2762.80\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9268    0.9277    0.9272     35027\n",
      "          股票     0.9335    0.9409    0.9372     33251\n",
      "          体育     0.9805    0.9809    0.9807     28283\n",
      "          娱乐     0.9307    0.9428    0.9367     19920\n",
      "          时政     0.8641    0.8921    0.8779     13515\n",
      "          社会     0.8505    0.8460    0.8483     11009\n",
      "          教育     0.9312    0.9213    0.9262      8987\n",
      "          财经     0.8647    0.8136    0.8384      7957\n",
      "          家居     0.8942    0.8898    0.8920      7063\n",
      "          游戏     0.9099    0.8829    0.8962      5285\n",
      "          房产     0.9734    0.9603    0.9668      4428\n",
      "          时尚     0.8710    0.8602    0.8656      2818\n",
      "          彩票     0.9358    0.8975    0.9162      1639\n",
      "          星座     0.8806    0.8472    0.8636       818\n",
      "\n",
      "    accuracy                         0.9234    180000\n",
      "   macro avg     0.9105    0.9002    0.9052    180000\n",
      "weighted avg     0.9234    0.9234    0.9233    180000\n",
      "\n",
      "INFO:root:| epoch  10 | dev | score (93.65, 92.57, 93.08) | f1 93.08 | time 258.22\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9349    0.9489    0.9418      3891\n",
      "          股票     0.9527    0.9437    0.9482      3694\n",
      "          体育     0.9817    0.9879    0.9848      3142\n",
      "          娱乐     0.9499    0.9602    0.9551      2213\n",
      "          时政     0.8741    0.9021    0.8879      1501\n",
      "          社会     0.8800    0.8757    0.8779      1223\n",
      "          教育     0.9425    0.9369    0.9397       998\n",
      "          财经     0.8904    0.8643    0.8772       884\n",
      "          家居     0.9444    0.9107    0.9273       784\n",
      "          游戏     0.9686    0.8836    0.9242       593\n",
      "          房产     0.9939    0.9878    0.9908       492\n",
      "          时尚     0.8739    0.9297    0.9009       313\n",
      "          彩票     0.9709    0.9176    0.9435       182\n",
      "          星座     0.9535    0.9111    0.9318        90\n",
      "\n",
      "    accuracy                         0.9397     20000\n",
      "   macro avg     0.9365    0.9257    0.9308     20000\n",
      "weighted avg     0.9400    0.9397    0.9397     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 93.01, current dev = 93.08\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch  11 | step 3523 | batch   3/352 | lr 0.00008 | loss 0.2389 | s/batch 2.95\n",
      "INFO:root:| epoch  11 | step 3526 | batch   6/352 | lr 0.00008 | loss 0.2049 | s/batch 15.03\n",
      "INFO:root:| epoch  11 | step 3529 | batch   9/352 | lr 0.00008 | loss 0.2772 | s/batch 12.31\n",
      "INFO:root:| epoch  11 | step 3532 | batch  12/352 | lr 0.00008 | loss 0.2064 | s/batch 5.38\n",
      "INFO:root:| epoch  11 | step 3535 | batch  15/352 | lr 0.00008 | loss 0.2102 | s/batch 12.89\n",
      "INFO:root:| epoch  11 | step 3538 | batch  18/352 | lr 0.00008 | loss 0.2121 | s/batch 6.76\n",
      "INFO:root:| epoch  11 | step 3541 | batch  21/352 | lr 0.00008 | loss 0.2422 | s/batch 10.07\n",
      "INFO:root:| epoch  11 | step 3544 | batch  24/352 | lr 0.00008 | loss 0.1923 | s/batch 11.49\n",
      "INFO:root:| epoch  11 | step 3547 | batch  27/352 | lr 0.00008 | loss 0.2651 | s/batch 8.59\n",
      "INFO:root:| epoch  11 | step 3550 | batch  30/352 | lr 0.00008 | loss 0.2476 | s/batch 9.66\n",
      "INFO:root:| epoch  11 | step 3553 | batch  33/352 | lr 0.00008 | loss 0.2194 | s/batch 11.39\n",
      "INFO:root:| epoch  11 | step 3556 | batch  36/352 | lr 0.00008 | loss 0.2240 | s/batch 6.45\n",
      "INFO:root:| epoch  11 | step 3559 | batch  39/352 | lr 0.00008 | loss 0.2454 | s/batch 4.62\n",
      "INFO:root:| epoch  11 | step 3562 | batch  42/352 | lr 0.00008 | loss 0.2470 | s/batch 3.24\n",
      "INFO:root:| epoch  11 | step 3565 | batch  45/352 | lr 0.00008 | loss 0.1932 | s/batch 8.85\n",
      "INFO:root:| epoch  11 | step 3568 | batch  48/352 | lr 0.00008 | loss 0.2289 | s/batch 9.79\n",
      "INFO:root:| epoch  11 | step 3571 | batch  51/352 | lr 0.00008 | loss 0.2662 | s/batch 2.41\n",
      "INFO:root:| epoch  11 | step 3574 | batch  54/352 | lr 0.00008 | loss 0.2219 | s/batch 4.11\n",
      "INFO:root:| epoch  11 | step 3577 | batch  57/352 | lr 0.00008 | loss 0.1701 | s/batch 5.66\n",
      "INFO:root:| epoch  11 | step 3580 | batch  60/352 | lr 0.00008 | loss 0.2498 | s/batch 9.33\n",
      "INFO:root:| epoch  11 | step 3583 | batch  63/352 | lr 0.00008 | loss 0.2250 | s/batch 5.08\n",
      "INFO:root:| epoch  11 | step 3586 | batch  66/352 | lr 0.00008 | loss 0.2459 | s/batch 8.39\n",
      "INFO:root:| epoch  11 | step 3589 | batch  69/352 | lr 0.00008 | loss 0.2556 | s/batch 6.13\n",
      "INFO:root:| epoch  11 | step 3592 | batch  72/352 | lr 0.00008 | loss 0.2277 | s/batch 4.73\n",
      "INFO:root:| epoch  11 | step 3595 | batch  75/352 | lr 0.00008 | loss 0.2056 | s/batch 8.80\n",
      "INFO:root:| epoch  11 | step 3598 | batch  78/352 | lr 0.00008 | loss 0.2410 | s/batch 8.47\n",
      "INFO:root:| epoch  11 | step 3601 | batch  81/352 | lr 0.00008 | loss 0.2290 | s/batch 3.06\n",
      "INFO:root:| epoch  11 | step 3604 | batch  84/352 | lr 0.00008 | loss 0.1988 | s/batch 6.73\n",
      "INFO:root:| epoch  11 | step 3607 | batch  87/352 | lr 0.00008 | loss 0.2374 | s/batch 9.96\n",
      "INFO:root:| epoch  11 | step 3610 | batch  90/352 | lr 0.00008 | loss 0.2542 | s/batch 10.01\n",
      "INFO:root:| epoch  11 | step 3613 | batch  93/352 | lr 0.00008 | loss 0.2085 | s/batch 9.11\n",
      "INFO:root:| epoch  11 | step 3616 | batch  96/352 | lr 0.00008 | loss 0.2262 | s/batch 9.35\n",
      "INFO:root:| epoch  11 | step 3619 | batch  99/352 | lr 0.00008 | loss 0.2599 | s/batch 9.91\n",
      "INFO:root:| epoch  11 | step 3622 | batch 102/352 | lr 0.00008 | loss 0.2342 | s/batch 7.59\n",
      "INFO:root:| epoch  11 | step 3625 | batch 105/352 | lr 0.00008 | loss 0.2098 | s/batch 5.35\n",
      "INFO:root:| epoch  11 | step 3628 | batch 108/352 | lr 0.00008 | loss 0.2345 | s/batch 4.31\n",
      "INFO:root:| epoch  11 | step 3631 | batch 111/352 | lr 0.00008 | loss 0.2184 | s/batch 11.53\n",
      "INFO:root:| epoch  11 | step 3634 | batch 114/352 | lr 0.00008 | loss 0.2461 | s/batch 5.79\n",
      "INFO:root:| epoch  11 | step 3637 | batch 117/352 | lr 0.00008 | loss 0.2149 | s/batch 4.76\n",
      "INFO:root:| epoch  11 | step 3640 | batch 120/352 | lr 0.00008 | loss 0.2372 | s/batch 4.37\n",
      "INFO:root:| epoch  11 | step 3643 | batch 123/352 | lr 0.00008 | loss 0.2412 | s/batch 10.59\n",
      "INFO:root:| epoch  11 | step 3646 | batch 126/352 | lr 0.00008 | loss 0.2712 | s/batch 7.10\n",
      "INFO:root:| epoch  11 | step 3649 | batch 129/352 | lr 0.00008 | loss 0.1823 | s/batch 5.85\n",
      "INFO:root:| epoch  11 | step 3652 | batch 132/352 | lr 0.00008 | loss 0.2766 | s/batch 10.12\n",
      "INFO:root:| epoch  11 | step 3655 | batch 135/352 | lr 0.00008 | loss 0.2041 | s/batch 7.18\n",
      "INFO:root:| epoch  11 | step 3658 | batch 138/352 | lr 0.00008 | loss 0.2546 | s/batch 13.26\n",
      "INFO:root:| epoch  11 | step 3661 | batch 141/352 | lr 0.00008 | loss 0.2317 | s/batch 9.71\n",
      "INFO:root:| epoch  11 | step 3664 | batch 144/352 | lr 0.00008 | loss 0.2421 | s/batch 3.96\n",
      "INFO:root:| epoch  11 | step 3667 | batch 147/352 | lr 0.00008 | loss 0.2434 | s/batch 10.98\n",
      "INFO:root:| epoch  11 | step 3670 | batch 150/352 | lr 0.00008 | loss 0.2641 | s/batch 9.13\n",
      "INFO:root:| epoch  11 | step 3673 | batch 153/352 | lr 0.00008 | loss 0.1927 | s/batch 11.08\n",
      "INFO:root:| epoch  11 | step 3676 | batch 156/352 | lr 0.00008 | loss 0.2522 | s/batch 9.71\n",
      "INFO:root:| epoch  11 | step 3679 | batch 159/352 | lr 0.00008 | loss 0.2184 | s/batch 10.96\n",
      "INFO:root:| epoch  11 | step 3682 | batch 162/352 | lr 0.00008 | loss 0.2342 | s/batch 6.45\n",
      "INFO:root:| epoch  11 | step 3685 | batch 165/352 | lr 0.00008 | loss 0.2608 | s/batch 14.70\n",
      "INFO:root:| epoch  11 | step 3688 | batch 168/352 | lr 0.00008 | loss 0.2330 | s/batch 15.54\n",
      "INFO:root:| epoch  11 | step 3691 | batch 171/352 | lr 0.00008 | loss 0.2513 | s/batch 8.12\n",
      "INFO:root:| epoch  11 | step 3694 | batch 174/352 | lr 0.00008 | loss 0.2476 | s/batch 5.75\n",
      "INFO:root:| epoch  11 | step 3697 | batch 177/352 | lr 0.00008 | loss 0.2243 | s/batch 6.28\n",
      "INFO:root:| epoch  11 | step 3700 | batch 180/352 | lr 0.00008 | loss 0.2430 | s/batch 9.48\n",
      "INFO:root:| epoch  11 | step 3703 | batch 183/352 | lr 0.00008 | loss 0.2312 | s/batch 4.15\n",
      "INFO:root:| epoch  11 | step 3706 | batch 186/352 | lr 0.00008 | loss 0.2188 | s/batch 3.28\n",
      "INFO:root:| epoch  11 | step 3709 | batch 189/352 | lr 0.00008 | loss 0.2289 | s/batch 4.48\n",
      "INFO:root:| epoch  11 | step 3712 | batch 192/352 | lr 0.00008 | loss 0.2058 | s/batch 10.61\n",
      "INFO:root:| epoch  11 | step 3715 | batch 195/352 | lr 0.00008 | loss 0.2235 | s/batch 3.97\n",
      "INFO:root:| epoch  11 | step 3718 | batch 198/352 | lr 0.00008 | loss 0.2447 | s/batch 2.81\n",
      "INFO:root:| epoch  11 | step 3721 | batch 201/352 | lr 0.00008 | loss 0.2370 | s/batch 4.46\n",
      "INFO:root:| epoch  11 | step 3724 | batch 204/352 | lr 0.00008 | loss 0.2491 | s/batch 3.86\n",
      "INFO:root:| epoch  11 | step 3727 | batch 207/352 | lr 0.00008 | loss 0.2329 | s/batch 8.57\n",
      "INFO:root:| epoch  11 | step 3730 | batch 210/352 | lr 0.00008 | loss 0.2700 | s/batch 10.10\n",
      "INFO:root:| epoch  11 | step 3733 | batch 213/352 | lr 0.00008 | loss 0.2669 | s/batch 11.16\n",
      "INFO:root:| epoch  11 | step 3736 | batch 216/352 | lr 0.00008 | loss 0.2143 | s/batch 6.42\n",
      "INFO:root:| epoch  11 | step 3739 | batch 219/352 | lr 0.00008 | loss 0.2449 | s/batch 7.73\n",
      "INFO:root:| epoch  11 | step 3742 | batch 222/352 | lr 0.00008 | loss 0.1805 | s/batch 4.83\n",
      "INFO:root:| epoch  11 | step 3745 | batch 225/352 | lr 0.00008 | loss 0.2128 | s/batch 5.76\n",
      "INFO:root:| epoch  11 | step 3748 | batch 228/352 | lr 0.00008 | loss 0.2109 | s/batch 5.15\n",
      "INFO:root:| epoch  11 | step 3751 | batch 231/352 | lr 0.00008 | loss 0.2766 | s/batch 9.99\n",
      "INFO:root:| epoch  11 | step 3754 | batch 234/352 | lr 0.00008 | loss 0.2596 | s/batch 11.11\n",
      "INFO:root:| epoch  11 | step 3757 | batch 237/352 | lr 0.00008 | loss 0.2311 | s/batch 4.19\n",
      "INFO:root:| epoch  11 | step 3760 | batch 240/352 | lr 0.00008 | loss 0.2521 | s/batch 8.62\n",
      "INFO:root:| epoch  11 | step 3763 | batch 243/352 | lr 0.00008 | loss 0.2403 | s/batch 7.91\n",
      "INFO:root:| epoch  11 | step 3766 | batch 246/352 | lr 0.00008 | loss 0.2060 | s/batch 4.56\n",
      "INFO:root:| epoch  11 | step 3769 | batch 249/352 | lr 0.00008 | loss 0.2205 | s/batch 10.79\n",
      "INFO:root:| epoch  11 | step 3772 | batch 252/352 | lr 0.00008 | loss 0.2415 | s/batch 9.73\n",
      "INFO:root:| epoch  11 | step 3775 | batch 255/352 | lr 0.00008 | loss 0.2345 | s/batch 9.69\n",
      "INFO:root:| epoch  11 | step 3778 | batch 258/352 | lr 0.00008 | loss 0.2016 | s/batch 5.16\n",
      "INFO:root:| epoch  11 | step 3781 | batch 261/352 | lr 0.00008 | loss 0.2374 | s/batch 7.09\n",
      "INFO:root:| epoch  11 | step 3784 | batch 264/352 | lr 0.00008 | loss 0.2026 | s/batch 10.64\n",
      "INFO:root:| epoch  11 | step 3787 | batch 267/352 | lr 0.00008 | loss 0.2333 | s/batch 9.28\n",
      "INFO:root:| epoch  11 | step 3790 | batch 270/352 | lr 0.00008 | loss 0.2671 | s/batch 11.94\n",
      "INFO:root:| epoch  11 | step 3793 | batch 273/352 | lr 0.00008 | loss 0.2243 | s/batch 10.76\n",
      "INFO:root:| epoch  11 | step 3796 | batch 276/352 | lr 0.00008 | loss 0.2008 | s/batch 8.63\n",
      "INFO:root:| epoch  11 | step 3799 | batch 279/352 | lr 0.00008 | loss 0.2085 | s/batch 9.74\n",
      "INFO:root:| epoch  11 | step 3802 | batch 282/352 | lr 0.00008 | loss 0.2297 | s/batch 7.70\n",
      "INFO:root:| epoch  11 | step 3805 | batch 285/352 | lr 0.00008 | loss 0.3126 | s/batch 15.22\n",
      "INFO:root:| epoch  11 | step 3808 | batch 288/352 | lr 0.00008 | loss 0.2096 | s/batch 9.47\n",
      "INFO:root:| epoch  11 | step 3811 | batch 291/352 | lr 0.00008 | loss 0.2497 | s/batch 7.22\n",
      "INFO:root:| epoch  11 | step 3814 | batch 294/352 | lr 0.00008 | loss 0.2043 | s/batch 9.78\n",
      "INFO:root:| epoch  11 | step 3817 | batch 297/352 | lr 0.00008 | loss 0.1751 | s/batch 7.08\n",
      "INFO:root:| epoch  11 | step 3820 | batch 300/352 | lr 0.00008 | loss 0.2181 | s/batch 5.77\n",
      "INFO:root:| epoch  11 | step 3823 | batch 303/352 | lr 0.00008 | loss 0.2144 | s/batch 6.99\n",
      "INFO:root:| epoch  11 | step 3826 | batch 306/352 | lr 0.00008 | loss 0.2279 | s/batch 4.44\n",
      "INFO:root:| epoch  11 | step 3829 | batch 309/352 | lr 0.00008 | loss 0.2686 | s/batch 12.16\n",
      "INFO:root:| epoch  11 | step 3832 | batch 312/352 | lr 0.00008 | loss 0.2059 | s/batch 10.31\n",
      "INFO:root:| epoch  11 | step 3835 | batch 315/352 | lr 0.00008 | loss 0.2032 | s/batch 4.19\n",
      "INFO:root:| epoch  11 | step 3838 | batch 318/352 | lr 0.00008 | loss 0.2378 | s/batch 5.71\n",
      "INFO:root:| epoch  11 | step 3841 | batch 321/352 | lr 0.00008 | loss 0.2590 | s/batch 9.21\n",
      "INFO:root:| epoch  11 | step 3844 | batch 324/352 | lr 0.00008 | loss 0.2145 | s/batch 5.15\n",
      "INFO:root:| epoch  11 | step 3847 | batch 327/352 | lr 0.00008 | loss 0.2371 | s/batch 9.92\n",
      "INFO:root:| epoch  11 | step 3850 | batch 330/352 | lr 0.00008 | loss 0.2654 | s/batch 9.98\n",
      "INFO:root:| epoch  11 | step 3853 | batch 333/352 | lr 0.00008 | loss 0.2673 | s/batch 9.50\n",
      "INFO:root:| epoch  11 | step 3856 | batch 336/352 | lr 0.00008 | loss 0.2427 | s/batch 3.20\n",
      "INFO:root:| epoch  11 | step 3859 | batch 339/352 | lr 0.00008 | loss 0.2356 | s/batch 10.45\n",
      "INFO:root:| epoch  11 | step 3862 | batch 342/352 | lr 0.00008 | loss 0.2159 | s/batch 8.54\n",
      "INFO:root:| epoch  11 | step 3865 | batch 345/352 | lr 0.00008 | loss 0.2293 | s/batch 9.03\n",
      "INFO:root:| epoch  11 | step 3868 | batch 348/352 | lr 0.00008 | loss 0.1895 | s/batch 6.17\n",
      "INFO:root:| epoch  11 | step 3871 | batch 351/352 | lr 0.00008 | loss 0.2595 | s/batch 2.65\n",
      "INFO:root:| epoch  11 | score (91.39, 90.29, 90.82) | f1 90.82 | loss 0.2319 | time 2786.60\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9264    0.9288    0.9276     35027\n",
      "          股票     0.9342    0.9418    0.9380     33251\n",
      "          体育     0.9821    0.9826    0.9823     28283\n",
      "          娱乐     0.9334    0.9465    0.9399     19920\n",
      "          时政     0.8633    0.8937    0.8782     13515\n",
      "          社会     0.8596    0.8475    0.8535     11009\n",
      "          教育     0.9300    0.9234    0.9267      8987\n",
      "          财经     0.8695    0.8165    0.8422      7957\n",
      "          家居     0.8929    0.8893    0.8911      7063\n",
      "          游戏     0.9152    0.8842    0.8994      5285\n",
      "          房产     0.9723    0.9589    0.9655      4428\n",
      "          时尚     0.8876    0.8659    0.8766      2818\n",
      "          彩票     0.9366    0.9018    0.9189      1639\n",
      "          星座     0.8921    0.8594    0.8755       818\n",
      "\n",
      "    accuracy                         0.9251    180000\n",
      "   macro avg     0.9139    0.9029    0.9082    180000\n",
      "weighted avg     0.9250    0.9251    0.9250    180000\n",
      "\n",
      "INFO:root:| epoch  11 | dev | score (93.76, 92.94, 93.33) | f1 93.33 | time 258.50\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9345    0.9499    0.9421      3891\n",
      "          股票     0.9570    0.9407    0.9488      3694\n",
      "          体育     0.9850    0.9850    0.9850      3142\n",
      "          娱乐     0.9580    0.9494    0.9537      2213\n",
      "          时政     0.8778    0.9001    0.8888      1501\n",
      "          社会     0.8570    0.8823    0.8695      1223\n",
      "          教育     0.9470    0.9309    0.9389       998\n",
      "          财经     0.8821    0.8801    0.8811       884\n",
      "          家居     0.9353    0.9222    0.9287       784\n",
      "          游戏     0.9430    0.9207    0.9317       593\n",
      "          房产     1.0000    0.9919    0.9959       492\n",
      "          时尚     0.9180    0.9297    0.9238       313\n",
      "          彩票     0.9441    0.9286    0.9363       182\n",
      "          星座     0.9878    0.9000    0.9419        90\n",
      "\n",
      "    accuracy                         0.9401     20000\n",
      "   macro avg     0.9376    0.9294    0.9333     20000\n",
      "weighted avg     0.9405    0.9401    0.9402     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 93.08, current dev = 93.33\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch  12 | step 3875 | batch   3/352 | lr 0.00008 | loss 0.2327 | s/batch 4.82\n",
      "INFO:root:| epoch  12 | step 3878 | batch   6/352 | lr 0.00008 | loss 0.2495 | s/batch 10.45\n",
      "INFO:root:| epoch  12 | step 3881 | batch   9/352 | lr 0.00008 | loss 0.2016 | s/batch 7.02\n",
      "INFO:root:| epoch  12 | step 3884 | batch  12/352 | lr 0.00008 | loss 0.1727 | s/batch 7.49\n",
      "INFO:root:| epoch  12 | step 3887 | batch  15/352 | lr 0.00008 | loss 0.2278 | s/batch 9.97\n",
      "INFO:root:| epoch  12 | step 3890 | batch  18/352 | lr 0.00008 | loss 0.1988 | s/batch 3.46\n",
      "INFO:root:| epoch  12 | step 3893 | batch  21/352 | lr 0.00008 | loss 0.2264 | s/batch 5.65\n",
      "INFO:root:| epoch  12 | step 3896 | batch  24/352 | lr 0.00008 | loss 0.2170 | s/batch 3.24\n",
      "INFO:root:| epoch  12 | step 3899 | batch  27/352 | lr 0.00008 | loss 0.2040 | s/batch 12.45\n",
      "INFO:root:| epoch  12 | step 3902 | batch  30/352 | lr 0.00008 | loss 0.2491 | s/batch 9.37\n",
      "INFO:root:| epoch  12 | step 3905 | batch  33/352 | lr 0.00008 | loss 0.2030 | s/batch 8.44\n",
      "INFO:root:| epoch  12 | step 3908 | batch  36/352 | lr 0.00008 | loss 0.2476 | s/batch 7.31\n",
      "INFO:root:| epoch  12 | step 3911 | batch  39/352 | lr 0.00008 | loss 0.2077 | s/batch 9.32\n",
      "INFO:root:| epoch  12 | step 3914 | batch  42/352 | lr 0.00008 | loss 0.3017 | s/batch 16.26\n",
      "INFO:root:| epoch  12 | step 3917 | batch  45/352 | lr 0.00008 | loss 0.2243 | s/batch 7.32\n",
      "INFO:root:| epoch  12 | step 3920 | batch  48/352 | lr 0.00008 | loss 0.2429 | s/batch 4.03\n",
      "INFO:root:| epoch  12 | step 3923 | batch  51/352 | lr 0.00008 | loss 0.2501 | s/batch 8.53\n",
      "INFO:root:| epoch  12 | step 3926 | batch  54/352 | lr 0.00008 | loss 0.2449 | s/batch 10.57\n",
      "INFO:root:| epoch  12 | step 3929 | batch  57/352 | lr 0.00008 | loss 0.2138 | s/batch 6.28\n",
      "INFO:root:| epoch  12 | step 3932 | batch  60/352 | lr 0.00008 | loss 0.2118 | s/batch 5.55\n",
      "INFO:root:| epoch  12 | step 3935 | batch  63/352 | lr 0.00008 | loss 0.2120 | s/batch 3.65\n",
      "INFO:root:| epoch  12 | step 3938 | batch  66/352 | lr 0.00008 | loss 0.2295 | s/batch 3.91\n",
      "INFO:root:| epoch  12 | step 3941 | batch  69/352 | lr 0.00008 | loss 0.2710 | s/batch 10.38\n",
      "INFO:root:| epoch  12 | step 3944 | batch  72/352 | lr 0.00008 | loss 0.2372 | s/batch 4.33\n",
      "INFO:root:| epoch  12 | step 3947 | batch  75/352 | lr 0.00008 | loss 0.2241 | s/batch 7.27\n",
      "INFO:root:| epoch  12 | step 3950 | batch  78/352 | lr 0.00008 | loss 0.2082 | s/batch 6.41\n",
      "INFO:root:| epoch  12 | step 3953 | batch  81/352 | lr 0.00008 | loss 0.2479 | s/batch 9.44\n",
      "INFO:root:| epoch  12 | step 3956 | batch  84/352 | lr 0.00008 | loss 0.2375 | s/batch 11.00\n",
      "INFO:root:| epoch  12 | step 3959 | batch  87/352 | lr 0.00008 | loss 0.2368 | s/batch 4.11\n",
      "INFO:root:| epoch  12 | step 3962 | batch  90/352 | lr 0.00008 | loss 0.2333 | s/batch 5.01\n",
      "INFO:root:| epoch  12 | step 3965 | batch  93/352 | lr 0.00008 | loss 0.2582 | s/batch 12.95\n",
      "INFO:root:| epoch  12 | step 3968 | batch  96/352 | lr 0.00008 | loss 0.2373 | s/batch 15.73\n",
      "INFO:root:| epoch  12 | step 3971 | batch  99/352 | lr 0.00008 | loss 0.2012 | s/batch 5.00\n",
      "INFO:root:| epoch  12 | step 3974 | batch 102/352 | lr 0.00008 | loss 0.2190 | s/batch 6.62\n",
      "INFO:root:| epoch  12 | step 3977 | batch 105/352 | lr 0.00008 | loss 0.2206 | s/batch 8.01\n",
      "INFO:root:| epoch  12 | step 3980 | batch 108/352 | lr 0.00008 | loss 0.2337 | s/batch 2.61\n",
      "INFO:root:| epoch  12 | step 3983 | batch 111/352 | lr 0.00008 | loss 0.1924 | s/batch 14.64\n",
      "INFO:root:| epoch  12 | step 3986 | batch 114/352 | lr 0.00008 | loss 0.2259 | s/batch 8.85\n",
      "INFO:root:| epoch  12 | step 3989 | batch 117/352 | lr 0.00008 | loss 0.2495 | s/batch 3.17\n",
      "INFO:root:| epoch  12 | step 3992 | batch 120/352 | lr 0.00008 | loss 0.2758 | s/batch 8.54\n",
      "INFO:root:| epoch  12 | step 3995 | batch 123/352 | lr 0.00008 | loss 0.2873 | s/batch 13.69\n",
      "INFO:root:| epoch  12 | step 3998 | batch 126/352 | lr 0.00008 | loss 0.2267 | s/batch 3.10\n",
      "INFO:root:| epoch  12 | step 4001 | batch 129/352 | lr 0.00006 | loss 0.2220 | s/batch 4.58\n",
      "INFO:root:| epoch  12 | step 4004 | batch 132/352 | lr 0.00006 | loss 0.2045 | s/batch 4.25\n",
      "INFO:root:| epoch  12 | step 4007 | batch 135/352 | lr 0.00006 | loss 0.2347 | s/batch 5.07\n",
      "INFO:root:| epoch  12 | step 4010 | batch 138/352 | lr 0.00006 | loss 0.2089 | s/batch 5.06\n",
      "INFO:root:| epoch  12 | step 4013 | batch 141/352 | lr 0.00006 | loss 0.2080 | s/batch 6.25\n",
      "INFO:root:| epoch  12 | step 4016 | batch 144/352 | lr 0.00006 | loss 0.1857 | s/batch 6.86\n",
      "INFO:root:| epoch  12 | step 4019 | batch 147/352 | lr 0.00006 | loss 0.2442 | s/batch 6.58\n",
      "INFO:root:| epoch  12 | step 4022 | batch 150/352 | lr 0.00006 | loss 0.2622 | s/batch 8.65\n",
      "INFO:root:| epoch  12 | step 4025 | batch 153/352 | lr 0.00006 | loss 0.2197 | s/batch 5.13\n",
      "INFO:root:| epoch  12 | step 4028 | batch 156/352 | lr 0.00006 | loss 0.2458 | s/batch 14.84\n",
      "INFO:root:| epoch  12 | step 4031 | batch 159/352 | lr 0.00006 | loss 0.1864 | s/batch 4.99\n",
      "INFO:root:| epoch  12 | step 4034 | batch 162/352 | lr 0.00006 | loss 0.2446 | s/batch 12.43\n",
      "INFO:root:| epoch  12 | step 4037 | batch 165/352 | lr 0.00006 | loss 0.2085 | s/batch 3.76\n",
      "INFO:root:| epoch  12 | step 4040 | batch 168/352 | lr 0.00006 | loss 0.2358 | s/batch 3.15\n",
      "INFO:root:| epoch  12 | step 4043 | batch 171/352 | lr 0.00006 | loss 0.2359 | s/batch 11.84\n",
      "INFO:root:| epoch  12 | step 4046 | batch 174/352 | lr 0.00006 | loss 0.2317 | s/batch 16.05\n",
      "INFO:root:| epoch  12 | step 4049 | batch 177/352 | lr 0.00006 | loss 0.2090 | s/batch 8.62\n",
      "INFO:root:| epoch  12 | step 4052 | batch 180/352 | lr 0.00006 | loss 0.2346 | s/batch 12.90\n",
      "INFO:root:| epoch  12 | step 4055 | batch 183/352 | lr 0.00006 | loss 0.2045 | s/batch 8.69\n",
      "INFO:root:| epoch  12 | step 4058 | batch 186/352 | lr 0.00006 | loss 0.2456 | s/batch 14.43\n",
      "INFO:root:| epoch  12 | step 4061 | batch 189/352 | lr 0.00006 | loss 0.2472 | s/batch 3.97\n",
      "INFO:root:| epoch  12 | step 4064 | batch 192/352 | lr 0.00006 | loss 0.2324 | s/batch 5.54\n",
      "INFO:root:| epoch  12 | step 4067 | batch 195/352 | lr 0.00006 | loss 0.2446 | s/batch 9.43\n",
      "INFO:root:| epoch  12 | step 4070 | batch 198/352 | lr 0.00006 | loss 0.2518 | s/batch 6.80\n",
      "INFO:root:| epoch  12 | step 4073 | batch 201/352 | lr 0.00006 | loss 0.2065 | s/batch 8.13\n",
      "INFO:root:| epoch  12 | step 4076 | batch 204/352 | lr 0.00006 | loss 0.2688 | s/batch 9.51\n",
      "INFO:root:| epoch  12 | step 4079 | batch 207/352 | lr 0.00006 | loss 0.2739 | s/batch 5.11\n",
      "INFO:root:| epoch  12 | step 4082 | batch 210/352 | lr 0.00006 | loss 0.2507 | s/batch 10.29\n",
      "INFO:root:| epoch  12 | step 4085 | batch 213/352 | lr 0.00006 | loss 0.2295 | s/batch 2.31\n",
      "INFO:root:| epoch  12 | step 4088 | batch 216/352 | lr 0.00006 | loss 0.2312 | s/batch 6.31\n",
      "INFO:root:| epoch  12 | step 4091 | batch 219/352 | lr 0.00006 | loss 0.1864 | s/batch 8.55\n",
      "INFO:root:| epoch  12 | step 4094 | batch 222/352 | lr 0.00006 | loss 0.2485 | s/batch 13.99\n",
      "INFO:root:| epoch  12 | step 4097 | batch 225/352 | lr 0.00006 | loss 0.2093 | s/batch 6.41\n",
      "INFO:root:| epoch  12 | step 4100 | batch 228/352 | lr 0.00006 | loss 0.2063 | s/batch 6.30\n",
      "INFO:root:| epoch  12 | step 4103 | batch 231/352 | lr 0.00006 | loss 0.2079 | s/batch 5.33\n",
      "INFO:root:| epoch  12 | step 4106 | batch 234/352 | lr 0.00006 | loss 0.1748 | s/batch 5.60\n",
      "INFO:root:| epoch  12 | step 4109 | batch 237/352 | lr 0.00006 | loss 0.2198 | s/batch 3.83\n",
      "INFO:root:| epoch  12 | step 4112 | batch 240/352 | lr 0.00006 | loss 0.2417 | s/batch 13.49\n",
      "INFO:root:| epoch  12 | step 4115 | batch 243/352 | lr 0.00006 | loss 0.2132 | s/batch 10.31\n",
      "INFO:root:| epoch  12 | step 4118 | batch 246/352 | lr 0.00006 | loss 0.2140 | s/batch 6.37\n",
      "INFO:root:| epoch  12 | step 4121 | batch 249/352 | lr 0.00006 | loss 0.2235 | s/batch 6.45\n",
      "INFO:root:| epoch  12 | step 4124 | batch 252/352 | lr 0.00006 | loss 0.2310 | s/batch 13.09\n",
      "INFO:root:| epoch  12 | step 4127 | batch 255/352 | lr 0.00006 | loss 0.2004 | s/batch 3.38\n",
      "INFO:root:| epoch  12 | step 4130 | batch 258/352 | lr 0.00006 | loss 0.2387 | s/batch 8.10\n",
      "INFO:root:| epoch  12 | step 4133 | batch 261/352 | lr 0.00006 | loss 0.2322 | s/batch 12.56\n",
      "INFO:root:| epoch  12 | step 4136 | batch 264/352 | lr 0.00006 | loss 0.2139 | s/batch 5.77\n",
      "INFO:root:| epoch  12 | step 4139 | batch 267/352 | lr 0.00006 | loss 0.1893 | s/batch 5.10\n",
      "INFO:root:| epoch  12 | step 4142 | batch 270/352 | lr 0.00006 | loss 0.1799 | s/batch 5.59\n",
      "INFO:root:| epoch  12 | step 4145 | batch 273/352 | lr 0.00006 | loss 0.2050 | s/batch 4.23\n",
      "INFO:root:| epoch  12 | step 4148 | batch 276/352 | lr 0.00006 | loss 0.2358 | s/batch 9.44\n",
      "INFO:root:| epoch  12 | step 4151 | batch 279/352 | lr 0.00006 | loss 0.2418 | s/batch 3.51\n",
      "INFO:root:| epoch  12 | step 4154 | batch 282/352 | lr 0.00006 | loss 0.2233 | s/batch 6.33\n",
      "INFO:root:| epoch  12 | step 4157 | batch 285/352 | lr 0.00006 | loss 0.1820 | s/batch 11.50\n",
      "INFO:root:| epoch  12 | step 4160 | batch 288/352 | lr 0.00006 | loss 0.2111 | s/batch 3.85\n",
      "INFO:root:| epoch  12 | step 4163 | batch 291/352 | lr 0.00006 | loss 0.1943 | s/batch 4.66\n",
      "INFO:root:| epoch  12 | step 4166 | batch 294/352 | lr 0.00006 | loss 0.2398 | s/batch 3.09\n",
      "INFO:root:| epoch  12 | step 4169 | batch 297/352 | lr 0.00006 | loss 0.2075 | s/batch 10.87\n",
      "INFO:root:| epoch  12 | step 4172 | batch 300/352 | lr 0.00006 | loss 0.1975 | s/batch 7.67\n",
      "INFO:root:| epoch  12 | step 4175 | batch 303/352 | lr 0.00006 | loss 0.2366 | s/batch 14.38\n",
      "INFO:root:| epoch  12 | step 4178 | batch 306/352 | lr 0.00006 | loss 0.2802 | s/batch 10.23\n",
      "INFO:root:| epoch  12 | step 4181 | batch 309/352 | lr 0.00006 | loss 0.2714 | s/batch 14.95\n",
      "INFO:root:| epoch  12 | step 4184 | batch 312/352 | lr 0.00006 | loss 0.1995 | s/batch 5.07\n",
      "INFO:root:| epoch  12 | step 4187 | batch 315/352 | lr 0.00006 | loss 0.2394 | s/batch 9.75\n",
      "INFO:root:| epoch  12 | step 4190 | batch 318/352 | lr 0.00006 | loss 0.2088 | s/batch 8.46\n",
      "INFO:root:| epoch  12 | step 4193 | batch 321/352 | lr 0.00006 | loss 0.2518 | s/batch 8.71\n",
      "INFO:root:| epoch  12 | step 4196 | batch 324/352 | lr 0.00006 | loss 0.2083 | s/batch 8.48\n",
      "INFO:root:| epoch  12 | step 4199 | batch 327/352 | lr 0.00006 | loss 0.2134 | s/batch 11.42\n",
      "INFO:root:| epoch  12 | step 4202 | batch 330/352 | lr 0.00006 | loss 0.2457 | s/batch 3.29\n",
      "INFO:root:| epoch  12 | step 4205 | batch 333/352 | lr 0.00006 | loss 0.2002 | s/batch 4.05\n",
      "INFO:root:| epoch  12 | step 4208 | batch 336/352 | lr 0.00006 | loss 0.2306 | s/batch 5.69\n",
      "INFO:root:| epoch  12 | step 4211 | batch 339/352 | lr 0.00006 | loss 0.2238 | s/batch 10.62\n",
      "INFO:root:| epoch  12 | step 4214 | batch 342/352 | lr 0.00006 | loss 0.2138 | s/batch 8.41\n",
      "INFO:root:| epoch  12 | step 4217 | batch 345/352 | lr 0.00006 | loss 0.2182 | s/batch 10.07\n",
      "INFO:root:| epoch  12 | step 4220 | batch 348/352 | lr 0.00006 | loss 0.2302 | s/batch 3.43\n",
      "INFO:root:| epoch  12 | step 4223 | batch 351/352 | lr 0.00006 | loss 0.3084 | s/batch 15.55\n",
      "INFO:root:| epoch  12 | score (91.63, 90.53, 91.06) | f1 91.06 | loss 0.2267 | time 2752.95\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9284    0.9296    0.9290     35027\n",
      "          股票     0.9358    0.9418    0.9388     33251\n",
      "          体育     0.9816    0.9828    0.9822     28283\n",
      "          娱乐     0.9374    0.9489    0.9431     19920\n",
      "          时政     0.8663    0.8942    0.8800     13515\n",
      "          社会     0.8630    0.8529    0.8579     11009\n",
      "          教育     0.9319    0.9268    0.9293      8987\n",
      "          财经     0.8714    0.8247    0.8474      7957\n",
      "          家居     0.9023    0.8996    0.9010      7063\n",
      "          游戏     0.9109    0.8895    0.9001      5285\n",
      "          房产     0.9695    0.9605    0.9649      4428\n",
      "          时尚     0.8912    0.8662    0.8785      2818\n",
      "          彩票     0.9310    0.8975    0.9139      1639\n",
      "          星座     0.9071    0.8594    0.8826       818\n",
      "\n",
      "    accuracy                         0.9270    180000\n",
      "   macro avg     0.9163    0.9053    0.9106    180000\n",
      "weighted avg     0.9269    0.9270    0.9269    180000\n",
      "\n",
      "INFO:root:| epoch  12 | dev | score (93.3, 93.22, 93.25) | f1 93.25 | time 258.64\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9395    0.9453    0.9424      3891\n",
      "          股票     0.9551    0.9448    0.9499      3694\n",
      "          体育     0.9850    0.9847    0.9849      3142\n",
      "          娱乐     0.9607    0.9507    0.9557      2213\n",
      "          时政     0.8716    0.9094    0.8901      1501\n",
      "          社会     0.8676    0.8839    0.8757      1223\n",
      "          教育     0.9408    0.9399    0.9404       998\n",
      "          财经     0.8888    0.8767    0.8827       884\n",
      "          家居     0.9426    0.9222    0.9323       784\n",
      "          游戏     0.9444    0.9174    0.9307       593\n",
      "          房产     1.0000    0.9878    0.9939       492\n",
      "          时尚     0.9236    0.9265    0.9250       313\n",
      "          彩票     0.9194    0.9396    0.9293       182\n",
      "          星座     0.9222    0.9222    0.9222        90\n",
      "\n",
      "    accuracy                         0.9411     20000\n",
      "   macro avg     0.9330    0.9322    0.9325     20000\n",
      "weighted avg     0.9415    0.9411    0.9412     20000\n",
      "\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch  13 | step 4227 | batch   3/352 | lr 0.00006 | loss 0.2371 | s/batch 10.92\n",
      "INFO:root:| epoch  13 | step 4230 | batch   6/352 | lr 0.00006 | loss 0.2436 | s/batch 4.44\n",
      "INFO:root:| epoch  13 | step 4233 | batch   9/352 | lr 0.00006 | loss 0.2179 | s/batch 5.52\n",
      "INFO:root:| epoch  13 | step 4236 | batch  12/352 | lr 0.00006 | loss 0.2009 | s/batch 2.84\n",
      "INFO:root:| epoch  13 | step 4239 | batch  15/352 | lr 0.00006 | loss 0.2206 | s/batch 6.19\n",
      "INFO:root:| epoch  13 | step 4242 | batch  18/352 | lr 0.00006 | loss 0.2010 | s/batch 8.63\n",
      "INFO:root:| epoch  13 | step 4245 | batch  21/352 | lr 0.00006 | loss 0.2333 | s/batch 4.38\n",
      "INFO:root:| epoch  13 | step 4248 | batch  24/352 | lr 0.00006 | loss 0.2328 | s/batch 5.92\n",
      "INFO:root:| epoch  13 | step 4251 | batch  27/352 | lr 0.00006 | loss 0.2218 | s/batch 8.93\n",
      "INFO:root:| epoch  13 | step 4254 | batch  30/352 | lr 0.00006 | loss 0.2209 | s/batch 6.26\n",
      "INFO:root:| epoch  13 | step 4257 | batch  33/352 | lr 0.00006 | loss 0.2125 | s/batch 11.20\n",
      "INFO:root:| epoch  13 | step 4260 | batch  36/352 | lr 0.00006 | loss 0.2541 | s/batch 4.03\n",
      "INFO:root:| epoch  13 | step 4263 | batch  39/352 | lr 0.00006 | loss 0.2280 | s/batch 3.72\n",
      "INFO:root:| epoch  13 | step 4266 | batch  42/352 | lr 0.00006 | loss 0.1877 | s/batch 6.31\n",
      "INFO:root:| epoch  13 | step 4269 | batch  45/352 | lr 0.00006 | loss 0.2077 | s/batch 7.14\n",
      "INFO:root:| epoch  13 | step 4272 | batch  48/352 | lr 0.00006 | loss 0.2595 | s/batch 12.03\n",
      "INFO:root:| epoch  13 | step 4275 | batch  51/352 | lr 0.00006 | loss 0.1764 | s/batch 4.98\n",
      "INFO:root:| epoch  13 | step 4278 | batch  54/352 | lr 0.00006 | loss 0.2228 | s/batch 14.13\n",
      "INFO:root:| epoch  13 | step 4281 | batch  57/352 | lr 0.00006 | loss 0.2123 | s/batch 10.35\n",
      "INFO:root:| epoch  13 | step 4284 | batch  60/352 | lr 0.00006 | loss 0.1808 | s/batch 4.04\n",
      "INFO:root:| epoch  13 | step 4287 | batch  63/352 | lr 0.00006 | loss 0.2366 | s/batch 6.56\n",
      "INFO:root:| epoch  13 | step 4290 | batch  66/352 | lr 0.00006 | loss 0.2878 | s/batch 16.22\n",
      "INFO:root:| epoch  13 | step 4293 | batch  69/352 | lr 0.00006 | loss 0.2427 | s/batch 2.81\n",
      "INFO:root:| epoch  13 | step 4296 | batch  72/352 | lr 0.00006 | loss 0.1815 | s/batch 8.63\n",
      "INFO:root:| epoch  13 | step 4299 | batch  75/352 | lr 0.00006 | loss 0.2294 | s/batch 9.56\n",
      "INFO:root:| epoch  13 | step 4302 | batch  78/352 | lr 0.00006 | loss 0.1865 | s/batch 7.62\n",
      "INFO:root:| epoch  13 | step 4305 | batch  81/352 | lr 0.00006 | loss 0.2150 | s/batch 11.39\n",
      "INFO:root:| epoch  13 | step 4308 | batch  84/352 | lr 0.00006 | loss 0.2141 | s/batch 4.66\n",
      "INFO:root:| epoch  13 | step 4311 | batch  87/352 | lr 0.00006 | loss 0.2230 | s/batch 1.73\n",
      "INFO:root:| epoch  13 | step 4314 | batch  90/352 | lr 0.00006 | loss 0.2032 | s/batch 2.73\n",
      "INFO:root:| epoch  13 | step 4317 | batch  93/352 | lr 0.00006 | loss 0.2739 | s/batch 10.36\n",
      "INFO:root:| epoch  13 | step 4320 | batch  96/352 | lr 0.00006 | loss 0.2331 | s/batch 5.68\n",
      "INFO:root:| epoch  13 | step 4323 | batch  99/352 | lr 0.00006 | loss 0.2375 | s/batch 8.57\n",
      "INFO:root:| epoch  13 | step 4326 | batch 102/352 | lr 0.00006 | loss 0.1958 | s/batch 4.80\n",
      "INFO:root:| epoch  13 | step 4329 | batch 105/352 | lr 0.00006 | loss 0.2613 | s/batch 3.16\n",
      "INFO:root:| epoch  13 | step 4332 | batch 108/352 | lr 0.00006 | loss 0.2514 | s/batch 15.07\n",
      "INFO:root:| epoch  13 | step 4335 | batch 111/352 | lr 0.00006 | loss 0.2293 | s/batch 5.69\n",
      "INFO:root:| epoch  13 | step 4338 | batch 114/352 | lr 0.00006 | loss 0.1545 | s/batch 6.33\n",
      "INFO:root:| epoch  13 | step 4341 | batch 117/352 | lr 0.00006 | loss 0.2350 | s/batch 14.65\n",
      "INFO:root:| epoch  13 | step 4344 | batch 120/352 | lr 0.00006 | loss 0.2196 | s/batch 13.02\n",
      "INFO:root:| epoch  13 | step 4347 | batch 123/352 | lr 0.00006 | loss 0.1953 | s/batch 3.28\n",
      "INFO:root:| epoch  13 | step 4350 | batch 126/352 | lr 0.00006 | loss 0.2458 | s/batch 14.23\n",
      "INFO:root:| epoch  13 | step 4353 | batch 129/352 | lr 0.00006 | loss 0.2081 | s/batch 5.02\n",
      "INFO:root:| epoch  13 | step 4356 | batch 132/352 | lr 0.00006 | loss 0.2174 | s/batch 4.71\n",
      "INFO:root:| epoch  13 | step 4359 | batch 135/352 | lr 0.00006 | loss 0.2276 | s/batch 5.00\n",
      "INFO:root:| epoch  13 | step 4362 | batch 138/352 | lr 0.00006 | loss 0.2287 | s/batch 8.34\n",
      "INFO:root:| epoch  13 | step 4365 | batch 141/352 | lr 0.00006 | loss 0.2200 | s/batch 8.37\n",
      "INFO:root:| epoch  13 | step 4368 | batch 144/352 | lr 0.00006 | loss 0.2283 | s/batch 9.18\n",
      "INFO:root:| epoch  13 | step 4371 | batch 147/352 | lr 0.00006 | loss 0.2137 | s/batch 5.65\n",
      "INFO:root:| epoch  13 | step 4374 | batch 150/352 | lr 0.00006 | loss 0.2476 | s/batch 13.31\n",
      "INFO:root:| epoch  13 | step 4377 | batch 153/352 | lr 0.00006 | loss 0.2233 | s/batch 3.94\n",
      "INFO:root:| epoch  13 | step 4380 | batch 156/352 | lr 0.00006 | loss 0.1970 | s/batch 3.69\n",
      "INFO:root:| epoch  13 | step 4383 | batch 159/352 | lr 0.00006 | loss 0.2499 | s/batch 9.73\n",
      "INFO:root:| epoch  13 | step 4386 | batch 162/352 | lr 0.00006 | loss 0.2446 | s/batch 10.52\n",
      "INFO:root:| epoch  13 | step 4389 | batch 165/352 | lr 0.00006 | loss 0.2106 | s/batch 6.97\n",
      "INFO:root:| epoch  13 | step 4392 | batch 168/352 | lr 0.00006 | loss 0.2411 | s/batch 10.38\n",
      "INFO:root:| epoch  13 | step 4395 | batch 171/352 | lr 0.00006 | loss 0.2134 | s/batch 7.39\n",
      "INFO:root:| epoch  13 | step 4398 | batch 174/352 | lr 0.00006 | loss 0.2310 | s/batch 5.70\n",
      "INFO:root:| epoch  13 | step 4401 | batch 177/352 | lr 0.00006 | loss 0.2142 | s/batch 7.01\n",
      "INFO:root:| epoch  13 | step 4404 | batch 180/352 | lr 0.00006 | loss 0.1988 | s/batch 5.62\n",
      "INFO:root:| epoch  13 | step 4407 | batch 183/352 | lr 0.00006 | loss 0.2571 | s/batch 14.63\n",
      "INFO:root:| epoch  13 | step 4410 | batch 186/352 | lr 0.00006 | loss 0.2331 | s/batch 16.14\n",
      "INFO:root:| epoch  13 | step 4413 | batch 189/352 | lr 0.00006 | loss 0.2262 | s/batch 4.04\n",
      "INFO:root:| epoch  13 | step 4416 | batch 192/352 | lr 0.00006 | loss 0.2175 | s/batch 10.34\n",
      "INFO:root:| epoch  13 | step 4419 | batch 195/352 | lr 0.00006 | loss 0.2481 | s/batch 3.43\n",
      "INFO:root:| epoch  13 | step 4422 | batch 198/352 | lr 0.00006 | loss 0.2275 | s/batch 9.87\n",
      "INFO:root:| epoch  13 | step 4425 | batch 201/352 | lr 0.00006 | loss 0.2018 | s/batch 7.25\n",
      "INFO:root:| epoch  13 | step 4428 | batch 204/352 | lr 0.00006 | loss 0.2448 | s/batch 2.72\n",
      "INFO:root:| epoch  13 | step 4431 | batch 207/352 | lr 0.00006 | loss 0.2450 | s/batch 16.20\n",
      "INFO:root:| epoch  13 | step 4434 | batch 210/352 | lr 0.00006 | loss 0.2601 | s/batch 19.24\n",
      "INFO:root:| epoch  13 | step 4437 | batch 213/352 | lr 0.00006 | loss 0.2095 | s/batch 2.78\n",
      "INFO:root:| epoch  13 | step 4440 | batch 216/352 | lr 0.00006 | loss 0.1846 | s/batch 8.20\n",
      "INFO:root:| epoch  13 | step 4443 | batch 219/352 | lr 0.00006 | loss 0.1744 | s/batch 9.54\n",
      "INFO:root:| epoch  13 | step 4446 | batch 222/352 | lr 0.00006 | loss 0.2308 | s/batch 11.28\n",
      "INFO:root:| epoch  13 | step 4449 | batch 225/352 | lr 0.00006 | loss 0.2128 | s/batch 5.08\n",
      "INFO:root:| epoch  13 | step 4452 | batch 228/352 | lr 0.00006 | loss 0.2702 | s/batch 8.70\n",
      "INFO:root:| epoch  13 | step 4455 | batch 231/352 | lr 0.00006 | loss 0.2285 | s/batch 11.44\n",
      "INFO:root:| epoch  13 | step 4458 | batch 234/352 | lr 0.00006 | loss 0.2180 | s/batch 3.85\n",
      "INFO:root:| epoch  13 | step 4461 | batch 237/352 | lr 0.00006 | loss 0.2428 | s/batch 4.40\n",
      "INFO:root:| epoch  13 | step 4464 | batch 240/352 | lr 0.00006 | loss 0.2634 | s/batch 9.26\n",
      "INFO:root:| epoch  13 | step 4467 | batch 243/352 | lr 0.00006 | loss 0.2223 | s/batch 4.34\n",
      "INFO:root:| epoch  13 | step 4470 | batch 246/352 | lr 0.00006 | loss 0.2698 | s/batch 8.77\n",
      "INFO:root:| epoch  13 | step 4473 | batch 249/352 | lr 0.00006 | loss 0.2382 | s/batch 11.30\n",
      "INFO:root:| epoch  13 | step 4476 | batch 252/352 | lr 0.00006 | loss 0.1868 | s/batch 8.41\n",
      "INFO:root:| epoch  13 | step 4479 | batch 255/352 | lr 0.00006 | loss 0.2542 | s/batch 5.36\n",
      "INFO:root:| epoch  13 | step 4482 | batch 258/352 | lr 0.00006 | loss 0.2327 | s/batch 5.93\n",
      "INFO:root:| epoch  13 | step 4485 | batch 261/352 | lr 0.00006 | loss 0.1682 | s/batch 6.95\n",
      "INFO:root:| epoch  13 | step 4488 | batch 264/352 | lr 0.00006 | loss 0.2461 | s/batch 4.35\n",
      "INFO:root:| epoch  13 | step 4491 | batch 267/352 | lr 0.00006 | loss 0.2482 | s/batch 8.55\n",
      "INFO:root:| epoch  13 | step 4494 | batch 270/352 | lr 0.00006 | loss 0.2555 | s/batch 12.59\n",
      "INFO:root:| epoch  13 | step 4497 | batch 273/352 | lr 0.00006 | loss 0.2192 | s/batch 10.17\n",
      "INFO:root:| epoch  13 | step 4500 | batch 276/352 | lr 0.00006 | loss 0.1889 | s/batch 10.28\n",
      "INFO:root:| epoch  13 | step 4503 | batch 279/352 | lr 0.00006 | loss 0.2379 | s/batch 8.09\n",
      "INFO:root:| epoch  13 | step 4506 | batch 282/352 | lr 0.00006 | loss 0.2763 | s/batch 15.27\n",
      "INFO:root:| epoch  13 | step 4509 | batch 285/352 | lr 0.00006 | loss 0.2058 | s/batch 6.94\n",
      "INFO:root:| epoch  13 | step 4512 | batch 288/352 | lr 0.00006 | loss 0.2883 | s/batch 7.80\n",
      "INFO:root:| epoch  13 | step 4515 | batch 291/352 | lr 0.00006 | loss 0.2154 | s/batch 3.14\n",
      "INFO:root:| epoch  13 | step 4518 | batch 294/352 | lr 0.00006 | loss 0.1982 | s/batch 4.98\n",
      "INFO:root:| epoch  13 | step 4521 | batch 297/352 | lr 0.00006 | loss 0.2598 | s/batch 2.61\n",
      "INFO:root:| epoch  13 | step 4524 | batch 300/352 | lr 0.00006 | loss 0.1947 | s/batch 5.98\n",
      "INFO:root:| epoch  13 | step 4527 | batch 303/352 | lr 0.00006 | loss 0.2124 | s/batch 3.74\n",
      "INFO:root:| epoch  13 | step 4530 | batch 306/352 | lr 0.00006 | loss 0.2480 | s/batch 15.87\n",
      "INFO:root:| epoch  13 | step 4533 | batch 309/352 | lr 0.00006 | loss 0.2318 | s/batch 5.63\n",
      "INFO:root:| epoch  13 | step 4536 | batch 312/352 | lr 0.00006 | loss 0.2144 | s/batch 11.85\n",
      "INFO:root:| epoch  13 | step 4539 | batch 315/352 | lr 0.00006 | loss 0.2003 | s/batch 10.39\n",
      "INFO:root:| epoch  13 | step 4542 | batch 318/352 | lr 0.00006 | loss 0.2332 | s/batch 2.72\n",
      "INFO:root:| epoch  13 | step 4545 | batch 321/352 | lr 0.00006 | loss 0.2038 | s/batch 5.63\n",
      "INFO:root:| epoch  13 | step 4548 | batch 324/352 | lr 0.00006 | loss 0.2312 | s/batch 11.04\n",
      "INFO:root:| epoch  13 | step 4551 | batch 327/352 | lr 0.00006 | loss 0.2237 | s/batch 3.08\n",
      "INFO:root:| epoch  13 | step 4554 | batch 330/352 | lr 0.00006 | loss 0.2034 | s/batch 5.24\n",
      "INFO:root:| epoch  13 | step 4557 | batch 333/352 | lr 0.00006 | loss 0.2228 | s/batch 12.34\n",
      "INFO:root:| epoch  13 | step 4560 | batch 336/352 | lr 0.00006 | loss 0.1972 | s/batch 7.59\n",
      "INFO:root:| epoch  13 | step 4563 | batch 339/352 | lr 0.00006 | loss 0.2200 | s/batch 10.23\n",
      "INFO:root:| epoch  13 | step 4566 | batch 342/352 | lr 0.00006 | loss 0.1908 | s/batch 6.37\n",
      "INFO:root:| epoch  13 | step 4569 | batch 345/352 | lr 0.00006 | loss 0.2245 | s/batch 9.89\n",
      "INFO:root:| epoch  13 | step 4572 | batch 348/352 | lr 0.00006 | loss 0.2251 | s/batch 6.20\n",
      "INFO:root:| epoch  13 | step 4575 | batch 351/352 | lr 0.00006 | loss 0.2341 | s/batch 9.83\n",
      "INFO:root:| epoch  13 | score (91.59, 90.77, 91.17) | f1 91.17 | loss 0.2242 | time 2755.59\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9297    0.9304    0.9300     35027\n",
      "          股票     0.9368    0.9433    0.9400     33251\n",
      "          体育     0.9812    0.9828    0.9820     28283\n",
      "          娱乐     0.9362    0.9481    0.9421     19920\n",
      "          时政     0.8687    0.8960    0.8821     13515\n",
      "          社会     0.8642    0.8517    0.8579     11009\n",
      "          教育     0.9335    0.9243    0.9289      8987\n",
      "          财经     0.8697    0.8269    0.8478      7957\n",
      "          家居     0.9008    0.8957    0.8982      7063\n",
      "          游戏     0.9154    0.8901    0.9025      5285\n",
      "          房产     0.9712    0.9605    0.9658      4428\n",
      "          时尚     0.8837    0.8683    0.8760      2818\n",
      "          彩票     0.9354    0.9097    0.9224      1639\n",
      "          星座     0.8966    0.8802    0.8883       818\n",
      "\n",
      "    accuracy                         0.9275    180000\n",
      "   macro avg     0.9159    0.9077    0.9117    180000\n",
      "weighted avg     0.9274    0.9275    0.9274    180000\n",
      "\n",
      "INFO:root:| epoch  13 | dev | score (93.86, 93.2, 93.5) | f1 93.5 | time 256.58\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9512    0.9365    0.9438      3891\n",
      "          股票     0.9577    0.9448    0.9512      3694\n",
      "          体育     0.9863    0.9838    0.9850      3142\n",
      "          娱乐     0.9564    0.9516    0.9540      2213\n",
      "          时政     0.8662    0.9141    0.8895      1501\n",
      "          社会     0.8332    0.8986    0.8647      1223\n",
      "          教育     0.9369    0.9369    0.9369       998\n",
      "          财经     0.8810    0.8880    0.8845       884\n",
      "          家居     0.9385    0.9349    0.9367       784\n",
      "          游戏     0.9542    0.9140    0.9337       593\n",
      "          房产     0.9980    0.9898    0.9939       492\n",
      "          时尚     0.9593    0.9042    0.9309       313\n",
      "          彩票     0.9337    0.9286    0.9311       182\n",
      "          星座     0.9881    0.9222    0.9540        90\n",
      "\n",
      "    accuracy                         0.9409     20000\n",
      "   macro avg     0.9386    0.9320    0.9350     20000\n",
      "weighted avg     0.9420    0.9409    0.9413     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 93.33, current dev = 93.50\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch  14 | step 4579 | batch   3/352 | lr 0.00006 | loss 0.1687 | s/batch 5.70\n",
      "INFO:root:| epoch  14 | step 4582 | batch   6/352 | lr 0.00006 | loss 0.1953 | s/batch 3.18\n",
      "INFO:root:| epoch  14 | step 4585 | batch   9/352 | lr 0.00006 | loss 0.2176 | s/batch 5.76\n",
      "INFO:root:| epoch  14 | step 4588 | batch  12/352 | lr 0.00006 | loss 0.2146 | s/batch 10.77\n",
      "INFO:root:| epoch  14 | step 4591 | batch  15/352 | lr 0.00006 | loss 0.2405 | s/batch 13.68\n",
      "INFO:root:| epoch  14 | step 4594 | batch  18/352 | lr 0.00006 | loss 0.2370 | s/batch 5.78\n",
      "INFO:root:| epoch  14 | step 4597 | batch  21/352 | lr 0.00006 | loss 0.2059 | s/batch 9.36\n",
      "INFO:root:| epoch  14 | step 4600 | batch  24/352 | lr 0.00006 | loss 0.2454 | s/batch 8.25\n",
      "INFO:root:| epoch  14 | step 4603 | batch  27/352 | lr 0.00006 | loss 0.2608 | s/batch 6.32\n",
      "INFO:root:| epoch  14 | step 4606 | batch  30/352 | lr 0.00006 | loss 0.2225 | s/batch 8.15\n",
      "INFO:root:| epoch  14 | step 4609 | batch  33/352 | lr 0.00006 | loss 0.2680 | s/batch 9.95\n",
      "INFO:root:| epoch  14 | step 4612 | batch  36/352 | lr 0.00006 | loss 0.2043 | s/batch 6.19\n",
      "INFO:root:| epoch  14 | step 4615 | batch  39/352 | lr 0.00006 | loss 0.2948 | s/batch 14.43\n",
      "INFO:root:| epoch  14 | step 4618 | batch  42/352 | lr 0.00006 | loss 0.2592 | s/batch 5.98\n",
      "INFO:root:| epoch  14 | step 4621 | batch  45/352 | lr 0.00006 | loss 0.1856 | s/batch 6.97\n",
      "INFO:root:| epoch  14 | step 4624 | batch  48/352 | lr 0.00006 | loss 0.2312 | s/batch 9.84\n",
      "INFO:root:| epoch  14 | step 4627 | batch  51/352 | lr 0.00006 | loss 0.2144 | s/batch 9.79\n",
      "INFO:root:| epoch  14 | step 4630 | batch  54/352 | lr 0.00006 | loss 0.2470 | s/batch 6.52\n",
      "INFO:root:| epoch  14 | step 4633 | batch  57/352 | lr 0.00006 | loss 0.2439 | s/batch 9.54\n",
      "INFO:root:| epoch  14 | step 4636 | batch  60/352 | lr 0.00006 | loss 0.1851 | s/batch 10.90\n",
      "INFO:root:| epoch  14 | step 4639 | batch  63/352 | lr 0.00006 | loss 0.2063 | s/batch 5.79\n",
      "INFO:root:| epoch  14 | step 4642 | batch  66/352 | lr 0.00006 | loss 0.2228 | s/batch 4.43\n",
      "INFO:root:| epoch  14 | step 4645 | batch  69/352 | lr 0.00006 | loss 0.2093 | s/batch 5.07\n",
      "INFO:root:| epoch  14 | step 4648 | batch  72/352 | lr 0.00006 | loss 0.2267 | s/batch 5.05\n",
      "INFO:root:| epoch  14 | step 4651 | batch  75/352 | lr 0.00006 | loss 0.2220 | s/batch 4.29\n",
      "INFO:root:| epoch  14 | step 4654 | batch  78/352 | lr 0.00006 | loss 0.2062 | s/batch 10.36\n",
      "INFO:root:| epoch  14 | step 4657 | batch  81/352 | lr 0.00006 | loss 0.1915 | s/batch 9.07\n",
      "INFO:root:| epoch  14 | step 4660 | batch  84/352 | lr 0.00006 | loss 0.2230 | s/batch 8.23\n",
      "INFO:root:| epoch  14 | step 4663 | batch  87/352 | lr 0.00006 | loss 0.2069 | s/batch 10.15\n",
      "INFO:root:| epoch  14 | step 4666 | batch  90/352 | lr 0.00006 | loss 0.2059 | s/batch 10.40\n",
      "INFO:root:| epoch  14 | step 4669 | batch  93/352 | lr 0.00006 | loss 0.2292 | s/batch 6.35\n",
      "INFO:root:| epoch  14 | step 4672 | batch  96/352 | lr 0.00006 | loss 0.2383 | s/batch 5.56\n",
      "INFO:root:| epoch  14 | step 4675 | batch  99/352 | lr 0.00006 | loss 0.1992 | s/batch 5.63\n",
      "INFO:root:| epoch  14 | step 4678 | batch 102/352 | lr 0.00006 | loss 0.2087 | s/batch 2.82\n",
      "INFO:root:| epoch  14 | step 4681 | batch 105/352 | lr 0.00006 | loss 0.2343 | s/batch 8.06\n",
      "INFO:root:| epoch  14 | step 4684 | batch 108/352 | lr 0.00006 | loss 0.1979 | s/batch 4.96\n",
      "INFO:root:| epoch  14 | step 4687 | batch 111/352 | lr 0.00006 | loss 0.1913 | s/batch 11.07\n",
      "INFO:root:| epoch  14 | step 4690 | batch 114/352 | lr 0.00006 | loss 0.2540 | s/batch 18.00\n",
      "INFO:root:| epoch  14 | step 4693 | batch 117/352 | lr 0.00006 | loss 0.2608 | s/batch 2.21\n",
      "INFO:root:| epoch  14 | step 4696 | batch 120/352 | lr 0.00006 | loss 0.2017 | s/batch 8.29\n",
      "INFO:root:| epoch  14 | step 4699 | batch 123/352 | lr 0.00006 | loss 0.2548 | s/batch 8.61\n",
      "INFO:root:| epoch  14 | step 4702 | batch 126/352 | lr 0.00006 | loss 0.2190 | s/batch 12.62\n",
      "INFO:root:| epoch  14 | step 4705 | batch 129/352 | lr 0.00006 | loss 0.2076 | s/batch 4.45\n",
      "INFO:root:| epoch  14 | step 4708 | batch 132/352 | lr 0.00006 | loss 0.2262 | s/batch 10.60\n",
      "INFO:root:| epoch  14 | step 4711 | batch 135/352 | lr 0.00006 | loss 0.2282 | s/batch 4.41\n",
      "INFO:root:| epoch  14 | step 4714 | batch 138/352 | lr 0.00006 | loss 0.2201 | s/batch 4.39\n",
      "INFO:root:| epoch  14 | step 4717 | batch 141/352 | lr 0.00006 | loss 0.2320 | s/batch 4.08\n",
      "INFO:root:| epoch  14 | step 4720 | batch 144/352 | lr 0.00006 | loss 0.2067 | s/batch 4.98\n",
      "INFO:root:| epoch  14 | step 4723 | batch 147/352 | lr 0.00006 | loss 0.1939 | s/batch 6.64\n",
      "INFO:root:| epoch  14 | step 4726 | batch 150/352 | lr 0.00006 | loss 0.2332 | s/batch 9.99\n",
      "INFO:root:| epoch  14 | step 4729 | batch 153/352 | lr 0.00006 | loss 0.2098 | s/batch 9.50\n",
      "INFO:root:| epoch  14 | step 4732 | batch 156/352 | lr 0.00006 | loss 0.2255 | s/batch 14.74\n",
      "INFO:root:| epoch  14 | step 4735 | batch 159/352 | lr 0.00006 | loss 0.2162 | s/batch 4.53\n",
      "INFO:root:| epoch  14 | step 4738 | batch 162/352 | lr 0.00006 | loss 0.2548 | s/batch 7.57\n",
      "INFO:root:| epoch  14 | step 4741 | batch 165/352 | lr 0.00006 | loss 0.1782 | s/batch 9.19\n",
      "INFO:root:| epoch  14 | step 4744 | batch 168/352 | lr 0.00006 | loss 0.2221 | s/batch 12.13\n",
      "INFO:root:| epoch  14 | step 4747 | batch 171/352 | lr 0.00006 | loss 0.1673 | s/batch 7.01\n",
      "INFO:root:| epoch  14 | step 4750 | batch 174/352 | lr 0.00006 | loss 0.2093 | s/batch 7.90\n",
      "INFO:root:| epoch  14 | step 4753 | batch 177/352 | lr 0.00006 | loss 0.2297 | s/batch 6.74\n",
      "INFO:root:| epoch  14 | step 4756 | batch 180/352 | lr 0.00006 | loss 0.2303 | s/batch 5.17\n",
      "INFO:root:| epoch  14 | step 4759 | batch 183/352 | lr 0.00006 | loss 0.1979 | s/batch 5.61\n",
      "INFO:root:| epoch  14 | step 4762 | batch 186/352 | lr 0.00006 | loss 0.2117 | s/batch 3.68\n",
      "INFO:root:| epoch  14 | step 4765 | batch 189/352 | lr 0.00006 | loss 0.2374 | s/batch 10.46\n",
      "INFO:root:| epoch  14 | step 4768 | batch 192/352 | lr 0.00006 | loss 0.2823 | s/batch 18.51\n",
      "INFO:root:| epoch  14 | step 4771 | batch 195/352 | lr 0.00006 | loss 0.2103 | s/batch 4.85\n",
      "INFO:root:| epoch  14 | step 4774 | batch 198/352 | lr 0.00006 | loss 0.2109 | s/batch 6.96\n",
      "INFO:root:| epoch  14 | step 4777 | batch 201/352 | lr 0.00006 | loss 0.2150 | s/batch 7.56\n",
      "INFO:root:| epoch  14 | step 4780 | batch 204/352 | lr 0.00006 | loss 0.2412 | s/batch 10.10\n",
      "INFO:root:| epoch  14 | step 4783 | batch 207/352 | lr 0.00006 | loss 0.2145 | s/batch 8.11\n",
      "INFO:root:| epoch  14 | step 4786 | batch 210/352 | lr 0.00006 | loss 0.2068 | s/batch 10.07\n",
      "INFO:root:| epoch  14 | step 4789 | batch 213/352 | lr 0.00006 | loss 0.2197 | s/batch 6.08\n",
      "INFO:root:| epoch  14 | step 4792 | batch 216/352 | lr 0.00006 | loss 0.2759 | s/batch 16.59\n",
      "INFO:root:| epoch  14 | step 4795 | batch 219/352 | lr 0.00006 | loss 0.2610 | s/batch 8.37\n",
      "INFO:root:| epoch  14 | step 4798 | batch 222/352 | lr 0.00006 | loss 0.2161 | s/batch 7.02\n",
      "INFO:root:| epoch  14 | step 4801 | batch 225/352 | lr 0.00006 | loss 0.2318 | s/batch 9.42\n",
      "INFO:root:| epoch  14 | step 4804 | batch 228/352 | lr 0.00006 | loss 0.2127 | s/batch 9.79\n",
      "INFO:root:| epoch  14 | step 4807 | batch 231/352 | lr 0.00006 | loss 0.2110 | s/batch 7.57\n",
      "INFO:root:| epoch  14 | step 4810 | batch 234/352 | lr 0.00006 | loss 0.2007 | s/batch 9.69\n",
      "INFO:root:| epoch  14 | step 4813 | batch 237/352 | lr 0.00006 | loss 0.2624 | s/batch 9.20\n",
      "INFO:root:| epoch  14 | step 4816 | batch 240/352 | lr 0.00006 | loss 0.2222 | s/batch 3.26\n",
      "INFO:root:| epoch  14 | step 4819 | batch 243/352 | lr 0.00006 | loss 0.1892 | s/batch 4.68\n",
      "INFO:root:| epoch  14 | step 4822 | batch 246/352 | lr 0.00006 | loss 0.2566 | s/batch 7.99\n",
      "INFO:root:| epoch  14 | step 4825 | batch 249/352 | lr 0.00006 | loss 0.1928 | s/batch 7.09\n",
      "INFO:root:| epoch  14 | step 4828 | batch 252/352 | lr 0.00006 | loss 0.2336 | s/batch 8.20\n",
      "INFO:root:| epoch  14 | step 4831 | batch 255/352 | lr 0.00006 | loss 0.2282 | s/batch 3.73\n",
      "INFO:root:| epoch  14 | step 4834 | batch 258/352 | lr 0.00006 | loss 0.2310 | s/batch 9.42\n",
      "INFO:root:| epoch  14 | step 4837 | batch 261/352 | lr 0.00006 | loss 0.2657 | s/batch 3.82\n",
      "INFO:root:| epoch  14 | step 4840 | batch 264/352 | lr 0.00006 | loss 0.2153 | s/batch 4.39\n",
      "INFO:root:| epoch  14 | step 4843 | batch 267/352 | lr 0.00006 | loss 0.2149 | s/batch 4.39\n",
      "INFO:root:| epoch  14 | step 4846 | batch 270/352 | lr 0.00006 | loss 0.2138 | s/batch 9.37\n",
      "INFO:root:| epoch  14 | step 4849 | batch 273/352 | lr 0.00006 | loss 0.2112 | s/batch 10.55\n",
      "INFO:root:| epoch  14 | step 4852 | batch 276/352 | lr 0.00006 | loss 0.2133 | s/batch 13.21\n",
      "INFO:root:| epoch  14 | step 4855 | batch 279/352 | lr 0.00006 | loss 0.2280 | s/batch 13.84\n",
      "INFO:root:| epoch  14 | step 4858 | batch 282/352 | lr 0.00006 | loss 0.2454 | s/batch 3.16\n",
      "INFO:root:| epoch  14 | step 4861 | batch 285/352 | lr 0.00006 | loss 0.2350 | s/batch 4.51\n",
      "INFO:root:| epoch  14 | step 4864 | batch 288/352 | lr 0.00006 | loss 0.2541 | s/batch 5.07\n",
      "INFO:root:| epoch  14 | step 4867 | batch 291/352 | lr 0.00006 | loss 0.2136 | s/batch 5.79\n",
      "INFO:root:| epoch  14 | step 4870 | batch 294/352 | lr 0.00006 | loss 0.1974 | s/batch 7.02\n",
      "INFO:root:| epoch  14 | step 4873 | batch 297/352 | lr 0.00006 | loss 0.2463 | s/batch 9.96\n",
      "INFO:root:| epoch  14 | step 4876 | batch 300/352 | lr 0.00006 | loss 0.2314 | s/batch 12.94\n",
      "INFO:root:| epoch  14 | step 4879 | batch 303/352 | lr 0.00006 | loss 0.2159 | s/batch 9.84\n",
      "INFO:root:| epoch  14 | step 4882 | batch 306/352 | lr 0.00006 | loss 0.2294 | s/batch 5.72\n",
      "INFO:root:| epoch  14 | step 4885 | batch 309/352 | lr 0.00006 | loss 0.2255 | s/batch 3.60\n",
      "INFO:root:| epoch  14 | step 4888 | batch 312/352 | lr 0.00006 | loss 0.2085 | s/batch 9.15\n",
      "INFO:root:| epoch  14 | step 4891 | batch 315/352 | lr 0.00006 | loss 0.1876 | s/batch 4.50\n",
      "INFO:root:| epoch  14 | step 4894 | batch 318/352 | lr 0.00006 | loss 0.1965 | s/batch 5.78\n",
      "INFO:root:| epoch  14 | step 4897 | batch 321/352 | lr 0.00006 | loss 0.2334 | s/batch 10.27\n",
      "INFO:root:| epoch  14 | step 4900 | batch 324/352 | lr 0.00006 | loss 0.2388 | s/batch 5.83\n",
      "INFO:root:| epoch  14 | step 4903 | batch 327/352 | lr 0.00006 | loss 0.2255 | s/batch 6.53\n",
      "INFO:root:| epoch  14 | step 4906 | batch 330/352 | lr 0.00006 | loss 0.1984 | s/batch 3.78\n",
      "INFO:root:| epoch  14 | step 4909 | batch 333/352 | lr 0.00006 | loss 0.2202 | s/batch 11.26\n",
      "INFO:root:| epoch  14 | step 4912 | batch 336/352 | lr 0.00006 | loss 0.1840 | s/batch 5.71\n",
      "INFO:root:| epoch  14 | step 4915 | batch 339/352 | lr 0.00006 | loss 0.2086 | s/batch 11.86\n",
      "INFO:root:| epoch  14 | step 4918 | batch 342/352 | lr 0.00006 | loss 0.2105 | s/batch 5.76\n",
      "INFO:root:| epoch  14 | step 4921 | batch 345/352 | lr 0.00006 | loss 0.2209 | s/batch 12.60\n",
      "INFO:root:| epoch  14 | step 4924 | batch 348/352 | lr 0.00006 | loss 0.2666 | s/batch 10.77\n",
      "INFO:root:| epoch  14 | step 4927 | batch 351/352 | lr 0.00006 | loss 0.2156 | s/batch 11.25\n",
      "INFO:root:| epoch  14 | score (91.72, 90.88, 91.29) | f1 91.29 | loss 0.2223 | time 2795.43\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9296    0.9310    0.9303     35027\n",
      "          股票     0.9371    0.9433    0.9402     33251\n",
      "          体育     0.9824    0.9825    0.9825     28283\n",
      "          娱乐     0.9379    0.9476    0.9427     19920\n",
      "          时政     0.8684    0.8990    0.8834     13515\n",
      "          社会     0.8629    0.8535    0.8582     11009\n",
      "          教育     0.9298    0.9266    0.9282      8987\n",
      "          财经     0.8716    0.8247    0.8475      7957\n",
      "          家居     0.9044    0.9005    0.9024      7063\n",
      "          游戏     0.9175    0.8861    0.9015      5285\n",
      "          房产     0.9771    0.9639    0.9704      4428\n",
      "          时尚     0.8908    0.8744    0.8825      2818\n",
      "          彩票     0.9329    0.9079    0.9202      1639\n",
      "          星座     0.8980    0.8826    0.8903       818\n",
      "\n",
      "    accuracy                         0.9281    180000\n",
      "   macro avg     0.9172    0.9088    0.9129    180000\n",
      "weighted avg     0.9281    0.9281    0.9280    180000\n",
      "\n",
      "INFO:root:| epoch  14 | dev | score (94.07, 93.07, 93.54) | f1 93.54 | time 259.07\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9374    0.9499    0.9436      3891\n",
      "          股票     0.9616    0.9415    0.9514      3694\n",
      "          体育     0.9854    0.9860    0.9857      3142\n",
      "          娱乐     0.9620    0.9503    0.9561      2213\n",
      "          时政     0.8744    0.9094    0.8916      1501\n",
      "          社会     0.8520    0.8945    0.8728      1223\n",
      "          教育     0.9455    0.9389    0.9422       998\n",
      "          财经     0.8935    0.8824    0.8879       884\n",
      "          家居     0.9401    0.9209    0.9304       784\n",
      "          游戏     0.9476    0.9140    0.9305       593\n",
      "          房产     0.9959    0.9919    0.9939       492\n",
      "          时尚     0.9270    0.9329    0.9299       313\n",
      "          彩票     0.9598    0.9176    0.9382       182\n",
      "          星座     0.9878    0.9000    0.9419        90\n",
      "\n",
      "    accuracy                         0.9422     20000\n",
      "   macro avg     0.9407    0.9307    0.9354     20000\n",
      "weighted avg     0.9427    0.9422    0.9423     20000\n",
      "\n",
      "INFO:root:Exceed history dev = 93.50, current dev = 93.54\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch  15 | step 4931 | batch   3/352 | lr 0.00006 | loss 0.2145 | s/batch 5.22\n",
      "INFO:root:| epoch  15 | step 4934 | batch   6/352 | lr 0.00006 | loss 0.1978 | s/batch 9.02\n",
      "INFO:root:| epoch  15 | step 4937 | batch   9/352 | lr 0.00006 | loss 0.2419 | s/batch 12.45\n",
      "INFO:root:| epoch  15 | step 4940 | batch  12/352 | lr 0.00006 | loss 0.2122 | s/batch 7.60\n",
      "INFO:root:| epoch  15 | step 4943 | batch  15/352 | lr 0.00006 | loss 0.2073 | s/batch 4.06\n",
      "INFO:root:| epoch  15 | step 4946 | batch  18/352 | lr 0.00006 | loss 0.2044 | s/batch 3.85\n",
      "INFO:root:| epoch  15 | step 4949 | batch  21/352 | lr 0.00006 | loss 0.2489 | s/batch 4.14\n",
      "INFO:root:| epoch  15 | step 4952 | batch  24/352 | lr 0.00006 | loss 0.1918 | s/batch 9.14\n",
      "INFO:root:| epoch  15 | step 4955 | batch  27/352 | lr 0.00006 | loss 0.2346 | s/batch 10.25\n",
      "INFO:root:| epoch  15 | step 4958 | batch  30/352 | lr 0.00006 | loss 0.2037 | s/batch 4.29\n",
      "INFO:root:| epoch  15 | step 4961 | batch  33/352 | lr 0.00006 | loss 0.2422 | s/batch 8.61\n",
      "INFO:root:| epoch  15 | step 4964 | batch  36/352 | lr 0.00006 | loss 0.2092 | s/batch 11.26\n",
      "INFO:root:| epoch  15 | step 4967 | batch  39/352 | lr 0.00006 | loss 0.2069 | s/batch 9.20\n",
      "INFO:root:| epoch  15 | step 4970 | batch  42/352 | lr 0.00006 | loss 0.2014 | s/batch 5.77\n",
      "INFO:root:| epoch  15 | step 4973 | batch  45/352 | lr 0.00006 | loss 0.2257 | s/batch 7.26\n",
      "INFO:root:| epoch  15 | step 4976 | batch  48/352 | lr 0.00006 | loss 0.2181 | s/batch 3.94\n",
      "INFO:root:| epoch  15 | step 4979 | batch  51/352 | lr 0.00006 | loss 0.2408 | s/batch 10.45\n",
      "INFO:root:| epoch  15 | step 4982 | batch  54/352 | lr 0.00006 | loss 0.2604 | s/batch 9.65\n",
      "INFO:root:| epoch  15 | step 4985 | batch  57/352 | lr 0.00006 | loss 0.2130 | s/batch 9.12\n",
      "INFO:root:| epoch  15 | step 4988 | batch  60/352 | lr 0.00006 | loss 0.2489 | s/batch 10.58\n",
      "INFO:root:| epoch  15 | step 4991 | batch  63/352 | lr 0.00006 | loss 0.1974 | s/batch 5.80\n",
      "INFO:root:| epoch  15 | step 4994 | batch  66/352 | lr 0.00006 | loss 0.2250 | s/batch 10.13\n",
      "INFO:root:| epoch  15 | step 4997 | batch  69/352 | lr 0.00006 | loss 0.2040 | s/batch 9.77\n",
      "INFO:root:| epoch  15 | step 5000 | batch  72/352 | lr 0.00005 | loss 0.2866 | s/batch 8.30\n",
      "INFO:root:| epoch  15 | step 5003 | batch  75/352 | lr 0.00005 | loss 0.2279 | s/batch 6.81\n",
      "INFO:root:| epoch  15 | step 5006 | batch  78/352 | lr 0.00005 | loss 0.2101 | s/batch 2.26\n",
      "INFO:root:| epoch  15 | step 5009 | batch  81/352 | lr 0.00005 | loss 0.2572 | s/batch 12.12\n",
      "INFO:root:| epoch  15 | step 5012 | batch  84/352 | lr 0.00005 | loss 0.2417 | s/batch 6.82\n",
      "INFO:root:| epoch  15 | step 5015 | batch  87/352 | lr 0.00005 | loss 0.2422 | s/batch 11.64\n",
      "INFO:root:| epoch  15 | step 5018 | batch  90/352 | lr 0.00005 | loss 0.2505 | s/batch 8.94\n",
      "INFO:root:| epoch  15 | step 5021 | batch  93/352 | lr 0.00005 | loss 0.1990 | s/batch 3.36\n",
      "INFO:root:| epoch  15 | step 5024 | batch  96/352 | lr 0.00005 | loss 0.2052 | s/batch 3.71\n",
      "INFO:root:| epoch  15 | step 5027 | batch  99/352 | lr 0.00005 | loss 0.2451 | s/batch 2.72\n",
      "INFO:root:| epoch  15 | step 5030 | batch 102/352 | lr 0.00005 | loss 0.2169 | s/batch 5.77\n",
      "INFO:root:| epoch  15 | step 5033 | batch 105/352 | lr 0.00005 | loss 0.1906 | s/batch 8.30\n",
      "INFO:root:| epoch  15 | step 5036 | batch 108/352 | lr 0.00005 | loss 0.2084 | s/batch 8.70\n",
      "INFO:root:| epoch  15 | step 5039 | batch 111/352 | lr 0.00005 | loss 0.2611 | s/batch 14.46\n",
      "INFO:root:| epoch  15 | step 5042 | batch 114/352 | lr 0.00005 | loss 0.2074 | s/batch 5.62\n",
      "INFO:root:| epoch  15 | step 5045 | batch 117/352 | lr 0.00005 | loss 0.2630 | s/batch 4.00\n",
      "INFO:root:| epoch  15 | step 5048 | batch 120/352 | lr 0.00005 | loss 0.2494 | s/batch 3.75\n",
      "INFO:root:| epoch  15 | step 5051 | batch 123/352 | lr 0.00005 | loss 0.1984 | s/batch 7.02\n",
      "INFO:root:| epoch  15 | step 5054 | batch 126/352 | lr 0.00005 | loss 0.2695 | s/batch 2.18\n",
      "INFO:root:| epoch  15 | step 5057 | batch 129/352 | lr 0.00005 | loss 0.2667 | s/batch 12.02\n",
      "INFO:root:| epoch  15 | step 5060 | batch 132/352 | lr 0.00005 | loss 0.2640 | s/batch 10.66\n",
      "INFO:root:| epoch  15 | step 5063 | batch 135/352 | lr 0.00005 | loss 0.2482 | s/batch 13.28\n",
      "INFO:root:| epoch  15 | step 5066 | batch 138/352 | lr 0.00005 | loss 0.2192 | s/batch 4.41\n",
      "INFO:root:| epoch  15 | step 5069 | batch 141/352 | lr 0.00005 | loss 0.1817 | s/batch 7.64\n",
      "INFO:root:| epoch  15 | step 5072 | batch 144/352 | lr 0.00005 | loss 0.1946 | s/batch 10.61\n",
      "INFO:root:| epoch  15 | step 5075 | batch 147/352 | lr 0.00005 | loss 0.1953 | s/batch 9.52\n",
      "INFO:root:| epoch  15 | step 5078 | batch 150/352 | lr 0.00005 | loss 0.2079 | s/batch 5.10\n",
      "INFO:root:| epoch  15 | step 5081 | batch 153/352 | lr 0.00005 | loss 0.1732 | s/batch 7.69\n",
      "INFO:root:| epoch  15 | step 5084 | batch 156/352 | lr 0.00005 | loss 0.2162 | s/batch 13.57\n",
      "INFO:root:| epoch  15 | step 5087 | batch 159/352 | lr 0.00005 | loss 0.2512 | s/batch 11.18\n",
      "INFO:root:| epoch  15 | step 5090 | batch 162/352 | lr 0.00005 | loss 0.2128 | s/batch 4.62\n",
      "INFO:root:| epoch  15 | step 5093 | batch 165/352 | lr 0.00005 | loss 0.1683 | s/batch 7.67\n",
      "INFO:root:| epoch  15 | step 5096 | batch 168/352 | lr 0.00005 | loss 0.2431 | s/batch 9.38\n",
      "INFO:root:| epoch  15 | step 5099 | batch 171/352 | lr 0.00005 | loss 0.1885 | s/batch 9.26\n",
      "INFO:root:| epoch  15 | step 5102 | batch 174/352 | lr 0.00005 | loss 0.2271 | s/batch 11.30\n",
      "INFO:root:| epoch  15 | step 5105 | batch 177/352 | lr 0.00005 | loss 0.1893 | s/batch 4.39\n",
      "INFO:root:| epoch  15 | step 5108 | batch 180/352 | lr 0.00005 | loss 0.1862 | s/batch 8.71\n",
      "INFO:root:| epoch  15 | step 5111 | batch 183/352 | lr 0.00005 | loss 0.2071 | s/batch 4.53\n",
      "INFO:root:| epoch  15 | step 5114 | batch 186/352 | lr 0.00005 | loss 0.2209 | s/batch 4.06\n",
      "INFO:root:| epoch  15 | step 5117 | batch 189/352 | lr 0.00005 | loss 0.2201 | s/batch 4.99\n",
      "INFO:root:| epoch  15 | step 5120 | batch 192/352 | lr 0.00005 | loss 0.2120 | s/batch 8.68\n",
      "INFO:root:| epoch  15 | step 5123 | batch 195/352 | lr 0.00005 | loss 0.2196 | s/batch 9.81\n",
      "INFO:root:| epoch  15 | step 5126 | batch 198/352 | lr 0.00005 | loss 0.1951 | s/batch 3.71\n",
      "INFO:root:| epoch  15 | step 5129 | batch 201/352 | lr 0.00005 | loss 0.1936 | s/batch 4.34\n",
      "INFO:root:| epoch  15 | step 5132 | batch 204/352 | lr 0.00005 | loss 0.2144 | s/batch 12.23\n",
      "INFO:root:| epoch  15 | step 5135 | batch 207/352 | lr 0.00005 | loss 0.1944 | s/batch 3.24\n",
      "INFO:root:| epoch  15 | step 5138 | batch 210/352 | lr 0.00005 | loss 0.2256 | s/batch 7.45\n",
      "INFO:root:| epoch  15 | step 5141 | batch 213/352 | lr 0.00005 | loss 0.2055 | s/batch 14.06\n",
      "INFO:root:| epoch  15 | step 5144 | batch 216/352 | lr 0.00005 | loss 0.2287 | s/batch 10.96\n",
      "INFO:root:| epoch  15 | step 5147 | batch 219/352 | lr 0.00005 | loss 0.1914 | s/batch 7.14\n",
      "INFO:root:| epoch  15 | step 5150 | batch 222/352 | lr 0.00005 | loss 0.2015 | s/batch 7.93\n",
      "INFO:root:| epoch  15 | step 5153 | batch 225/352 | lr 0.00005 | loss 0.2215 | s/batch 5.67\n",
      "INFO:root:| epoch  15 | step 5156 | batch 228/352 | lr 0.00005 | loss 0.2068 | s/batch 4.80\n",
      "INFO:root:| epoch  15 | step 5159 | batch 231/352 | lr 0.00005 | loss 0.2480 | s/batch 13.38\n",
      "INFO:root:| epoch  15 | step 5162 | batch 234/352 | lr 0.00005 | loss 0.2102 | s/batch 5.17\n",
      "INFO:root:| epoch  15 | step 5165 | batch 237/352 | lr 0.00005 | loss 0.2283 | s/batch 11.03\n",
      "INFO:root:| epoch  15 | step 5168 | batch 240/352 | lr 0.00005 | loss 0.2709 | s/batch 8.39\n",
      "INFO:root:| epoch  15 | step 5171 | batch 243/352 | lr 0.00005 | loss 0.2448 | s/batch 16.61\n",
      "INFO:root:| epoch  15 | step 5174 | batch 246/352 | lr 0.00005 | loss 0.2039 | s/batch 5.12\n",
      "INFO:root:| epoch  15 | step 5177 | batch 249/352 | lr 0.00005 | loss 0.2033 | s/batch 9.93\n",
      "INFO:root:| epoch  15 | step 5180 | batch 252/352 | lr 0.00005 | loss 0.2294 | s/batch 14.85\n",
      "INFO:root:| epoch  15 | step 5183 | batch 255/352 | lr 0.00005 | loss 0.2322 | s/batch 16.39\n",
      "INFO:root:| epoch  15 | step 5186 | batch 258/352 | lr 0.00005 | loss 0.2570 | s/batch 9.62\n",
      "INFO:root:| epoch  15 | step 5189 | batch 261/352 | lr 0.00005 | loss 0.2434 | s/batch 13.76\n",
      "INFO:root:| epoch  15 | step 5192 | batch 264/352 | lr 0.00005 | loss 0.1760 | s/batch 4.48\n",
      "INFO:root:| epoch  15 | step 5195 | batch 267/352 | lr 0.00005 | loss 0.2525 | s/batch 10.57\n",
      "INFO:root:| epoch  15 | step 5198 | batch 270/352 | lr 0.00005 | loss 0.2002 | s/batch 9.44\n",
      "INFO:root:| epoch  15 | step 5201 | batch 273/352 | lr 0.00005 | loss 0.2238 | s/batch 3.45\n",
      "INFO:root:| epoch  15 | step 5204 | batch 276/352 | lr 0.00005 | loss 0.1708 | s/batch 11.80\n",
      "INFO:root:| epoch  15 | step 5207 | batch 279/352 | lr 0.00005 | loss 0.1928 | s/batch 4.44\n",
      "INFO:root:| epoch  15 | step 5210 | batch 282/352 | lr 0.00005 | loss 0.1783 | s/batch 3.47\n",
      "INFO:root:| epoch  15 | step 5213 | batch 285/352 | lr 0.00005 | loss 0.1965 | s/batch 7.77\n",
      "INFO:root:| epoch  15 | step 5216 | batch 288/352 | lr 0.00005 | loss 0.2181 | s/batch 9.46\n",
      "INFO:root:| epoch  15 | step 5219 | batch 291/352 | lr 0.00005 | loss 0.2212 | s/batch 13.07\n",
      "INFO:root:| epoch  15 | step 5222 | batch 294/352 | lr 0.00005 | loss 0.2389 | s/batch 9.85\n",
      "INFO:root:| epoch  15 | step 5225 | batch 297/352 | lr 0.00005 | loss 0.2120 | s/batch 5.68\n",
      "INFO:root:| epoch  15 | step 5228 | batch 300/352 | lr 0.00005 | loss 0.2207 | s/batch 4.01\n",
      "INFO:root:| epoch  15 | step 5231 | batch 303/352 | lr 0.00005 | loss 0.2380 | s/batch 11.69\n",
      "INFO:root:| epoch  15 | step 5234 | batch 306/352 | lr 0.00005 | loss 0.2147 | s/batch 3.98\n",
      "INFO:root:| epoch  15 | step 5237 | batch 309/352 | lr 0.00005 | loss 0.1943 | s/batch 4.44\n",
      "INFO:root:| epoch  15 | step 5240 | batch 312/352 | lr 0.00005 | loss 0.1731 | s/batch 4.56\n",
      "INFO:root:| epoch  15 | step 5243 | batch 315/352 | lr 0.00005 | loss 0.2255 | s/batch 2.66\n",
      "INFO:root:| epoch  15 | step 5246 | batch 318/352 | lr 0.00005 | loss 0.2404 | s/batch 16.15\n",
      "INFO:root:| epoch  15 | step 5249 | batch 321/352 | lr 0.00005 | loss 0.2254 | s/batch 9.95\n",
      "INFO:root:| epoch  15 | step 5252 | batch 324/352 | lr 0.00005 | loss 0.2277 | s/batch 12.59\n",
      "INFO:root:| epoch  15 | step 5255 | batch 327/352 | lr 0.00005 | loss 0.2208 | s/batch 5.85\n",
      "INFO:root:| epoch  15 | step 5258 | batch 330/352 | lr 0.00005 | loss 0.1827 | s/batch 4.51\n",
      "INFO:root:| epoch  15 | step 5261 | batch 333/352 | lr 0.00005 | loss 0.2368 | s/batch 8.76\n",
      "INFO:root:| epoch  15 | step 5264 | batch 336/352 | lr 0.00005 | loss 0.1851 | s/batch 9.03\n",
      "INFO:root:| epoch  15 | step 5267 | batch 339/352 | lr 0.00005 | loss 0.1771 | s/batch 10.51\n",
      "INFO:root:| epoch  15 | step 5270 | batch 342/352 | lr 0.00005 | loss 0.1827 | s/batch 9.83\n",
      "INFO:root:| epoch  15 | step 5273 | batch 345/352 | lr 0.00005 | loss 0.2101 | s/batch 7.41\n",
      "INFO:root:| epoch  15 | step 5276 | batch 348/352 | lr 0.00005 | loss 0.2426 | s/batch 2.59\n",
      "INFO:root:| epoch  15 | step 5279 | batch 351/352 | lr 0.00005 | loss 0.1836 | s/batch 7.22\n",
      "INFO:root:| epoch  15 | score (91.97, 90.98, 91.46) | f1 91.46 | loss 0.2181 | time 2809.88\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9306    0.9312    0.9309     35027\n",
      "          股票     0.9367    0.9436    0.9401     33251\n",
      "          体育     0.9812    0.9825    0.9818     28283\n",
      "          娱乐     0.9384    0.9498    0.9441     19920\n",
      "          时政     0.8704    0.9014    0.8856     13515\n",
      "          社会     0.8648    0.8548    0.8598     11009\n",
      "          教育     0.9340    0.9271    0.9305      8987\n",
      "          财经     0.8744    0.8258    0.8494      7957\n",
      "          家居     0.9087    0.9002    0.9044      7063\n",
      "          游戏     0.9167    0.8910    0.9037      5285\n",
      "          房产     0.9773    0.9645    0.9709      4428\n",
      "          时尚     0.8912    0.8783    0.8847      2818\n",
      "          彩票     0.9349    0.9024    0.9183      1639\n",
      "          星座     0.9165    0.8851    0.9005       818\n",
      "\n",
      "    accuracy                         0.9290    180000\n",
      "   macro avg     0.9197    0.9098    0.9146    180000\n",
      "weighted avg     0.9289    0.9290    0.9289    180000\n",
      "\n",
      "INFO:root:| epoch  15 | dev | score (93.86, 93.11, 93.47) | f1 93.47 | time 260.20\n",
      "INFO:root:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9474    0.9450    0.9462      3891\n",
      "          股票     0.9547    0.9469    0.9508      3694\n",
      "          体育     0.9857    0.9863    0.9860      3142\n",
      "          娱乐     0.9508    0.9602    0.9555      2213\n",
      "          时政     0.8833    0.9074    0.8952      1501\n",
      "          社会     0.8640    0.8937    0.8786      1223\n",
      "          教育     0.9442    0.9329    0.9385       998\n",
      "          财经     0.8905    0.8835    0.8870       884\n",
      "          家居     0.9369    0.9286    0.9327       784\n",
      "          游戏     0.9594    0.9157    0.9370       593\n",
      "          房产     1.0000    0.9858    0.9928       492\n",
      "          时尚     0.9206    0.9265    0.9236       313\n",
      "          彩票     0.9385    0.9231    0.9307       182\n",
      "          星座     0.9643    0.9000    0.9310        90\n",
      "\n",
      "    accuracy                         0.9431     20000\n",
      "   macro avg     0.9386    0.9311    0.9347     20000\n",
      "weighted avg     0.9434    0.9431    0.9432     20000\n",
      "\n",
      "INFO:root:---------Start Testing--------\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# build trainer\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clip = 5.0\n",
    "epochs = 15\n",
    "early_stops = 3\n",
    "log_interval = 10\n",
    "\n",
    "test_batch_size = 512\n",
    "train_batch_size = 512\n",
    "\n",
    "save_model = './model/cnn.bin'\n",
    "save_test = './result/cnn.csv'\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        self.report = True\n",
    "\n",
    "        # get_examples() 返回的结果是 一个 list\n",
    "        # 每个元素是一个 tuple: (label, 句子数量，doc)\n",
    "        # 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)\n",
    "        self.train_data = get_examples(train_data, vocab)\n",
    "        self.batch_num = int(np.ceil(len(self.train_data) / float(train_batch_size)))\n",
    "        self.dev_data = get_examples(dev_data, vocab)\n",
    "        self.test_data = get_examples(test_data, vocab)\n",
    "\n",
    "        # criterion\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # label name\n",
    "        self.target_names = vocab.target_names\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = Optimizer(model.all_parameters)\n",
    "\n",
    "        # count\n",
    "        self.step = 0\n",
    "        self.early_stop = -1\n",
    "        self.best_train_f1, self.best_dev_f1 = 0, 0\n",
    "        self.last_epoch = epochs\n",
    "\n",
    "    def train(self):\n",
    "        logging.info('Start training...')\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_f1 = self._train(epoch)\n",
    "\n",
    "            dev_f1 = self._eval(epoch)\n",
    "\n",
    "            if self.best_dev_f1 <= dev_f1:\n",
    "                logging.info(\n",
    "                    \"Exceed history dev = %.2f, current dev = %.2f\" % (self.best_dev_f1, dev_f1))\n",
    "                torch.save(self.model.state_dict(), save_model)\n",
    "\n",
    "                self.best_train_f1 = train_f1\n",
    "                self.best_dev_f1 = dev_f1\n",
    "                self.early_stop = 0\n",
    "            else:\n",
    "                self.early_stop += 1\n",
    "                if self.early_stop == early_stops:\n",
    "                    logging.info(\n",
    "                        \"Eearly stop in epoch %d, best train: %.2f, dev: %.2f\" % (\n",
    "                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))\n",
    "                    self.last_epoch = epoch\n",
    "                    break\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(torch.load(save_model))\n",
    "        self._eval(self.last_epoch + 1, test=True)\n",
    "\n",
    "    def _train(self, epoch):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.model.train()\n",
    "\n",
    "        start_time = time.time()\n",
    "        epoch_start_time = time.time()\n",
    "        overall_losses = 0\n",
    "        losses = 0\n",
    "        batch_idx = 1\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch_data in data_iter(self.train_data, train_batch_size, shuffle=True):\n",
    "            torch.cuda.empty_cache()\n",
    "            # batch_inputs: (batch_inputs1, batch_inputs2, batch_masks)\n",
    "            # 形状都是：batch_size * doc_len * sent_len\n",
    "            # batch_labels: batch_size\n",
    "            batch_inputs, batch_labels = self.batch2tensor(batch_data)\n",
    "            # batch_outputs：b * num_labels\n",
    "            batch_outputs = self.model(batch_inputs)\n",
    "            # criterion 是 CrossEntropyLoss，真实标签的形状是：N\n",
    "            # 预测标签的形状是：(N,C)\n",
    "            loss = self.criterion(batch_outputs, batch_labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            loss_value = loss.detach().cpu().item()\n",
    "            losses += loss_value\n",
    "            overall_losses += loss_value\n",
    "            # 把预测值转换为一维，方便下面做 classification_report，计算 f1\n",
    "            y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n",
    "            y_true.extend(batch_labels.cpu().numpy().tolist())\n",
    "            # 梯度裁剪\n",
    "            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)\n",
    "            for optimizer, scheduler in zip(self.optimizer.optims, self.optimizer.schedulers):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            self.step += 1\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "\n",
    "                lrs = self.optimizer.get_lr()\n",
    "                logging.info(\n",
    "                    '| epoch {:3d} | step {:3d} | batch {:3d}/{:3d} | lr{} | loss {:.4f} | s/batch {:.2f}'.format(\n",
    "                        epoch, self.step, batch_idx, self.batch_num, lrs,\n",
    "                        losses / log_interval,\n",
    "                        elapsed / log_interval))\n",
    "\n",
    "                losses = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "            batch_idx += 1\n",
    "\n",
    "        overall_losses /= self.batch_num\n",
    "        during_time = time.time() - epoch_start_time\n",
    "\n",
    "        # reformat 保留 4 位数字\n",
    "        overall_losses = reformat(overall_losses, 4)\n",
    "        score, f1 = get_score(y_true, y_pred)\n",
    "\n",
    "        logging.info(\n",
    "            '| epoch {:3d} | score {} | f1 {} | loss {:.4f} | time {:.2f}'.format(epoch, score, f1,\n",
    "                                                                                  overall_losses, during_time))\n",
    "        # 如果预测和真实的标签都包含相同的类别数目，才能调用 classification_report\n",
    "        if set(y_true) == set(y_pred) and self.report:\n",
    "            report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n",
    "            logging.info('\\n' + report)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    # 这里验证集、测试集都使用这个函数，通过 test 来区分使用哪个数据集\n",
    "    def _eval(self, epoch, test=False):\n",
    "        self.model.eval()\n",
    "        start_time = time.time()\n",
    "        data = self.test_data if test else self.dev_data\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_data in data_iter(data, test_batch_size, shuffle=False):\n",
    "                torch.cuda.empty_cache()\n",
    "                            # batch_inputs: (batch_inputs1, batch_inputs2, batch_masks)\n",
    "            # 形状都是：batch_size * doc_len * sent_len\n",
    "            # batch_labels: batch_size\n",
    "                batch_inputs, batch_labels = self.batch2tensor(batch_data)\n",
    "                # batch_outputs：b * num_labels\n",
    "                batch_outputs = self.model(batch_inputs)\n",
    "                # 把预测值转换为一维，方便下面做 classification_report，计算 f1\n",
    "                y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n",
    "                y_true.extend(batch_labels.cpu().numpy().tolist())\n",
    "\n",
    "            score, f1 = get_score(y_true, y_pred)\n",
    "\n",
    "            during_time = time.time() - start_time\n",
    "\n",
    "            if test:\n",
    "                df = pd.DataFrame({'label': y_pred})\n",
    "                df.to_csv(save_test, index=False, sep=',')\n",
    "            else:\n",
    "                logging.info(\n",
    "                    '| epoch {:3d} | dev | score {} | f1 {} | time {:.2f}'.format(epoch, score, f1,\n",
    "                                                                              during_time))\n",
    "                if set(y_true) == set(y_pred) and self.report:\n",
    "                    report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n",
    "                    logging.info('\\n' + report)\n",
    "\n",
    "        return f1\n",
    "\n",
    "\n",
    "    # data 参数就是 get_examples() 得到的，经过了分 batch\n",
    "    # batch_data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)\n",
    "    # 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)\n",
    "    def batch2tensor(self, batch_data):\n",
    "        '''\n",
    "            [[label, doc_len, [[sent_len, [word_id0, ...], [exword_id0, ...]], ...]]\n",
    "        '''\n",
    "        batch_size = len(batch_data)\n",
    "        doc_labels = []\n",
    "        doc_lens = []\n",
    "        doc_max_sent_len = []\n",
    "        for doc_data in batch_data:\n",
    "            # doc_data 代表一篇新闻，是一个 tuple: (label, 句子数量，doc)\n",
    "            # doc_data[0] 是 label\n",
    "            doc_labels.append(doc_data[0])\n",
    "            # doc_data[1] 是 这篇文章的句子数量\n",
    "            doc_lens.append(doc_data[1])\n",
    "            # doc_data[2] 是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)\n",
    "            # 所以 sent_data[0] 表示每个句子的长度（单词个数）\n",
    "            sent_lens = [sent_data[0] for sent_data in doc_data[2]]\n",
    "            # 取出这篇新闻中最长的句子长度（单词个数）\n",
    "            max_sent_len = max(sent_lens)\n",
    "            doc_max_sent_len.append(max_sent_len)\n",
    "\n",
    "        # 取出最长的句子数量\n",
    "        max_doc_len = max(doc_lens)\n",
    "        # 取出这批 batch 数据中最长的句子长度（单词个数）\n",
    "        max_sent_len = max(doc_max_sent_len)\n",
    "        # 创建 数据\n",
    "        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n",
    "        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n",
    "        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)\n",
    "        batch_labels = torch.LongTensor(doc_labels)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for sent_idx in range(doc_lens[b]):\n",
    "                # batch_data[b][2] 表示一个 list，是一篇文章中的句子\n",
    "                sent_data = batch_data[b][2][sent_idx] #sent_data 表示一个句子\n",
    "                for word_idx in range(sent_data[0]): # sent_data[0] 是句子长度(单词数量)\n",
    "                    # sent_data[1] 表示 word_ids\n",
    "                    batch_inputs1[b, sent_idx, word_idx] = sent_data[1][word_idx]\n",
    "                    # # sent_data[2] 表示 extword_ids\n",
    "                    batch_inputs2[b, sent_idx, word_idx] = sent_data[2][word_idx]\n",
    "                    # mask 表示 哪个位置是有词，后面计算 attention 时，没有词的地方会被置为 0\n",
    "                    batch_masks[b, sent_idx, word_idx] = 1\n",
    "\n",
    "        if use_cuda:\n",
    "            batch_inputs1 = batch_inputs1.to(device)\n",
    "            batch_inputs2 = batch_inputs2.to(device)\n",
    "            batch_masks = batch_masks.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "        return (batch_inputs1, batch_inputs2, batch_masks), batch_labels\n",
    "\n",
    "# train\n",
    "logging.info(\"---------Build Trainer--------\")\n",
    "trainer = Trainer(model, vocab)\n",
    "logging.info(\"---------Start Training--------\")\n",
    "trainer.train()\n",
    "\n",
    "# test\n",
    "logging.info(\"---------Start Testing--------\")\n",
    "trainer.test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T06:15:20.833478Z",
     "start_time": "2025-01-19T17:09:27.185513300Z"
    }
   },
   "id": "6922105021825b94",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T16:50:35.386571800Z",
     "start_time": "2025-01-19T16:50:35.371924300Z"
    }
   },
   "id": "580c5a67f21c0cd4",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:---------adj--------\n",
      "INFO:root:Some adjacency matrix or variable\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "# 记录日志信息\n",
    "logging.info(\"---------adj--------\")\n",
    "logging.info(\"Some adjacency matrix or variable\")  # 示例变量"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T16:51:24.923322700Z",
     "start_time": "2025-01-19T16:51:24.909914300Z"
    }
   },
   "id": "42086e65bf6c0ab5",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start training...\n",
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:| epoch   1 | step 1158 | batch  10/1407 | lr 0.00015 | loss 0.4311 | s/batch 1.64\n",
      "INFO:root:| epoch   1 | step 1168 | batch  20/1407 | lr 0.00015 | loss 0.4907 | s/batch 2.14\n",
      "INFO:root:| epoch   1 | step 1178 | batch  30/1407 | lr 0.00015 | loss 0.4402 | s/batch 2.25\n",
      "INFO:root:| epoch   1 | step 1188 | batch  40/1407 | lr 0.00015 | loss 0.4645 | s/batch 1.82\n",
      "INFO:root:| epoch   1 | step 1198 | batch  50/1407 | lr 0.00015 | loss 0.4253 | s/batch 2.02\n",
      "INFO:root:| epoch   1 | step 1208 | batch  60/1407 | lr 0.00015 | loss 0.4680 | s/batch 1.39\n",
      "INFO:root:| epoch   1 | step 1218 | batch  70/1407 | lr 0.00015 | loss 0.4368 | s/batch 2.33\n",
      "INFO:root:| epoch   1 | step 1228 | batch  80/1407 | lr 0.00015 | loss 0.4787 | s/batch 1.76\n",
      "INFO:root:| epoch   1 | step 1238 | batch  90/1407 | lr 0.00015 | loss 0.4875 | s/batch 1.39\n",
      "INFO:root:| epoch   1 | step 1248 | batch 100/1407 | lr 0.00015 | loss 0.4479 | s/batch 1.19\n",
      "INFO:root:| epoch   1 | step 1258 | batch 110/1407 | lr 0.00015 | loss 0.4362 | s/batch 1.78\n",
      "INFO:root:| epoch   1 | step 1268 | batch 120/1407 | lr 0.00015 | loss 0.4153 | s/batch 1.54\n",
      "INFO:root:| epoch   1 | step 1278 | batch 130/1407 | lr 0.00015 | loss 0.3949 | s/batch 1.89\n",
      "INFO:root:| epoch   1 | step 1288 | batch 140/1407 | lr 0.00015 | loss 0.4226 | s/batch 1.68\n",
      "INFO:root:| epoch   1 | step 1298 | batch 150/1407 | lr 0.00015 | loss 0.3962 | s/batch 1.26\n",
      "INFO:root:| epoch   1 | step 1308 | batch 160/1407 | lr 0.00015 | loss 0.4896 | s/batch 1.82\n",
      "INFO:root:| epoch   1 | step 1318 | batch 170/1407 | lr 0.00015 | loss 0.4779 | s/batch 1.56\n",
      "INFO:root:| epoch   1 | step 1328 | batch 180/1407 | lr 0.00015 | loss 0.4830 | s/batch 1.59\n",
      "INFO:root:| epoch   1 | step 1338 | batch 190/1407 | lr 0.00015 | loss 0.4665 | s/batch 1.57\n",
      "INFO:root:| epoch   1 | step 1348 | batch 200/1407 | lr 0.00015 | loss 0.4688 | s/batch 1.40\n",
      "INFO:root:| epoch   1 | step 1358 | batch 210/1407 | lr 0.00015 | loss 0.4335 | s/batch 1.79\n",
      "INFO:root:| epoch   1 | step 1368 | batch 220/1407 | lr 0.00015 | loss 0.4444 | s/batch 1.66\n",
      "INFO:root:| epoch   1 | step 1378 | batch 230/1407 | lr 0.00015 | loss 0.4456 | s/batch 2.07\n",
      "INFO:root:| epoch   1 | step 1388 | batch 240/1407 | lr 0.00015 | loss 0.3536 | s/batch 1.47\n",
      "INFO:root:| epoch   1 | step 1398 | batch 250/1407 | lr 0.00015 | loss 0.4781 | s/batch 1.58\n",
      "INFO:root:| epoch   1 | step 1408 | batch 260/1407 | lr 0.00015 | loss 0.4586 | s/batch 2.07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# test\u001B[39;00m\n\u001B[0;32m      4\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtest()\n",
      "Cell \u001B[1;32mIn[43], line 48\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     46\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStart training...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 48\u001B[0m     train_f1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m     dev_f1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eval(epoch)\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_dev_f1 \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m dev_f1:\n",
      "Cell \u001B[1;32mIn[43], line 89\u001B[0m, in \u001B[0;36mTrainer._train\u001B[1;34m(self, epoch)\u001B[0m\n\u001B[0;32m     85\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# batch_inputs: (batch_inputs1, batch_inputs2, batch_masks)\u001B[39;00m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;66;03m# 形状都是：batch_size * doc_len * sent_len\u001B[39;00m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# batch_labels: batch_size\u001B[39;00m\n\u001B[1;32m---> 89\u001B[0m batch_inputs, batch_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch2tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;66;03m# batch_outputs：b * num_labels\u001B[39;00m\n\u001B[0;32m     91\u001B[0m batch_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(batch_inputs)\n",
      "Cell \u001B[1;32mIn[43], line 227\u001B[0m, in \u001B[0;36mTrainer.batch2tensor\u001B[1;34m(self, batch_data)\u001B[0m\n\u001B[0;32m    225\u001B[0m             batch_inputs2[b, sent_idx, word_idx] \u001B[38;5;241m=\u001B[39m sent_data[\u001B[38;5;241m2\u001B[39m][word_idx]\n\u001B[0;32m    226\u001B[0m             \u001B[38;5;66;03m# mask 表示 哪个位置是有词，后面计算 attention 时，没有词的地方会被置为 0\u001B[39;00m\n\u001B[1;32m--> 227\u001B[0m             batch_masks[b, sent_idx, word_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cuda:\n\u001B[0;32m    230\u001B[0m     batch_inputs1 \u001B[38;5;241m=\u001B[39m batch_inputs1\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# test\n",
    "trainer.test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T16:59:19.060734300Z",
     "start_time": "2025-01-19T16:51:48.263062500Z"
    }
   },
   "id": "473519d45bb349c2",
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "nlp",
   "language": "python",
   "display_name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
